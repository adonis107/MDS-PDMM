{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36d037dc",
   "metadata": {},
   "source": [
    "# Sequential Training Pipeline\n",
    "\n",
    "Train models sequentially across multiple days:\n",
    "- **Dataset**: TOTF\n",
    "- **Scaler**: Box-Cox\n",
    "- **Models**: Transformer+OCSVM, PRAE, PNN\n",
    "- **Training Strategy**: Train on first hour of each day (5-min blocks), test on rest of day\n",
    "- **Final Test**: Final Day (25) (morning + rest of day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70488f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import joblib\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from PDMM import preprocessing as prep\n",
    "from PDMM import  machine_learning as ml\n",
    "from PDMM import visualization as viz\n",
    "from PDMM.pipeline import AnomalyDetectionPipeline, sequential_training_pipeline\n",
    "\n",
    "os.makedirs('../models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6509aeca",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5112e917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Configuration\n",
    "DATA_DIR = '../data/TOTF.PA-book'  # Directory with daily TOTF files\n",
    "SCALER_TYPE = 'box-cox'\n",
    "\n",
    "# Sequential Training Parameters\n",
    "NUM_DAYS = 7            # Train on first 24 days, 1-indexed\n",
    "FIRST_HOUR_MINUTES = 60  # Use first hour of each day\n",
    "TRAIN_BLOCK_MINUTES = 10  # 5-minute training blocks\n",
    "VAL_BLOCK_MINUTES = 10    # 5-minute validation blocks\n",
    "\n",
    "# Model Hyperparameters\n",
    "SEQ_LENGTH = 25\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 1000\n",
    "HIDDEN_DIM = 64\n",
    "LR = 1e-3\n",
    "PATIENCE = 5\n",
    "\n",
    "# Model Types to Train\n",
    "MODELS = ['transformer_ocsvm', 'prae', 'pnn']\n",
    "\n",
    "# Feature Sets\n",
    "FEATURE_SETS = ['base', 'tao', 'poutre', 'hawkes', 'ofi']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e12ff49",
   "metadata": {},
   "source": [
    "## Check Available Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1784cd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25 data files in ../data/TOTF.PA-book\n"
     ]
    }
   ],
   "source": [
    "files = sorted([f for f in os.listdir(DATA_DIR) if f.endswith(('.csv', '.csv.gz', '.parquet'))])\n",
    "print(f\"Found {len(files)} data files in {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36fad85",
   "metadata": {},
   "source": [
    "## Train Models Sequentially\n",
    "\n",
    "Train each model type on 30 days, evaluate on rest of each day + Day 31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c789243",
   "metadata": {},
   "source": [
    "### 1. Transformer + OC-SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aa28ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING: Transformer + OC-SVM\n",
      "Pipeline initialized on device: cuda\n",
      "Starting sequential training on 7 days...\n",
      "Train block: 10 min, Val block: 10 min\n",
      "================================================================================\n",
      "Day 1/7: 2015-01-02-TOTF.PA-book.csv.gz\n",
      "--------------------------------------------------------------------------------\n",
      "Loading data from ../data/TOTF.PA-book\\2015-01-02-TOTF.PA-book.csv.gz...\n",
      "Successfully loaded 640429 rows.\n",
      "Filtered 2801 pre-market samples.\n",
      "First hour samples: 74476, Rest of day: 563152\n",
      "Training on 3 time blocks...\n",
      "Block 1/3: Train=21255, Val=12380\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Initializing transformer_ocsvm model...\n",
      "Initializing Transformer Autoencoder...\n",
      "Training Autoencoder (Max Epochs=1000)...\n",
      "Epoch 1/1000 - Train Loss: 0.695619 | Val Loss: 0.629904\n",
      "Validation loss decreased (inf --> 0.629904).  Saving model ...\n",
      "Epoch 2/1000 - Train Loss: 0.497259 | Val Loss: 0.542634\n",
      "Validation loss decreased (0.629904 --> 0.542634).  Saving model ...\n",
      "Epoch 3/1000 - Train Loss: 0.421251 | Val Loss: 0.556924\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 4/1000 - Train Loss: 0.378479 | Val Loss: 0.548158\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 5/1000 - Train Loss: 0.343680 | Val Loss: 0.531984\n",
      "Validation loss decreased (0.542634 --> 0.531984).  Saving model ...\n",
      "Epoch 6/1000 - Train Loss: 0.311250 | Val Loss: 0.514951\n",
      "Validation loss decreased (0.531984 --> 0.514951).  Saving model ...\n",
      "Epoch 7/1000 - Train Loss: 0.280026 | Val Loss: 0.520869\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 8/1000 - Train Loss: 0.246469 | Val Loss: 0.490201\n",
      "Validation loss decreased (0.514951 --> 0.490201).  Saving model ...\n",
      "Epoch 9/1000 - Train Loss: 0.213023 | Val Loss: 0.446823\n",
      "Validation loss decreased (0.490201 --> 0.446823).  Saving model ...\n",
      "Epoch 10/1000 - Train Loss: 0.189845 | Val Loss: 0.445231\n",
      "Validation loss decreased (0.446823 --> 0.445231).  Saving model ...\n",
      "Epoch 11/1000 - Train Loss: 0.172855 | Val Loss: 0.399489\n",
      "Validation loss decreased (0.445231 --> 0.399489).  Saving model ...\n",
      "Epoch 12/1000 - Train Loss: 0.160453 | Val Loss: 0.387892\n",
      "Validation loss decreased (0.399489 --> 0.387892).  Saving model ...\n",
      "Epoch 13/1000 - Train Loss: 0.149907 | Val Loss: 0.374474\n",
      "Validation loss decreased (0.387892 --> 0.374474).  Saving model ...\n",
      "Epoch 14/1000 - Train Loss: 0.138903 | Val Loss: 0.373751\n",
      "Validation loss decreased (0.374474 --> 0.373751).  Saving model ...\n",
      "Epoch 15/1000 - Train Loss: 0.131435 | Val Loss: 0.355168\n",
      "Validation loss decreased (0.373751 --> 0.355168).  Saving model ...\n",
      "Epoch 16/1000 - Train Loss: 0.124326 | Val Loss: 0.345708\n",
      "Validation loss decreased (0.355168 --> 0.345708).  Saving model ...\n",
      "Epoch 17/1000 - Train Loss: 0.119268 | Val Loss: 0.333283\n",
      "Validation loss decreased (0.345708 --> 0.333283).  Saving model ...\n",
      "Epoch 18/1000 - Train Loss: 0.115956 | Val Loss: 0.338997\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 19/1000 - Train Loss: 0.113014 | Val Loss: 0.328751\n",
      "Validation loss decreased (0.333283 --> 0.328751).  Saving model ...\n",
      "Epoch 20/1000 - Train Loss: 0.108882 | Val Loss: 0.314590\n",
      "Validation loss decreased (0.328751 --> 0.314590).  Saving model ...\n",
      "Epoch 21/1000 - Train Loss: 0.105347 | Val Loss: 0.316012\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 22/1000 - Train Loss: 0.102198 | Val Loss: 0.316697\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 23/1000 - Train Loss: 0.100363 | Val Loss: 0.314195\n",
      "Validation loss decreased (0.314590 --> 0.314195).  Saving model ...\n",
      "Epoch 24/1000 - Train Loss: 0.099225 | Val Loss: 0.308883\n",
      "Validation loss decreased (0.314195 --> 0.308883).  Saving model ...\n",
      "Epoch 25/1000 - Train Loss: 0.096942 | Val Loss: 0.309642\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 26/1000 - Train Loss: 0.094443 | Val Loss: 0.309383\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 27/1000 - Train Loss: 0.093179 | Val Loss: 0.299005\n",
      "Validation loss decreased (0.308883 --> 0.299005).  Saving model ...\n",
      "Epoch 28/1000 - Train Loss: 0.090978 | Val Loss: 0.299845\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 29/1000 - Train Loss: 0.089620 | Val Loss: 0.300835\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 30/1000 - Train Loss: 0.090438 | Val Loss: 0.299396\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 31/1000 - Train Loss: 0.087426 | Val Loss: 0.297007\n",
      "Validation loss decreased (0.299005 --> 0.297007).  Saving model ...\n",
      "Epoch 32/1000 - Train Loss: 0.086273 | Val Loss: 0.290121\n",
      "Validation loss decreased (0.297007 --> 0.290121).  Saving model ...\n",
      "Epoch 33/1000 - Train Loss: 0.085543 | Val Loss: 0.299899\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 34/1000 - Train Loss: 0.085870 | Val Loss: 0.286455\n",
      "Validation loss decreased (0.290121 --> 0.286455).  Saving model ...\n",
      "Epoch 35/1000 - Train Loss: 0.083724 | Val Loss: 0.292288\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 36/1000 - Train Loss: 0.083130 | Val Loss: 0.298730\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 37/1000 - Train Loss: 0.082048 | Val Loss: 0.292475\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 38/1000 - Train Loss: 0.081929 | Val Loss: 0.290880\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 39/1000 - Train Loss: 0.081258 | Val Loss: 0.284059\n",
      "Validation loss decreased (0.286455 --> 0.284059).  Saving model ...\n",
      "Epoch 40/1000 - Train Loss: 0.079179 | Val Loss: 0.283743\n",
      "Validation loss decreased (0.284059 --> 0.283743).  Saving model ...\n",
      "Epoch 41/1000 - Train Loss: 0.077156 | Val Loss: 0.279778\n",
      "Validation loss decreased (0.283743 --> 0.279778).  Saving model ...\n",
      "Epoch 42/1000 - Train Loss: 0.076555 | Val Loss: 0.285556\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 43/1000 - Train Loss: 0.075662 | Val Loss: 0.281592\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 44/1000 - Train Loss: 0.075970 | Val Loss: 0.278543\n",
      "Validation loss decreased (0.279778 --> 0.278543).  Saving model ...\n",
      "Epoch 45/1000 - Train Loss: 0.073734 | Val Loss: 0.278134\n",
      "Validation loss decreased (0.278543 --> 0.278134).  Saving model ...\n",
      "Epoch 46/1000 - Train Loss: 0.073672 | Val Loss: 0.279896\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 47/1000 - Train Loss: 0.072553 | Val Loss: 0.275658\n",
      "Validation loss decreased (0.278134 --> 0.275658).  Saving model ...\n",
      "Epoch 48/1000 - Train Loss: 0.071846 | Val Loss: 0.276887\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 49/1000 - Train Loss: 0.071552 | Val Loss: 0.274204\n",
      "Validation loss decreased (0.275658 --> 0.274204).  Saving model ...\n",
      "Epoch 50/1000 - Train Loss: 0.071368 | Val Loss: 0.271759\n",
      "Validation loss decreased (0.274204 --> 0.271759).  Saving model ...\n",
      "Epoch 51/1000 - Train Loss: 0.071101 | Val Loss: 0.271075\n",
      "Validation loss decreased (0.271759 --> 0.271075).  Saving model ...\n",
      "Epoch 52/1000 - Train Loss: 0.070577 | Val Loss: 0.269934\n",
      "Validation loss decreased (0.271075 --> 0.269934).  Saving model ...\n",
      "Epoch 53/1000 - Train Loss: 0.070376 | Val Loss: 0.273319\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 54/1000 - Train Loss: 0.069435 | Val Loss: 0.272204\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 55/1000 - Train Loss: 0.069190 | Val Loss: 0.275223\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 56/1000 - Train Loss: 0.068918 | Val Loss: 0.274088\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 57/1000 - Train Loss: 0.068877 | Val Loss: 0.271314\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Extracting Latent Representations for OC-SVM...\n",
      "Training One-Class SVM (nu=0.01)...\n",
      "Block 2/3: Train=13265, Val=7183\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Transformer Autoencoder...\n",
      "Training Autoencoder (Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 0.741190 | Val Loss: 0.647747\n",
      "Validation loss decreased (inf --> 0.647747).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 0.548019 | Val Loss: 0.561789\n",
      "Validation loss decreased (0.647747 --> 0.561789).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 0.498185 | Val Loss: 0.503684\n",
      "Validation loss decreased (0.561789 --> 0.503684).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 0.436597 | Val Loss: 0.466798\n",
      "Validation loss decreased (0.503684 --> 0.466798).  Saving model ...\n",
      "Epoch 5/500 - Train Loss: 0.398597 | Val Loss: 0.436219\n",
      "Validation loss decreased (0.466798 --> 0.436219).  Saving model ...\n",
      "Epoch 6/500 - Train Loss: 0.363610 | Val Loss: 0.417613\n",
      "Validation loss decreased (0.436219 --> 0.417613).  Saving model ...\n",
      "Epoch 7/500 - Train Loss: 0.335955 | Val Loss: 0.401179\n",
      "Validation loss decreased (0.417613 --> 0.401179).  Saving model ...\n",
      "Epoch 8/500 - Train Loss: 0.313140 | Val Loss: 0.381059\n",
      "Validation loss decreased (0.401179 --> 0.381059).  Saving model ...\n",
      "Epoch 9/500 - Train Loss: 0.284994 | Val Loss: 0.348027\n",
      "Validation loss decreased (0.381059 --> 0.348027).  Saving model ...\n",
      "Epoch 10/500 - Train Loss: 0.255568 | Val Loss: 0.322666\n",
      "Validation loss decreased (0.348027 --> 0.322666).  Saving model ...\n",
      "Epoch 11/500 - Train Loss: 0.232564 | Val Loss: 0.301437\n",
      "Validation loss decreased (0.322666 --> 0.301437).  Saving model ...\n",
      "Epoch 12/500 - Train Loss: 0.212924 | Val Loss: 0.282800\n",
      "Validation loss decreased (0.301437 --> 0.282800).  Saving model ...\n",
      "Epoch 13/500 - Train Loss: 0.196158 | Val Loss: 0.282806\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 14/500 - Train Loss: 0.182534 | Val Loss: 0.269987\n",
      "Validation loss decreased (0.282800 --> 0.269987).  Saving model ...\n",
      "Epoch 15/500 - Train Loss: 0.170887 | Val Loss: 0.266699\n",
      "Validation loss decreased (0.269987 --> 0.266699).  Saving model ...\n",
      "Epoch 16/500 - Train Loss: 0.161379 | Val Loss: 0.260931\n",
      "Validation loss decreased (0.266699 --> 0.260931).  Saving model ...\n",
      "Epoch 17/500 - Train Loss: 0.151146 | Val Loss: 0.254010\n",
      "Validation loss decreased (0.260931 --> 0.254010).  Saving model ...\n",
      "Epoch 18/500 - Train Loss: 0.142683 | Val Loss: 0.247612\n",
      "Validation loss decreased (0.254010 --> 0.247612).  Saving model ...\n",
      "Epoch 19/500 - Train Loss: 0.135186 | Val Loss: 0.243970\n",
      "Validation loss decreased (0.247612 --> 0.243970).  Saving model ...\n",
      "Epoch 20/500 - Train Loss: 0.129065 | Val Loss: 0.241503\n",
      "Validation loss decreased (0.243970 --> 0.241503).  Saving model ...\n",
      "Epoch 21/500 - Train Loss: 0.123153 | Val Loss: 0.237882\n",
      "Validation loss decreased (0.241503 --> 0.237882).  Saving model ...\n",
      "Epoch 22/500 - Train Loss: 0.117677 | Val Loss: 0.236860\n",
      "Validation loss decreased (0.237882 --> 0.236860).  Saving model ...\n",
      "Epoch 23/500 - Train Loss: 0.113318 | Val Loss: 0.230539\n",
      "Validation loss decreased (0.236860 --> 0.230539).  Saving model ...\n",
      "Epoch 24/500 - Train Loss: 0.108763 | Val Loss: 0.228779\n",
      "Validation loss decreased (0.230539 --> 0.228779).  Saving model ...\n",
      "Epoch 25/500 - Train Loss: 0.104623 | Val Loss: 0.229303\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 26/500 - Train Loss: 0.100974 | Val Loss: 0.227257\n",
      "Validation loss decreased (0.228779 --> 0.227257).  Saving model ...\n",
      "Epoch 27/500 - Train Loss: 0.097544 | Val Loss: 0.226699\n",
      "Validation loss decreased (0.227257 --> 0.226699).  Saving model ...\n",
      "Epoch 28/500 - Train Loss: 0.094556 | Val Loss: 0.227019\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 29/500 - Train Loss: 0.092005 | Val Loss: 0.221693\n",
      "Validation loss decreased (0.226699 --> 0.221693).  Saving model ...\n",
      "Epoch 30/500 - Train Loss: 0.089847 | Val Loss: 0.225046\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 31/500 - Train Loss: 0.087029 | Val Loss: 0.219599\n",
      "Validation loss decreased (0.221693 --> 0.219599).  Saving model ...\n",
      "Epoch 32/500 - Train Loss: 0.084902 | Val Loss: 0.222198\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 33/500 - Train Loss: 0.082960 | Val Loss: 0.221515\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 34/500 - Train Loss: 0.080538 | Val Loss: 0.216743\n",
      "Validation loss decreased (0.219599 --> 0.216743).  Saving model ...\n",
      "Epoch 35/500 - Train Loss: 0.078661 | Val Loss: 0.218304\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 36/500 - Train Loss: 0.077035 | Val Loss: 0.221258\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 37/500 - Train Loss: 0.074952 | Val Loss: 0.220396\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 38/500 - Train Loss: 0.073339 | Val Loss: 0.219804\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 39/500 - Train Loss: 0.071734 | Val Loss: 0.214194\n",
      "Validation loss decreased (0.216743 --> 0.214194).  Saving model ...\n",
      "Epoch 40/500 - Train Loss: 0.070341 | Val Loss: 0.222082\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 41/500 - Train Loss: 0.068702 | Val Loss: 0.212847\n",
      "Validation loss decreased (0.214194 --> 0.212847).  Saving model ...\n",
      "Epoch 42/500 - Train Loss: 0.067614 | Val Loss: 0.218207\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 43/500 - Train Loss: 0.065887 | Val Loss: 0.215603\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 44/500 - Train Loss: 0.064966 | Val Loss: 0.213759\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 45/500 - Train Loss: 0.063910 | Val Loss: 0.210910\n",
      "Validation loss decreased (0.212847 --> 0.210910).  Saving model ...\n",
      "Epoch 46/500 - Train Loss: 0.062647 | Val Loss: 0.214908\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 47/500 - Train Loss: 0.061457 | Val Loss: 0.213456\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 48/500 - Train Loss: 0.060512 | Val Loss: 0.210609\n",
      "Validation loss decreased (0.210910 --> 0.210609).  Saving model ...\n",
      "Epoch 49/500 - Train Loss: 0.059520 | Val Loss: 0.211557\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 50/500 - Train Loss: 0.058785 | Val Loss: 0.211886\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 51/500 - Train Loss: 0.057839 | Val Loss: 0.206774\n",
      "Validation loss decreased (0.210609 --> 0.206774).  Saving model ...\n",
      "Epoch 52/500 - Train Loss: 0.057356 | Val Loss: 0.212250\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 53/500 - Train Loss: 0.056725 | Val Loss: 0.208700\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 54/500 - Train Loss: 0.055857 | Val Loss: 0.209690\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 55/500 - Train Loss: 0.055101 | Val Loss: 0.207700\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 56/500 - Train Loss: 0.054359 | Val Loss: 0.213614\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Extracting Latent Representations for OC-SVM...\n",
      "Training One-Class SVM (nu=0.01)...\n",
      "Block 3/3: Train=7918, Val=12475\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Transformer Autoencoder...\n",
      "Training Autoencoder (Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 1.706718 | Val Loss: 5.295427\n",
      "Validation loss decreased (inf --> 5.295427).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 0.923670 | Val Loss: 3.724409\n",
      "Validation loss decreased (5.295427 --> 3.724409).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 0.715987 | Val Loss: 3.093190\n",
      "Validation loss decreased (3.724409 --> 3.093190).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 0.657063 | Val Loss: 2.778967\n",
      "Validation loss decreased (3.093190 --> 2.778967).  Saving model ...\n",
      "Epoch 5/500 - Train Loss: 0.605121 | Val Loss: 2.540004\n",
      "Validation loss decreased (2.778967 --> 2.540004).  Saving model ...\n",
      "Epoch 6/500 - Train Loss: 0.553423 | Val Loss: 2.377568\n",
      "Validation loss decreased (2.540004 --> 2.377568).  Saving model ...\n",
      "Epoch 7/500 - Train Loss: 0.512682 | Val Loss: 2.268372\n",
      "Validation loss decreased (2.377568 --> 2.268372).  Saving model ...\n",
      "Epoch 8/500 - Train Loss: 0.483149 | Val Loss: 2.250581\n",
      "Validation loss decreased (2.268372 --> 2.250581).  Saving model ...\n",
      "Epoch 9/500 - Train Loss: 0.444990 | Val Loss: 2.275084\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 10/500 - Train Loss: 0.410447 | Val Loss: 2.273230\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 11/500 - Train Loss: 0.379151 | Val Loss: 2.316022\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 12/500 - Train Loss: 0.350604 | Val Loss: 2.172976\n",
      "Validation loss decreased (2.250581 --> 2.172976).  Saving model ...\n",
      "Epoch 13/500 - Train Loss: 0.330021 | Val Loss: 2.266441\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 14/500 - Train Loss: 0.312009 | Val Loss: 2.117858\n",
      "Validation loss decreased (2.172976 --> 2.117858).  Saving model ...\n",
      "Epoch 15/500 - Train Loss: 0.294652 | Val Loss: 2.135250\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 16/500 - Train Loss: 0.279211 | Val Loss: 2.138715\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 17/500 - Train Loss: 0.265756 | Val Loss: 2.098860\n",
      "Validation loss decreased (2.117858 --> 2.098860).  Saving model ...\n",
      "Epoch 18/500 - Train Loss: 0.253782 | Val Loss: 2.006904\n",
      "Validation loss decreased (2.098860 --> 2.006904).  Saving model ...\n",
      "Epoch 19/500 - Train Loss: 0.243818 | Val Loss: 2.005239\n",
      "Validation loss decreased (2.006904 --> 2.005239).  Saving model ...\n",
      "Epoch 20/500 - Train Loss: 0.236006 | Val Loss: 2.032209\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 21/500 - Train Loss: 0.227629 | Val Loss: 2.008698\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 22/500 - Train Loss: 0.218126 | Val Loss: 2.051380\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 23/500 - Train Loss: 0.209949 | Val Loss: 1.992079\n",
      "Validation loss decreased (2.005239 --> 1.992079).  Saving model ...\n",
      "Epoch 24/500 - Train Loss: 0.200428 | Val Loss: 2.007612\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 25/500 - Train Loss: 0.193179 | Val Loss: 1.970112\n",
      "Validation loss decreased (1.992079 --> 1.970112).  Saving model ...\n",
      "Epoch 26/500 - Train Loss: 0.188026 | Val Loss: 1.944186\n",
      "Validation loss decreased (1.970112 --> 1.944186).  Saving model ...\n",
      "Epoch 27/500 - Train Loss: 0.182293 | Val Loss: 1.939076\n",
      "Validation loss decreased (1.944186 --> 1.939076).  Saving model ...\n",
      "Epoch 28/500 - Train Loss: 0.178232 | Val Loss: 2.038054\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 29/500 - Train Loss: 0.172726 | Val Loss: 1.960786\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 30/500 - Train Loss: 0.168076 | Val Loss: 1.973359\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 31/500 - Train Loss: 0.162746 | Val Loss: 1.921828\n",
      "Validation loss decreased (1.939076 --> 1.921828).  Saving model ...\n",
      "Epoch 32/500 - Train Loss: 0.157833 | Val Loss: 1.888578\n",
      "Validation loss decreased (1.921828 --> 1.888578).  Saving model ...\n",
      "Epoch 33/500 - Train Loss: 0.153162 | Val Loss: 1.983747\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 34/500 - Train Loss: 0.148369 | Val Loss: 1.947672\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 35/500 - Train Loss: 0.143565 | Val Loss: 1.968118\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 36/500 - Train Loss: 0.140757 | Val Loss: 1.949087\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 37/500 - Train Loss: 0.137658 | Val Loss: 1.962324\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Extracting Latent Representations for OC-SVM...\n",
      "Training One-Class SVM (nu=0.01)...\n",
      "\n",
      "Evaluating on rest of Day 1...\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Evaluating model...\n",
      "Day 1 Results: AUROC=1.0000, AUPRC=1.0000, F4=0.9982\n",
      "Day 2/7: 2015-01-05-TOTF.PA-book.csv.gz\n",
      "--------------------------------------------------------------------------------\n",
      "Loading data from ../data/TOTF.PA-book\\2015-01-05-TOTF.PA-book.csv.gz...\n",
      "Successfully loaded 1202740 rows.\n",
      "Filtered 1920 pre-market samples.\n",
      "First hour samples: 113192, Rest of day: 1087628\n",
      "Training on 3 time blocks...\n",
      "Block 1/3: Train=24352, Val=30164\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Transformer Autoencoder...\n",
      "Training Autoencoder (Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 27.047735 | Val Loss: 12.679100\n",
      "Validation loss decreased (inf --> 12.679100).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 7.001117 | Val Loss: 2.001260\n",
      "Validation loss decreased (12.679100 --> 2.001260).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 1.864258 | Val Loss: 0.978908\n",
      "Validation loss decreased (2.001260 --> 0.978908).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 1.350753 | Val Loss: 0.929809\n",
      "Validation loss decreased (0.978908 --> 0.929809).  Saving model ...\n",
      "Epoch 5/500 - Train Loss: 1.137737 | Val Loss: 0.913381\n",
      "Validation loss decreased (0.929809 --> 0.913381).  Saving model ...\n",
      "Epoch 6/500 - Train Loss: 0.993959 | Val Loss: 0.894419\n",
      "Validation loss decreased (0.913381 --> 0.894419).  Saving model ...\n",
      "Epoch 7/500 - Train Loss: 0.904935 | Val Loss: 0.851886\n",
      "Validation loss decreased (0.894419 --> 0.851886).  Saving model ...\n",
      "Epoch 8/500 - Train Loss: 0.843660 | Val Loss: 0.816825\n",
      "Validation loss decreased (0.851886 --> 0.816825).  Saving model ...\n",
      "Epoch 9/500 - Train Loss: 0.787150 | Val Loss: 0.800774\n",
      "Validation loss decreased (0.816825 --> 0.800774).  Saving model ...\n",
      "Epoch 10/500 - Train Loss: 0.726509 | Val Loss: 0.842076\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 11/500 - Train Loss: 0.666828 | Val Loss: 0.792336\n",
      "Validation loss decreased (0.800774 --> 0.792336).  Saving model ...\n",
      "Epoch 12/500 - Train Loss: 0.612341 | Val Loss: 0.745826\n",
      "Validation loss decreased (0.792336 --> 0.745826).  Saving model ...\n",
      "Epoch 13/500 - Train Loss: 0.561838 | Val Loss: 0.746934\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 14/500 - Train Loss: 0.521617 | Val Loss: 0.709966\n",
      "Validation loss decreased (0.745826 --> 0.709966).  Saving model ...\n",
      "Epoch 15/500 - Train Loss: 0.481662 | Val Loss: 0.680406\n",
      "Validation loss decreased (0.709966 --> 0.680406).  Saving model ...\n",
      "Epoch 16/500 - Train Loss: 0.448164 | Val Loss: 0.699789\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 17/500 - Train Loss: 0.417917 | Val Loss: 0.673024\n",
      "Validation loss decreased (0.680406 --> 0.673024).  Saving model ...\n",
      "Epoch 18/500 - Train Loss: 0.391076 | Val Loss: 0.684359\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 19/500 - Train Loss: 0.363839 | Val Loss: 0.619015\n",
      "Validation loss decreased (0.673024 --> 0.619015).  Saving model ...\n",
      "Epoch 20/500 - Train Loss: 0.337796 | Val Loss: 0.584899\n",
      "Validation loss decreased (0.619015 --> 0.584899).  Saving model ...\n",
      "Epoch 21/500 - Train Loss: 0.315316 | Val Loss: 0.565964\n",
      "Validation loss decreased (0.584899 --> 0.565964).  Saving model ...\n",
      "Epoch 22/500 - Train Loss: 0.300722 | Val Loss: 0.564199\n",
      "Validation loss decreased (0.565964 --> 0.564199).  Saving model ...\n",
      "Epoch 23/500 - Train Loss: 0.285378 | Val Loss: 0.542822\n",
      "Validation loss decreased (0.564199 --> 0.542822).  Saving model ...\n",
      "Epoch 24/500 - Train Loss: 0.267487 | Val Loss: 0.540267\n",
      "Validation loss decreased (0.542822 --> 0.540267).  Saving model ...\n",
      "Epoch 25/500 - Train Loss: 0.252398 | Val Loss: 0.547273\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 26/500 - Train Loss: 0.242532 | Val Loss: 0.535537\n",
      "Validation loss decreased (0.540267 --> 0.535537).  Saving model ...\n",
      "Epoch 27/500 - Train Loss: 0.225580 | Val Loss: 0.532878\n",
      "Validation loss decreased (0.535537 --> 0.532878).  Saving model ...\n",
      "Epoch 28/500 - Train Loss: 0.220208 | Val Loss: 0.525717\n",
      "Validation loss decreased (0.532878 --> 0.525717).  Saving model ...\n",
      "Epoch 29/500 - Train Loss: 0.207802 | Val Loss: 0.511667\n",
      "Validation loss decreased (0.525717 --> 0.511667).  Saving model ...\n",
      "Epoch 30/500 - Train Loss: 0.209075 | Val Loss: 0.498684\n",
      "Validation loss decreased (0.511667 --> 0.498684).  Saving model ...\n",
      "Epoch 31/500 - Train Loss: 0.193720 | Val Loss: 0.490489\n",
      "Validation loss decreased (0.498684 --> 0.490489).  Saving model ...\n",
      "Epoch 32/500 - Train Loss: 0.184706 | Val Loss: 0.518727\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 33/500 - Train Loss: 0.180759 | Val Loss: 0.490839\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 34/500 - Train Loss: 0.174838 | Val Loss: 0.483047\n",
      "Validation loss decreased (0.490489 --> 0.483047).  Saving model ...\n",
      "Epoch 35/500 - Train Loss: 0.171895 | Val Loss: 0.498108\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 36/500 - Train Loss: 0.174540 | Val Loss: 0.493182\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 37/500 - Train Loss: 0.178597 | Val Loss: 0.499365\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 38/500 - Train Loss: 0.157765 | Val Loss: 0.477665\n",
      "Validation loss decreased (0.483047 --> 0.477665).  Saving model ...\n",
      "Epoch 39/500 - Train Loss: 0.153188 | Val Loss: 0.466449\n",
      "Validation loss decreased (0.477665 --> 0.466449).  Saving model ...\n",
      "Epoch 40/500 - Train Loss: 0.150822 | Val Loss: 0.476720\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 41/500 - Train Loss: 0.147252 | Val Loss: 0.466513\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 42/500 - Train Loss: 0.142705 | Val Loss: 0.476351\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 43/500 - Train Loss: 0.139182 | Val Loss: 0.468473\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 44/500 - Train Loss: 0.136545 | Val Loss: 0.463456\n",
      "Validation loss decreased (0.466449 --> 0.463456).  Saving model ...\n",
      "Epoch 45/500 - Train Loss: 0.133132 | Val Loss: 0.445121\n",
      "Validation loss decreased (0.463456 --> 0.445121).  Saving model ...\n",
      "Epoch 46/500 - Train Loss: 0.130855 | Val Loss: 0.464298\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 47/500 - Train Loss: 0.128171 | Val Loss: 0.468871\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 48/500 - Train Loss: 0.125361 | Val Loss: 0.441078\n",
      "Validation loss decreased (0.445121 --> 0.441078).  Saving model ...\n",
      "Epoch 49/500 - Train Loss: 0.131302 | Val Loss: 0.469928\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 50/500 - Train Loss: 0.127082 | Val Loss: 0.449289\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 51/500 - Train Loss: 0.132356 | Val Loss: 0.455856\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 52/500 - Train Loss: 0.118465 | Val Loss: 0.456974\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 53/500 - Train Loss: 0.114342 | Val Loss: 0.450680\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Extracting Latent Representations for OC-SVM...\n",
      "Training One-Class SVM (nu=0.01)...\n",
      "Block 2/3: Train=20778, Val=12683\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Transformer Autoencoder...\n",
      "Training Autoencoder (Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 17.805764 | Val Loss: 6.679554\n",
      "Validation loss decreased (inf --> 6.679554).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 4.639589 | Val Loss: 0.847726\n",
      "Validation loss decreased (6.679554 --> 0.847726).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 1.107749 | Val Loss: 0.588642\n",
      "Validation loss decreased (0.847726 --> 0.588642).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 0.808812 | Val Loss: 0.686000\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 5/500 - Train Loss: 0.800518 | Val Loss: 0.710625\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 6/500 - Train Loss: 0.799481 | Val Loss: 0.707081\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 7/500 - Train Loss: 0.798988 | Val Loss: 0.700249\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 8/500 - Train Loss: 0.795648 | Val Loss: 0.614754\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Extracting Latent Representations for OC-SVM...\n",
      "Training One-Class SVM (nu=0.01)...\n",
      "Block 3/3: Train=13412, Val=11803\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Transformer Autoencoder...\n",
      "Training Autoencoder (Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 14.583748 | Val Loss: 12.230111\n",
      "Validation loss decreased (inf --> 12.230111).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 6.646570 | Val Loss: 5.189185\n",
      "Validation loss decreased (12.230111 --> 5.189185).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 2.434644 | Val Loss: 1.994238\n",
      "Validation loss decreased (5.189185 --> 1.994238).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 1.096568 | Val Loss: 1.041817\n",
      "Validation loss decreased (1.994238 --> 1.041817).  Saving model ...\n",
      "Epoch 5/500 - Train Loss: 0.841821 | Val Loss: 0.817339\n",
      "Validation loss decreased (1.041817 --> 0.817339).  Saving model ...\n",
      "Epoch 6/500 - Train Loss: 0.792987 | Val Loss: 0.741974\n",
      "Validation loss decreased (0.817339 --> 0.741974).  Saving model ...\n",
      "Epoch 7/500 - Train Loss: 0.743444 | Val Loss: 0.727795\n",
      "Validation loss decreased (0.741974 --> 0.727795).  Saving model ...\n",
      "Epoch 8/500 - Train Loss: 0.710344 | Val Loss: 0.702488\n",
      "Validation loss decreased (0.727795 --> 0.702488).  Saving model ...\n",
      "Epoch 9/500 - Train Loss: 0.681026 | Val Loss: 0.652769\n",
      "Validation loss decreased (0.702488 --> 0.652769).  Saving model ...\n",
      "Epoch 10/500 - Train Loss: 0.643664 | Val Loss: 0.631159\n",
      "Validation loss decreased (0.652769 --> 0.631159).  Saving model ...\n",
      "Epoch 11/500 - Train Loss: 0.603939 | Val Loss: 0.687521\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 12/500 - Train Loss: 0.566496 | Val Loss: 0.595911\n",
      "Validation loss decreased (0.631159 --> 0.595911).  Saving model ...\n",
      "Epoch 13/500 - Train Loss: 0.534082 | Val Loss: 0.568072\n",
      "Validation loss decreased (0.595911 --> 0.568072).  Saving model ...\n",
      "Epoch 14/500 - Train Loss: 0.502926 | Val Loss: 0.557256\n",
      "Validation loss decreased (0.568072 --> 0.557256).  Saving model ...\n",
      "Epoch 15/500 - Train Loss: 0.475129 | Val Loss: 0.548944\n",
      "Validation loss decreased (0.557256 --> 0.548944).  Saving model ...\n",
      "Epoch 16/500 - Train Loss: 0.447894 | Val Loss: 0.506724\n",
      "Validation loss decreased (0.548944 --> 0.506724).  Saving model ...\n",
      "Epoch 17/500 - Train Loss: 0.419174 | Val Loss: 0.535172\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 18/500 - Train Loss: 0.388761 | Val Loss: 0.478702\n",
      "Validation loss decreased (0.506724 --> 0.478702).  Saving model ...\n",
      "Epoch 19/500 - Train Loss: 0.361851 | Val Loss: 0.497703\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 20/500 - Train Loss: 0.337982 | Val Loss: 0.453260\n",
      "Validation loss decreased (0.478702 --> 0.453260).  Saving model ...\n",
      "Epoch 21/500 - Train Loss: 0.317615 | Val Loss: 0.459224\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 22/500 - Train Loss: 0.299110 | Val Loss: 0.437652\n",
      "Validation loss decreased (0.453260 --> 0.437652).  Saving model ...\n",
      "Epoch 23/500 - Train Loss: 0.282020 | Val Loss: 0.438259\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 24/500 - Train Loss: 0.264345 | Val Loss: 0.430756\n",
      "Validation loss decreased (0.437652 --> 0.430756).  Saving model ...\n",
      "Epoch 25/500 - Train Loss: 0.251004 | Val Loss: 0.435320\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 26/500 - Train Loss: 0.238021 | Val Loss: 0.426603\n",
      "Validation loss decreased (0.430756 --> 0.426603).  Saving model ...\n",
      "Epoch 27/500 - Train Loss: 0.226224 | Val Loss: 0.422671\n",
      "Validation loss decreased (0.426603 --> 0.422671).  Saving model ...\n",
      "Epoch 28/500 - Train Loss: 0.214995 | Val Loss: 0.395644\n",
      "Validation loss decreased (0.422671 --> 0.395644).  Saving model ...\n",
      "Epoch 29/500 - Train Loss: 0.204351 | Val Loss: 0.404057\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 30/500 - Train Loss: 0.195439 | Val Loss: 0.385571\n",
      "Validation loss decreased (0.395644 --> 0.385571).  Saving model ...\n",
      "Epoch 31/500 - Train Loss: 0.187284 | Val Loss: 0.407017\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 32/500 - Train Loss: 0.179855 | Val Loss: 0.369725\n",
      "Validation loss decreased (0.385571 --> 0.369725).  Saving model ...\n",
      "Epoch 33/500 - Train Loss: 0.172954 | Val Loss: 0.391210\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 34/500 - Train Loss: 0.165360 | Val Loss: 0.399999\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 35/500 - Train Loss: 0.158273 | Val Loss: 0.381078\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 36/500 - Train Loss: 0.154062 | Val Loss: 0.377842\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 37/500 - Train Loss: 0.148923 | Val Loss: 0.362458\n",
      "Validation loss decreased (0.369725 --> 0.362458).  Saving model ...\n",
      "Epoch 38/500 - Train Loss: 0.143580 | Val Loss: 0.376387\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 39/500 - Train Loss: 0.138667 | Val Loss: 0.375280\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 40/500 - Train Loss: 0.134191 | Val Loss: 0.369767\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 41/500 - Train Loss: 0.129280 | Val Loss: 0.386340\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 42/500 - Train Loss: 0.125583 | Val Loss: 0.367836\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Extracting Latent Representations for OC-SVM...\n",
      "Training One-Class SVM (nu=0.01)...\n",
      "\n",
      "Evaluating on rest of Day 2...\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Evaluating model...\n",
      "Day 2 Results: AUROC=1.0000, AUPRC=1.0000, F4=0.9989\n",
      "Day 3/7: 2015-01-06-TOTF.PA-book.csv.gz\n",
      "--------------------------------------------------------------------------------\n",
      "Loading data from ../data/TOTF.PA-book\\2015-01-06-TOTF.PA-book.csv.gz...\n",
      "Successfully loaded 1272556 rows.\n",
      "Filtered 1896 pre-market samples.\n",
      "First hour samples: 170233, Rest of day: 1100427\n",
      "Training on 3 time blocks...\n",
      "Block 1/3: Train=30901, Val=36877\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Transformer Autoencoder...\n",
      "Training Autoencoder (Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 206.606346 | Val Loss: 168.580442\n",
      "Validation loss decreased (inf --> 168.580442).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 95.946089 | Val Loss: 63.935029\n",
      "Validation loss decreased (168.580442 --> 63.935029).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 26.405147 | Val Loss: 15.436196\n",
      "Validation loss decreased (63.935029 --> 15.436196).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 4.532858 | Val Loss: 4.091058\n",
      "Validation loss decreased (15.436196 --> 4.091058).  Saving model ...\n",
      "Epoch 5/500 - Train Loss: 1.407360 | Val Loss: 2.403836\n",
      "Validation loss decreased (4.091058 --> 2.403836).  Saving model ...\n",
      "Epoch 6/500 - Train Loss: 1.180238 | Val Loss: 2.128870\n",
      "Validation loss decreased (2.403836 --> 2.128870).  Saving model ...\n",
      "Epoch 7/500 - Train Loss: 1.167844 | Val Loss: 2.126639\n",
      "Validation loss decreased (2.128870 --> 2.126639).  Saving model ...\n",
      "Epoch 8/500 - Train Loss: 1.167156 | Val Loss: 2.161842\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 9/500 - Train Loss: 1.167155 | Val Loss: 2.112102\n",
      "Validation loss decreased (2.126639 --> 2.112102).  Saving model ...\n",
      "Epoch 10/500 - Train Loss: 1.166947 | Val Loss: 2.114363\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 11/500 - Train Loss: 1.166856 | Val Loss: 2.105941\n",
      "Validation loss decreased (2.112102 --> 2.105941).  Saving model ...\n",
      "Epoch 12/500 - Train Loss: 1.166612 | Val Loss: 2.176531\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 13/500 - Train Loss: 1.166956 | Val Loss: 2.154253\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 14/500 - Train Loss: 1.161938 | Val Loss: 2.005631\n",
      "Validation loss decreased (2.105941 --> 2.005631).  Saving model ...\n",
      "Epoch 15/500 - Train Loss: 0.999888 | Val Loss: 1.614597\n",
      "Validation loss decreased (2.005631 --> 1.614597).  Saving model ...\n",
      "Epoch 16/500 - Train Loss: 0.928957 | Val Loss: 1.643082\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 17/500 - Train Loss: 0.901743 | Val Loss: 1.642053\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 18/500 - Train Loss: 0.873705 | Val Loss: 1.619284\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 19/500 - Train Loss: 0.852827 | Val Loss: 1.692207\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 20/500 - Train Loss: 0.832659 | Val Loss: 1.737870\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Extracting Latent Representations for OC-SVM...\n",
      "Training One-Class SVM (nu=0.01)...\n",
      "Block 2/3: Train=23621, Val=28400\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Transformer Autoencoder...\n",
      "Training Autoencoder (Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 231.958055 | Val Loss: 240.626192\n",
      "Validation loss decreased (inf --> 240.626192).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 143.840646 | Val Loss: 141.970205\n",
      "Validation loss decreased (240.626192 --> 141.970205).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 68.833448 | Val Loss: 67.371701\n",
      "Validation loss decreased (141.970205 --> 67.371701).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 23.838800 | Val Loss: 27.034785\n",
      "Validation loss decreased (67.371701 --> 27.034785).  Saving model ...\n",
      "Epoch 5/500 - Train Loss: 6.043312 | Val Loss: 11.390041\n",
      "Validation loss decreased (27.034785 --> 11.390041).  Saving model ...\n",
      "Epoch 6/500 - Train Loss: 1.582569 | Val Loss: 6.569203\n",
      "Validation loss decreased (11.390041 --> 6.569203).  Saving model ...\n",
      "Epoch 7/500 - Train Loss: 0.838945 | Val Loss: 5.225012\n",
      "Validation loss decreased (6.569203 --> 5.225012).  Saving model ...\n",
      "Epoch 8/500 - Train Loss: 0.743979 | Val Loss: 4.870044\n",
      "Validation loss decreased (5.225012 --> 4.870044).  Saving model ...\n",
      "Epoch 9/500 - Train Loss: 0.733221 | Val Loss: 4.774507\n",
      "Validation loss decreased (4.870044 --> 4.774507).  Saving model ...\n",
      "Epoch 10/500 - Train Loss: 0.732960 | Val Loss: 4.761495\n",
      "Validation loss decreased (4.774507 --> 4.761495).  Saving model ...\n",
      "Epoch 11/500 - Train Loss: 0.732406 | Val Loss: 4.715037\n",
      "Validation loss decreased (4.761495 --> 4.715037).  Saving model ...\n",
      "Epoch 12/500 - Train Loss: 0.732277 | Val Loss: 4.789758\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 13/500 - Train Loss: 0.732266 | Val Loss: 4.709583\n",
      "Validation loss decreased (4.715037 --> 4.709583).  Saving model ...\n",
      "Epoch 14/500 - Train Loss: 0.732816 | Val Loss: 4.649857\n",
      "Validation loss decreased (4.709583 --> 4.649857).  Saving model ...\n",
      "Epoch 15/500 - Train Loss: 0.732961 | Val Loss: 4.694783\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 16/500 - Train Loss: 0.732295 | Val Loss: 4.708817\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 17/500 - Train Loss: 0.732347 | Val Loss: 4.764589\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 18/500 - Train Loss: 0.732266 | Val Loss: 4.728433\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 19/500 - Train Loss: 0.732299 | Val Loss: 4.756532\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Extracting Latent Representations for OC-SVM...\n",
      "Training One-Class SVM (nu=0.01)...\n",
      "Block 3/3: Train=30334, Val=20100\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Transformer Autoencoder...\n",
      "Training Autoencoder (Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 306.071292 | Val Loss: 216.666695\n",
      "Validation loss decreased (inf --> 216.666695).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 164.534327 | Val Loss: 90.261858\n",
      "Validation loss decreased (216.666695 --> 90.261858).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 58.866423 | Val Loss: 21.375068\n",
      "Validation loss decreased (90.261858 --> 21.375068).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 12.977382 | Val Loss: 2.677215\n",
      "Validation loss decreased (21.375068 --> 2.677215).  Saving model ...\n",
      "Epoch 5/500 - Train Loss: 2.340282 | Val Loss: 0.778036\n",
      "Validation loss decreased (2.677215 --> 0.778036).  Saving model ...\n",
      "Epoch 6/500 - Train Loss: 0.985393 | Val Loss: 0.885120\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 7/500 - Train Loss: 0.867350 | Val Loss: 0.965097\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 8/500 - Train Loss: 0.859469 | Val Loss: 0.983304\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 9/500 - Train Loss: 0.858304 | Val Loss: 0.987585\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 10/500 - Train Loss: 0.858972 | Val Loss: 1.007091\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Extracting Latent Representations for OC-SVM...\n",
      "Training One-Class SVM (nu=0.01)...\n",
      "\n",
      "Evaluating on rest of Day 3...\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Evaluating model...\n",
      "Day 3 Results: AUROC=0.9991, AUPRC=0.9982, F4=0.9635\n",
      "Day 4/7: 2015-01-07-TOTF.PA-book.csv.gz\n",
      "--------------------------------------------------------------------------------\n",
      "Loading data from ../data/TOTF.PA-book\\2015-01-07-TOTF.PA-book.csv.gz...\n",
      "Successfully loaded 1124864 rows.\n",
      "Filtered 2053 pre-market samples.\n",
      "First hour samples: 128092, Rest of day: 994719\n",
      "Training on 3 time blocks...\n",
      "Block 1/3: Train=19569, Val=27024\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Transformer Autoencoder...\n",
      "Training Autoencoder (Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 283.278717 | Val Loss: 238.761948\n",
      "Validation loss decreased (inf --> 238.761948).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 205.087231 | Val Loss: 159.827162\n",
      "Validation loss decreased (238.761948 --> 159.827162).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 128.638769 | Val Loss: 90.642698\n",
      "Validation loss decreased (159.827162 --> 90.642698).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 67.783109 | Val Loss: 41.875242\n",
      "Validation loss decreased (90.642698 --> 41.875242).  Saving model ...\n",
      "Epoch 5/500 - Train Loss: 29.135673 | Val Loss: 15.333152\n",
      "Validation loss decreased (41.875242 --> 15.333152).  Saving model ...\n",
      "Epoch 6/500 - Train Loss: 10.230462 | Val Loss: 4.615268\n",
      "Validation loss decreased (15.333152 --> 4.615268).  Saving model ...\n",
      "Epoch 7/500 - Train Loss: 3.231445 | Val Loss: 1.477319\n",
      "Validation loss decreased (4.615268 --> 1.477319).  Saving model ...\n",
      "Epoch 8/500 - Train Loss: 1.235895 | Val Loss: 0.844052\n",
      "Validation loss decreased (1.477319 --> 0.844052).  Saving model ...\n",
      "Epoch 9/500 - Train Loss: 0.768558 | Val Loss: 0.763185\n",
      "Validation loss decreased (0.844052 --> 0.763185).  Saving model ...\n",
      "Epoch 10/500 - Train Loss: 0.646600 | Val Loss: 0.731488\n",
      "Validation loss decreased (0.763185 --> 0.731488).  Saving model ...\n",
      "Epoch 11/500 - Train Loss: 0.582871 | Val Loss: 0.666753\n",
      "Validation loss decreased (0.731488 --> 0.666753).  Saving model ...\n",
      "Epoch 12/500 - Train Loss: 0.556398 | Val Loss: 0.697836\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 13/500 - Train Loss: 0.547546 | Val Loss: 0.701716\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 14/500 - Train Loss: 0.540909 | Val Loss: 0.646612\n",
      "Validation loss decreased (0.666753 --> 0.646612).  Saving model ...\n",
      "Epoch 15/500 - Train Loss: 0.526614 | Val Loss: 0.724583\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 16/500 - Train Loss: 0.507218 | Val Loss: 0.735630\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 17/500 - Train Loss: 0.491954 | Val Loss: 0.715205\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 18/500 - Train Loss: 0.476507 | Val Loss: 0.727251\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 19/500 - Train Loss: 0.455960 | Val Loss: 0.689120\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Extracting Latent Representations for OC-SVM...\n",
      "Training One-Class SVM (nu=0.01)...\n",
      "Block 2/3: Train=27589, Val=28408\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Transformer Autoencoder...\n",
      "Training Autoencoder (Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 216.298645 | Val Loss: 130.892987\n",
      "Validation loss decreased (inf --> 130.892987).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 114.509800 | Val Loss: 49.656755\n",
      "Validation loss decreased (130.892987 --> 49.656755).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 40.720544 | Val Loss: 9.667438\n",
      "Validation loss decreased (49.656755 --> 9.667438).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 9.264008 | Val Loss: 1.784264\n",
      "Validation loss decreased (9.667438 --> 1.784264).  Saving model ...\n",
      "Epoch 5/500 - Train Loss: 2.052175 | Val Loss: 2.541035\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 6/500 - Train Loss: 1.146469 | Val Loss: 3.192500\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 7/500 - Train Loss: 1.070988 | Val Loss: 3.374990\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 8/500 - Train Loss: 1.065831 | Val Loss: 3.461653\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 9/500 - Train Loss: 1.066279 | Val Loss: 3.484869\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Extracting Latent Representations for OC-SVM...\n",
      "Training One-Class SVM (nu=0.01)...\n",
      "Block 3/3: Train=14055, Val=11447\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Transformer Autoencoder...\n",
      "Training Autoencoder (Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 229.498026 | Val Loss: 208.552419\n",
      "Validation loss decreased (inf --> 208.552419).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 185.257089 | Val Loss: 162.134697\n",
      "Validation loss decreased (208.552419 --> 162.134697).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 138.830924 | Val Loss: 116.872119\n",
      "Validation loss decreased (162.134697 --> 116.872119).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 96.154623 | Val Loss: 77.320607\n",
      "Validation loss decreased (116.872119 --> 77.320607).  Saving model ...\n",
      "Epoch 5/500 - Train Loss: 60.772606 | Val Loss: 46.376102\n",
      "Validation loss decreased (77.320607 --> 46.376102).  Saving model ...\n",
      "Epoch 6/500 - Train Loss: 34.640131 | Val Loss: 25.004165\n",
      "Validation loss decreased (46.376102 --> 25.004165).  Saving model ...\n",
      "Epoch 7/500 - Train Loss: 17.725116 | Val Loss: 12.194038\n",
      "Validation loss decreased (25.004165 --> 12.194038).  Saving model ...\n",
      "Epoch 8/500 - Train Loss: 8.248494 | Val Loss: 5.578076\n",
      "Validation loss decreased (12.194038 --> 5.578076).  Saving model ...\n",
      "Epoch 9/500 - Train Loss: 3.681809 | Val Loss: 2.642233\n",
      "Validation loss decreased (5.578076 --> 2.642233).  Saving model ...\n",
      "Epoch 10/500 - Train Loss: 1.770950 | Val Loss: 1.501029\n",
      "Validation loss decreased (2.642233 --> 1.501029).  Saving model ...\n",
      "Epoch 11/500 - Train Loss: 1.064511 | Val Loss: 1.084951\n",
      "Validation loss decreased (1.501029 --> 1.084951).  Saving model ...\n",
      "Epoch 12/500 - Train Loss: 0.826855 | Val Loss: 0.950463\n",
      "Validation loss decreased (1.084951 --> 0.950463).  Saving model ...\n",
      "Epoch 13/500 - Train Loss: 0.749179 | Val Loss: 0.915316\n",
      "Validation loss decreased (0.950463 --> 0.915316).  Saving model ...\n",
      "Epoch 14/500 - Train Loss: 0.728265 | Val Loss: 0.906061\n",
      "Validation loss decreased (0.915316 --> 0.906061).  Saving model ...\n",
      "Epoch 15/500 - Train Loss: 0.718947 | Val Loss: 0.903516\n",
      "Validation loss decreased (0.906061 --> 0.903516).  Saving model ...\n",
      "Epoch 16/500 - Train Loss: 0.718838 | Val Loss: 0.893030\n",
      "Validation loss decreased (0.903516 --> 0.893030).  Saving model ...\n",
      "Epoch 17/500 - Train Loss: 0.716962 | Val Loss: 0.899862\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 18/500 - Train Loss: 0.717056 | Val Loss: 0.907041\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 19/500 - Train Loss: 0.716721 | Val Loss: 0.891576\n",
      "Validation loss decreased (0.893030 --> 0.891576).  Saving model ...\n",
      "Epoch 20/500 - Train Loss: 0.717496 | Val Loss: 0.895854\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 21/500 - Train Loss: 0.716438 | Val Loss: 0.907756\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 22/500 - Train Loss: 0.717283 | Val Loss: 0.905555\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 23/500 - Train Loss: 0.718527 | Val Loss: 0.902309\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 24/500 - Train Loss: 0.717815 | Val Loss: 0.894102\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Extracting Latent Representations for OC-SVM...\n",
      "Training One-Class SVM (nu=0.01)...\n",
      "\n",
      "Evaluating on rest of Day 4...\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Evaluating model...\n",
      "Day 4 Results: AUROC=0.9949, AUPRC=0.9943, F4=0.9541\n",
      "Day 5/7: 2015-01-08-TOTF.PA-book.csv.gz\n",
      "--------------------------------------------------------------------------------\n",
      "Loading data from ../data/TOTF.PA-book\\2015-01-08-TOTF.PA-book.csv.gz...\n",
      "Successfully loaded 1101381 rows.\n",
      "Filtered 1868 pre-market samples.\n",
      "First hour samples: 125163, Rest of day: 974350\n",
      "Training on 3 time blocks...\n",
      "Block 1/3: Train=17618, Val=32995\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Transformer Autoencoder...\n",
      "Training Autoencoder (Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 80.108757 | Val Loss: 79.941329\n",
      "Validation loss decreased (inf --> 79.941329).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 47.560796 | Val Loss: 45.442481\n",
      "Validation loss decreased (79.941329 --> 45.442481).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 22.312082 | Val Loss: 21.366756\n",
      "Validation loss decreased (45.442481 --> 21.366756).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 8.193917 | Val Loss: 9.086762\n",
      "Validation loss decreased (21.366756 --> 9.086762).  Saving model ...\n",
      "Epoch 5/500 - Train Loss: 2.786290 | Val Loss: 4.306285\n",
      "Validation loss decreased (9.086762 --> 4.306285).  Saving model ...\n",
      "Epoch 6/500 - Train Loss: 1.342946 | Val Loss: 2.770413\n",
      "Validation loss decreased (4.306285 --> 2.770413).  Saving model ...\n",
      "Epoch 7/500 - Train Loss: 1.002621 | Val Loss: 2.098721\n",
      "Validation loss decreased (2.770413 --> 2.098721).  Saving model ...\n",
      "Epoch 8/500 - Train Loss: 0.848680 | Val Loss: 2.128777\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 9/500 - Train Loss: 0.835941 | Val Loss: 1.917122\n",
      "Validation loss decreased (2.098721 --> 1.917122).  Saving model ...\n",
      "Epoch 10/500 - Train Loss: 0.780271 | Val Loss: 1.919344\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 11/500 - Train Loss: 0.761344 | Val Loss: 1.550761\n",
      "Validation loss decreased (1.917122 --> 1.550761).  Saving model ...\n",
      "Epoch 12/500 - Train Loss: 0.743871 | Val Loss: 1.722763\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 13/500 - Train Loss: 0.714569 | Val Loss: 1.469856\n",
      "Validation loss decreased (1.550761 --> 1.469856).  Saving model ...\n",
      "Epoch 14/500 - Train Loss: 0.704275 | Val Loss: 1.718058\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 15/500 - Train Loss: 0.682106 | Val Loss: 1.441588\n",
      "Validation loss decreased (1.469856 --> 1.441588).  Saving model ...\n",
      "Epoch 16/500 - Train Loss: 0.662057 | Val Loss: 1.543439\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 17/500 - Train Loss: 0.639450 | Val Loss: 1.532741\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 18/500 - Train Loss: 0.624543 | Val Loss: 1.542436\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 19/500 - Train Loss: 0.611182 | Val Loss: 1.587507\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 20/500 - Train Loss: 0.588126 | Val Loss: 1.957237\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Extracting Latent Representations for OC-SVM...\n",
      "Training One-Class SVM (nu=0.01)...\n",
      "Block 2/3: Train=32355, Val=16660\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Transformer Autoencoder...\n",
      "Training Autoencoder (Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 71.556462 | Val Loss: 26.513912\n",
      "Validation loss decreased (inf --> 26.513912).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 17.914263 | Val Loss: 1.788561\n",
      "Validation loss decreased (26.513912 --> 1.788561).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 1.931191 | Val Loss: 1.177473\n",
      "Validation loss decreased (1.788561 --> 1.177473).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 0.612755 | Val Loss: 1.754624\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 5/500 - Train Loss: 0.523260 | Val Loss: 1.902104\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 6/500 - Train Loss: 0.460653 | Val Loss: 1.773789\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 7/500 - Train Loss: 0.417081 | Val Loss: 1.604266\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 8/500 - Train Loss: 0.378380 | Val Loss: 1.684777\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Extracting Latent Representations for OC-SVM...\n",
      "Training One-Class SVM (nu=0.01)...\n",
      "Block 3/3: Train=11364, Val=14171\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Transformer Autoencoder...\n",
      "Training Autoencoder (Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 92.958571 | Val Loss: 72.893775\n",
      "Validation loss decreased (inf --> 72.893775).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 73.105012 | Val Loss: 54.258974\n",
      "Validation loss decreased (72.893775 --> 54.258974).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 53.499901 | Val Loss: 37.207039\n",
      "Validation loss decreased (54.258974 --> 37.207039).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 36.349593 | Val Loss: 23.148526\n",
      "Validation loss decreased (37.207039 --> 23.148526).  Saving model ...\n",
      "Epoch 5/500 - Train Loss: 22.615484 | Val Loss: 12.811765\n",
      "Validation loss decreased (23.148526 --> 12.811765).  Saving model ...\n",
      "Epoch 6/500 - Train Loss: 12.764859 | Val Loss: 6.207351\n",
      "Validation loss decreased (12.811765 --> 6.207351).  Saving model ...\n",
      "Epoch 7/500 - Train Loss: 6.544921 | Val Loss: 2.657982\n",
      "Validation loss decreased (6.207351 --> 2.657982).  Saving model ...\n",
      "Epoch 8/500 - Train Loss: 3.123122 | Val Loss: 1.135175\n",
      "Validation loss decreased (2.657982 --> 1.135175).  Saving model ...\n",
      "Epoch 9/500 - Train Loss: 1.495573 | Val Loss: 0.679205\n",
      "Validation loss decreased (1.135175 --> 0.679205).  Saving model ...\n",
      "Epoch 10/500 - Train Loss: 0.814257 | Val Loss: 0.648479\n",
      "Validation loss decreased (0.679205 --> 0.648479).  Saving model ...\n",
      "Epoch 11/500 - Train Loss: 0.563069 | Val Loss: 0.725425\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 12/500 - Train Loss: 0.477467 | Val Loss: 0.800883\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 13/500 - Train Loss: 0.447518 | Val Loss: 0.851709\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 14/500 - Train Loss: 0.439566 | Val Loss: 0.885111\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 15/500 - Train Loss: 0.437125 | Val Loss: 0.886394\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Extracting Latent Representations for OC-SVM...\n",
      "Training One-Class SVM (nu=0.01)...\n",
      "\n",
      "Evaluating on rest of Day 5...\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Evaluating model...\n",
      "Day 5 Results: AUROC=0.9162, AUPRC=0.8569, F4=0.9448\n",
      "Day 6/7: 2015-01-09-TOTF.PA-book.csv.gz\n",
      "--------------------------------------------------------------------------------\n",
      "Loading data from ../data/TOTF.PA-book\\2015-01-09-TOTF.PA-book.csv.gz...\n",
      "Successfully loaded 1278519 rows.\n",
      "Filtered 2045 pre-market samples.\n",
      "First hour samples: 107573, Rest of day: 1168901\n",
      "Training on 3 time blocks...\n",
      "Block 1/3: Train=19152, Val=20617\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Transformer Autoencoder...\n",
      "Training Autoencoder (Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 17.705720 | Val Loss: 7.956499\n",
      "Validation loss decreased (inf --> 7.956499).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 5.464986 | Val Loss: 1.474484\n",
      "Validation loss decreased (7.956499 --> 1.474484).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 1.546336 | Val Loss: 0.682953\n",
      "Validation loss decreased (1.474484 --> 0.682953).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 0.987557 | Val Loss: 0.616319\n",
      "Validation loss decreased (0.682953 --> 0.616319).  Saving model ...\n",
      "Epoch 5/500 - Train Loss: 0.855325 | Val Loss: 0.637329\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 6/500 - Train Loss: 0.784372 | Val Loss: 0.611414\n",
      "Validation loss decreased (0.616319 --> 0.611414).  Saving model ...\n",
      "Epoch 7/500 - Train Loss: 0.721481 | Val Loss: 0.603727\n",
      "Validation loss decreased (0.611414 --> 0.603727).  Saving model ...\n",
      "Epoch 8/500 - Train Loss: 0.671022 | Val Loss: 0.612142\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 9/500 - Train Loss: 0.628910 | Val Loss: 0.559079\n",
      "Validation loss decreased (0.603727 --> 0.559079).  Saving model ...\n",
      "Epoch 10/500 - Train Loss: 0.588959 | Val Loss: 0.550454\n",
      "Validation loss decreased (0.559079 --> 0.550454).  Saving model ...\n",
      "Epoch 11/500 - Train Loss: 0.547620 | Val Loss: 0.532384\n",
      "Validation loss decreased (0.550454 --> 0.532384).  Saving model ...\n",
      "Epoch 12/500 - Train Loss: 0.508372 | Val Loss: 0.535759\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 13/500 - Train Loss: 0.476783 | Val Loss: 0.526986\n",
      "Validation loss decreased (0.532384 --> 0.526986).  Saving model ...\n",
      "Epoch 14/500 - Train Loss: 0.444946 | Val Loss: 0.535942\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 15/500 - Train Loss: 0.416688 | Val Loss: 0.494009\n",
      "Validation loss decreased (0.526986 --> 0.494009).  Saving model ...\n",
      "Epoch 16/500 - Train Loss: 0.386969 | Val Loss: 0.496138\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 17/500 - Train Loss: 0.357034 | Val Loss: 0.482291\n",
      "Validation loss decreased (0.494009 --> 0.482291).  Saving model ...\n",
      "Epoch 18/500 - Train Loss: 0.333257 | Val Loss: 0.468190\n",
      "Validation loss decreased (0.482291 --> 0.468190).  Saving model ...\n",
      "Epoch 19/500 - Train Loss: 0.311502 | Val Loss: 0.455003\n",
      "Validation loss decreased (0.468190 --> 0.455003).  Saving model ...\n",
      "Epoch 20/500 - Train Loss: 0.293254 | Val Loss: 0.454554\n",
      "Validation loss decreased (0.455003 --> 0.454554).  Saving model ...\n",
      "Epoch 21/500 - Train Loss: 0.277250 | Val Loss: 0.432929\n",
      "Validation loss decreased (0.454554 --> 0.432929).  Saving model ...\n",
      "Epoch 22/500 - Train Loss: 0.260849 | Val Loss: 0.424313\n",
      "Validation loss decreased (0.432929 --> 0.424313).  Saving model ...\n",
      "Epoch 23/500 - Train Loss: 0.244603 | Val Loss: 0.414316\n",
      "Validation loss decreased (0.424313 --> 0.414316).  Saving model ...\n",
      "Epoch 24/500 - Train Loss: 0.228940 | Val Loss: 0.420867\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 25/500 - Train Loss: 0.216489 | Val Loss: 0.423857\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 26/500 - Train Loss: 0.204864 | Val Loss: 0.398095\n",
      "Validation loss decreased (0.414316 --> 0.398095).  Saving model ...\n",
      "Epoch 27/500 - Train Loss: 0.195363 | Val Loss: 0.396798\n",
      "Validation loss decreased (0.398095 --> 0.396798).  Saving model ...\n",
      "Epoch 28/500 - Train Loss: 0.185692 | Val Loss: 0.387068\n",
      "Validation loss decreased (0.396798 --> 0.387068).  Saving model ...\n",
      "Epoch 29/500 - Train Loss: 0.178898 | Val Loss: 0.382761\n",
      "Validation loss decreased (0.387068 --> 0.382761).  Saving model ...\n",
      "Epoch 30/500 - Train Loss: 0.172270 | Val Loss: 0.379394\n",
      "Validation loss decreased (0.382761 --> 0.379394).  Saving model ...\n",
      "Epoch 31/500 - Train Loss: 0.165090 | Val Loss: 0.369656\n",
      "Validation loss decreased (0.379394 --> 0.369656).  Saving model ...\n",
      "Epoch 32/500 - Train Loss: 0.160201 | Val Loss: 0.362879\n",
      "Validation loss decreased (0.369656 --> 0.362879).  Saving model ...\n",
      "Epoch 33/500 - Train Loss: 0.155977 | Val Loss: 0.376339\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 34/500 - Train Loss: 0.150208 | Val Loss: 0.368758\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 35/500 - Train Loss: 0.144396 | Val Loss: 0.378111\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 36/500 - Train Loss: 0.142113 | Val Loss: 0.363665\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 37/500 - Train Loss: 0.138793 | Val Loss: 0.356144\n",
      "Validation loss decreased (0.362879 --> 0.356144).  Saving model ...\n",
      "Epoch 38/500 - Train Loss: 0.133032 | Val Loss: 0.358343\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 39/500 - Train Loss: 0.129299 | Val Loss: 0.354887\n",
      "Validation loss decreased (0.356144 --> 0.354887).  Saving model ...\n",
      "Epoch 40/500 - Train Loss: 0.126655 | Val Loss: 0.361063\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 41/500 - Train Loss: 0.125150 | Val Loss: 0.358551\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 42/500 - Train Loss: 0.120859 | Val Loss: 0.349140\n",
      "Validation loss decreased (0.354887 --> 0.349140).  Saving model ...\n",
      "Epoch 43/500 - Train Loss: 0.118471 | Val Loss: 0.351817\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 44/500 - Train Loss: 0.117249 | Val Loss: 0.343893\n",
      "Validation loss decreased (0.349140 --> 0.343893).  Saving model ...\n",
      "Epoch 45/500 - Train Loss: 0.115148 | Val Loss: 0.352919\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 46/500 - Train Loss: 0.113873 | Val Loss: 0.349659\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 47/500 - Train Loss: 0.110577 | Val Loss: 0.339850\n",
      "Validation loss decreased (0.343893 --> 0.339850).  Saving model ...\n",
      "Epoch 48/500 - Train Loss: 0.108802 | Val Loss: 0.346883\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 49/500 - Train Loss: 0.106981 | Val Loss: 0.345213\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 50/500 - Train Loss: 0.105342 | Val Loss: 0.347071\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 51/500 - Train Loss: 0.104107 | Val Loss: 0.336889\n",
      "Validation loss decreased (0.339850 --> 0.336889).  Saving model ...\n",
      "Epoch 52/500 - Train Loss: 0.107451 | Val Loss: 0.336510\n",
      "Validation loss decreased (0.336889 --> 0.336510).  Saving model ...\n",
      "Epoch 53/500 - Train Loss: 0.103601 | Val Loss: 0.341190\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 54/500 - Train Loss: 0.099546 | Val Loss: 0.341127\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 55/500 - Train Loss: 0.098825 | Val Loss: 0.346941\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 56/500 - Train Loss: 0.097035 | Val Loss: 0.334086\n",
      "Validation loss decreased (0.336510 --> 0.334086).  Saving model ...\n",
      "Epoch 57/500 - Train Loss: 0.095411 | Val Loss: 0.327739\n",
      "Validation loss decreased (0.334086 --> 0.327739).  Saving model ...\n",
      "Epoch 58/500 - Train Loss: 0.093942 | Val Loss: 0.330883\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 59/500 - Train Loss: 0.093068 | Val Loss: 0.342543\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 60/500 - Train Loss: 0.092009 | Val Loss: 0.337623\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 61/500 - Train Loss: 0.091096 | Val Loss: 0.330419\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 62/500 - Train Loss: 0.090597 | Val Loss: 0.334754\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Extracting Latent Representations for OC-SVM...\n",
      "Training One-Class SVM (nu=0.01)...\n",
      "Block 2/3: Train=16206, Val=22140\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Transformer Autoencoder...\n",
      "Training Autoencoder (Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 15.512312 | Val Loss: 5.472732\n",
      "Validation loss decreased (inf --> 5.472732).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 5.508268 | Val Loss: 1.041195\n",
      "Validation loss decreased (5.472732 --> 1.041195).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 1.354469 | Val Loss: 0.750637\n",
      "Validation loss decreased (1.041195 --> 0.750637).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 0.557061 | Val Loss: 1.105307\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 5/500 - Train Loss: 0.480671 | Val Loss: 1.151021\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 6/500 - Train Loss: 0.440602 | Val Loss: 1.154051\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 7/500 - Train Loss: 0.410849 | Val Loss: 1.134250\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 8/500 - Train Loss: 0.383095 | Val Loss: 1.097244\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Extracting Latent Representations for OC-SVM...\n",
      "Training One-Class SVM (nu=0.01)...\n",
      "Block 3/3: Train=13494, Val=15964\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Transformer Autoencoder...\n",
      "Training Autoencoder (Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 23.560408 | Val Loss: 15.872725\n",
      "Validation loss decreased (inf --> 15.872725).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 12.518323 | Val Loss: 7.019638\n",
      "Validation loss decreased (15.872725 --> 7.019638).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 5.192839 | Val Loss: 2.361700\n",
      "Validation loss decreased (7.019638 --> 2.361700).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 1.859819 | Val Loss: 0.828981\n",
      "Validation loss decreased (2.361700 --> 0.828981).  Saving model ...\n",
      "Epoch 5/500 - Train Loss: 0.862646 | Val Loss: 0.573830\n",
      "Validation loss decreased (0.828981 --> 0.573830).  Saving model ...\n",
      "Epoch 6/500 - Train Loss: 0.672001 | Val Loss: 0.575049\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 7/500 - Train Loss: 0.644302 | Val Loss: 0.592589\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 8/500 - Train Loss: 0.642576 | Val Loss: 0.576248\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 9/500 - Train Loss: 0.598809 | Val Loss: 0.607030\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 10/500 - Train Loss: 0.575671 | Val Loss: 0.623799\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Extracting Latent Representations for OC-SVM...\n",
      "Training One-Class SVM (nu=0.01)...\n",
      "\n",
      "Evaluating on rest of Day 6...\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Evaluating model...\n",
      "Day 6 Results: AUROC=0.9987, AUPRC=0.9960, F4=0.9861\n",
      "Day 7/7: 2015-01-12-TOTF.PA-book.csv.gz\n",
      "--------------------------------------------------------------------------------\n",
      "Loading data from ../data/TOTF.PA-book\\2015-01-12-TOTF.PA-book.csv.gz...\n",
      "Successfully loaded 1178388 rows.\n",
      "Filtered 2074 pre-market samples.\n",
      "First hour samples: 97467, Rest of day: 1078847\n",
      "Training on 3 time blocks...\n",
      "Block 1/3: Train=13396, Val=19727\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Transformer Autoencoder...\n",
      "Training Autoencoder (Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 95.160907 | Val Loss: 96.656876\n",
      "Validation loss decreased (inf --> 96.656876).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 70.125469 | Val Loss: 69.261635\n",
      "Validation loss decreased (96.656876 --> 69.261635).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 45.954116 | Val Loss: 44.839514\n",
      "Validation loss decreased (69.261635 --> 44.839514).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 26.685024 | Val Loss: 26.223202\n",
      "Validation loss decreased (44.839514 --> 26.223202).  Saving model ...\n",
      "Epoch 5/500 - Train Loss: 13.496572 | Val Loss: 14.022821\n",
      "Validation loss decreased (26.223202 --> 14.022821).  Saving model ...\n",
      "Epoch 6/500 - Train Loss: 5.960477 | Val Loss: 7.234928\n",
      "Validation loss decreased (14.022821 --> 7.234928).  Saving model ...\n",
      "Epoch 7/500 - Train Loss: 2.480052 | Val Loss: 3.991696\n",
      "Validation loss decreased (7.234928 --> 3.991696).  Saving model ...\n",
      "Epoch 8/500 - Train Loss: 1.129517 | Val Loss: 2.597313\n",
      "Validation loss decreased (3.991696 --> 2.597313).  Saving model ...\n",
      "Epoch 9/500 - Train Loss: 0.702849 | Val Loss: 1.995685\n",
      "Validation loss decreased (2.597313 --> 1.995685).  Saving model ...\n",
      "Epoch 10/500 - Train Loss: 0.575490 | Val Loss: 1.780946\n",
      "Validation loss decreased (1.995685 --> 1.780946).  Saving model ...\n",
      "Epoch 11/500 - Train Loss: 0.543905 | Val Loss: 1.656742\n",
      "Validation loss decreased (1.780946 --> 1.656742).  Saving model ...\n",
      "Epoch 12/500 - Train Loss: 0.540388 | Val Loss: 1.667764\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 13/500 - Train Loss: 0.538139 | Val Loss: 1.618086\n",
      "Validation loss decreased (1.656742 --> 1.618086).  Saving model ...\n",
      "Epoch 14/500 - Train Loss: 0.536421 | Val Loss: 1.627337\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 15/500 - Train Loss: 0.524321 | Val Loss: 1.561855\n",
      "Validation loss decreased (1.618086 --> 1.561855).  Saving model ...\n",
      "Epoch 16/500 - Train Loss: 0.482237 | Val Loss: 1.575444\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 17/500 - Train Loss: 0.469889 | Val Loss: 1.572033\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 18/500 - Train Loss: 0.459182 | Val Loss: 1.645330\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 19/500 - Train Loss: 0.454863 | Val Loss: 1.544385\n",
      "Validation loss decreased (1.561855 --> 1.544385).  Saving model ...\n",
      "Epoch 20/500 - Train Loss: 0.452245 | Val Loss: 1.492135\n",
      "Validation loss decreased (1.544385 --> 1.492135).  Saving model ...\n",
      "Epoch 21/500 - Train Loss: 0.445853 | Val Loss: 1.617930\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 22/500 - Train Loss: 0.440456 | Val Loss: 1.555006\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 23/500 - Train Loss: 0.437746 | Val Loss: 1.568990\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 24/500 - Train Loss: 0.430550 | Val Loss: 1.639589\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 25/500 - Train Loss: 0.427489 | Val Loss: 1.613681\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Extracting Latent Representations for OC-SVM...\n",
      "Training One-Class SVM (nu=0.01)...\n",
      "Block 2/3: Train=18530, Val=17941\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Transformer Autoencoder...\n",
      "Training Autoencoder (Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 103.446200 | Val Loss: 67.236209\n",
      "Validation loss decreased (inf --> 67.236209).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 62.588337 | Val Loss: 33.216201\n",
      "Validation loss decreased (67.236209 --> 33.216201).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 29.910425 | Val Loss: 11.451256\n",
      "Validation loss decreased (33.216201 --> 11.451256).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 10.851528 | Val Loss: 2.446681\n",
      "Validation loss decreased (11.451256 --> 2.446681).  Saving model ...\n",
      "Epoch 5/500 - Train Loss: 3.163689 | Val Loss: 0.670070\n",
      "Validation loss decreased (2.446681 --> 0.670070).  Saving model ...\n",
      "Epoch 6/500 - Train Loss: 1.090340 | Val Loss: 0.874337\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 7/500 - Train Loss: 0.697450 | Val Loss: 1.140695\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 8/500 - Train Loss: 0.637930 | Val Loss: 1.252105\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 9/500 - Train Loss: 0.618567 | Val Loss: 1.303009\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 10/500 - Train Loss: 0.605518 | Val Loss: 1.115849\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Extracting Latent Representations for OC-SVM...\n",
      "Training One-Class SVM (nu=0.01)...\n",
      "Block 3/3: Train=12909, Val=14964\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Transformer Autoencoder...\n",
      "Training Autoencoder (Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 98.007857 | Val Loss: 91.170277\n",
      "Validation loss decreased (inf --> 91.170277).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 73.420261 | Val Loss: 65.619577\n",
      "Validation loss decreased (91.170277 --> 65.619577).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 49.632854 | Val Loss: 42.821526\n",
      "Validation loss decreased (65.619577 --> 42.821526).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 30.101369 | Val Loss: 25.108495\n",
      "Validation loss decreased (42.821526 --> 25.108495).  Saving model ...\n",
      "Epoch 5/500 - Train Loss: 16.100514 | Val Loss: 13.196521\n",
      "Validation loss decreased (25.108495 --> 13.196521).  Saving model ...\n",
      "Epoch 6/500 - Train Loss: 7.554047 | Val Loss: 6.391474\n",
      "Validation loss decreased (13.196521 --> 6.391474).  Saving model ...\n",
      "Epoch 7/500 - Train Loss: 3.202145 | Val Loss: 3.097914\n",
      "Validation loss decreased (6.391474 --> 3.097914).  Saving model ...\n",
      "Epoch 8/500 - Train Loss: 1.367890 | Val Loss: 1.726053\n",
      "Validation loss decreased (3.097914 --> 1.726053).  Saving model ...\n",
      "Epoch 9/500 - Train Loss: 0.722551 | Val Loss: 1.197718\n",
      "Validation loss decreased (1.726053 --> 1.197718).  Saving model ...\n",
      "Epoch 10/500 - Train Loss: 0.525781 | Val Loss: 1.009704\n",
      "Validation loss decreased (1.197718 --> 1.009704).  Saving model ...\n",
      "Epoch 11/500 - Train Loss: 0.471904 | Val Loss: 0.934823\n",
      "Validation loss decreased (1.009704 --> 0.934823).  Saving model ...\n",
      "Epoch 12/500 - Train Loss: 0.457923 | Val Loss: 0.908304\n",
      "Validation loss decreased (0.934823 --> 0.908304).  Saving model ...\n",
      "Epoch 13/500 - Train Loss: 0.454001 | Val Loss: 0.894560\n",
      "Validation loss decreased (0.908304 --> 0.894560).  Saving model ...\n",
      "Epoch 14/500 - Train Loss: 0.443693 | Val Loss: 0.887358\n",
      "Validation loss decreased (0.894560 --> 0.887358).  Saving model ...\n",
      "Epoch 15/500 - Train Loss: 0.422399 | Val Loss: 0.861295\n",
      "Validation loss decreased (0.887358 --> 0.861295).  Saving model ...\n",
      "Epoch 16/500 - Train Loss: 0.405491 | Val Loss: 0.876738\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 17/500 - Train Loss: 0.394749 | Val Loss: 0.839266\n",
      "Validation loss decreased (0.861295 --> 0.839266).  Saving model ...\n",
      "Epoch 18/500 - Train Loss: 0.382865 | Val Loss: 0.833867\n",
      "Validation loss decreased (0.839266 --> 0.833867).  Saving model ...\n",
      "Epoch 19/500 - Train Loss: 0.373929 | Val Loss: 0.832640\n",
      "Validation loss decreased (0.833867 --> 0.832640).  Saving model ...\n",
      "Epoch 20/500 - Train Loss: 0.362213 | Val Loss: 0.812096\n",
      "Validation loss decreased (0.832640 --> 0.812096).  Saving model ...\n",
      "Epoch 21/500 - Train Loss: 0.350373 | Val Loss: 0.861090\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 22/500 - Train Loss: 0.337966 | Val Loss: 0.821569\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 23/500 - Train Loss: 0.327129 | Val Loss: 0.804763\n",
      "Validation loss decreased (0.812096 --> 0.804763).  Saving model ...\n",
      "Epoch 24/500 - Train Loss: 0.317338 | Val Loss: 0.806651\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 25/500 - Train Loss: 0.305661 | Val Loss: 0.802199\n",
      "Validation loss decreased (0.804763 --> 0.802199).  Saving model ...\n",
      "Epoch 26/500 - Train Loss: 0.292398 | Val Loss: 0.802597\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 27/500 - Train Loss: 0.281324 | Val Loss: 0.787117\n",
      "Validation loss decreased (0.802199 --> 0.787117).  Saving model ...\n",
      "Epoch 28/500 - Train Loss: 0.272620 | Val Loss: 0.769614\n",
      "Validation loss decreased (0.787117 --> 0.769614).  Saving model ...\n",
      "Epoch 29/500 - Train Loss: 0.264597 | Val Loss: 0.782298\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 30/500 - Train Loss: 0.257234 | Val Loss: 0.753647\n",
      "Validation loss decreased (0.769614 --> 0.753647).  Saving model ...\n",
      "Epoch 31/500 - Train Loss: 0.249751 | Val Loss: 0.738838\n",
      "Validation loss decreased (0.753647 --> 0.738838).  Saving model ...\n",
      "Epoch 32/500 - Train Loss: 0.242803 | Val Loss: 0.732026\n",
      "Validation loss decreased (0.738838 --> 0.732026).  Saving model ...\n",
      "Epoch 33/500 - Train Loss: 0.234158 | Val Loss: 0.723924\n",
      "Validation loss decreased (0.732026 --> 0.723924).  Saving model ...\n",
      "Epoch 34/500 - Train Loss: 0.227676 | Val Loss: 0.761515\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 35/500 - Train Loss: 0.220713 | Val Loss: 0.746869\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 36/500 - Train Loss: 0.215231 | Val Loss: 0.724310\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 37/500 - Train Loss: 0.208965 | Val Loss: 0.715803\n",
      "Validation loss decreased (0.723924 --> 0.715803).  Saving model ...\n",
      "Epoch 38/500 - Train Loss: 0.203648 | Val Loss: 0.699925\n",
      "Validation loss decreased (0.715803 --> 0.699925).  Saving model ...\n",
      "Epoch 39/500 - Train Loss: 0.198900 | Val Loss: 0.697373\n",
      "Validation loss decreased (0.699925 --> 0.697373).  Saving model ...\n",
      "Epoch 40/500 - Train Loss: 0.193864 | Val Loss: 0.697930\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 41/500 - Train Loss: 0.188657 | Val Loss: 0.672034\n",
      "Validation loss decreased (0.697373 --> 0.672034).  Saving model ...\n",
      "Epoch 42/500 - Train Loss: 0.183494 | Val Loss: 0.678156\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 43/500 - Train Loss: 0.178836 | Val Loss: 0.672834\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 44/500 - Train Loss: 0.174273 | Val Loss: 0.718428\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 45/500 - Train Loss: 0.170072 | Val Loss: 0.666087\n",
      "Validation loss decreased (0.672034 --> 0.666087).  Saving model ...\n",
      "Epoch 46/500 - Train Loss: 0.164847 | Val Loss: 0.672893\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 47/500 - Train Loss: 0.161344 | Val Loss: 0.659114\n",
      "Validation loss decreased (0.666087 --> 0.659114).  Saving model ...\n",
      "Epoch 48/500 - Train Loss: 0.156641 | Val Loss: 0.648554\n",
      "Validation loss decreased (0.659114 --> 0.648554).  Saving model ...\n",
      "Epoch 49/500 - Train Loss: 0.151954 | Val Loss: 0.663275\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 50/500 - Train Loss: 0.147828 | Val Loss: 0.630944\n",
      "Validation loss decreased (0.648554 --> 0.630944).  Saving model ...\n",
      "Epoch 51/500 - Train Loss: 0.143796 | Val Loss: 0.624201\n",
      "Validation loss decreased (0.630944 --> 0.624201).  Saving model ...\n",
      "Epoch 52/500 - Train Loss: 0.140048 | Val Loss: 0.632959\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 53/500 - Train Loss: 0.136329 | Val Loss: 0.625152\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 54/500 - Train Loss: 0.132334 | Val Loss: 0.626411\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 55/500 - Train Loss: 0.128476 | Val Loss: 0.610529\n",
      "Validation loss decreased (0.624201 --> 0.610529).  Saving model ...\n",
      "Epoch 56/500 - Train Loss: 0.124985 | Val Loss: 0.609635\n",
      "Validation loss decreased (0.610529 --> 0.609635).  Saving model ...\n",
      "Epoch 57/500 - Train Loss: 0.121633 | Val Loss: 0.614030\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 58/500 - Train Loss: 0.118294 | Val Loss: 0.612149\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 59/500 - Train Loss: 0.115646 | Val Loss: 0.597395\n",
      "Validation loss decreased (0.609635 --> 0.597395).  Saving model ...\n",
      "Epoch 60/500 - Train Loss: 0.112694 | Val Loss: 0.601329\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 61/500 - Train Loss: 0.110132 | Val Loss: 0.591360\n",
      "Validation loss decreased (0.597395 --> 0.591360).  Saving model ...\n",
      "Epoch 62/500 - Train Loss: 0.107541 | Val Loss: 0.598638\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 63/500 - Train Loss: 0.105230 | Val Loss: 0.612564\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 64/500 - Train Loss: 0.102947 | Val Loss: 0.592113\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 65/500 - Train Loss: 0.100668 | Val Loss: 0.586241\n",
      "Validation loss decreased (0.591360 --> 0.586241).  Saving model ...\n",
      "Epoch 66/500 - Train Loss: 0.098384 | Val Loss: 0.591195\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 67/500 - Train Loss: 0.096306 | Val Loss: 0.585270\n",
      "Validation loss decreased (0.586241 --> 0.585270).  Saving model ...\n",
      "Epoch 68/500 - Train Loss: 0.094462 | Val Loss: 0.583506\n",
      "Validation loss decreased (0.585270 --> 0.583506).  Saving model ...\n",
      "Epoch 69/500 - Train Loss: 0.092654 | Val Loss: 0.559672\n",
      "Validation loss decreased (0.583506 --> 0.559672).  Saving model ...\n",
      "Epoch 70/500 - Train Loss: 0.091261 | Val Loss: 0.568454\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 71/500 - Train Loss: 0.089204 | Val Loss: 0.568883\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 72/500 - Train Loss: 0.088112 | Val Loss: 0.583606\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 73/500 - Train Loss: 0.086141 | Val Loss: 0.567808\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 74/500 - Train Loss: 0.084646 | Val Loss: 0.554692\n",
      "Validation loss decreased (0.559672 --> 0.554692).  Saving model ...\n",
      "Epoch 75/500 - Train Loss: 0.083080 | Val Loss: 0.568733\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 76/500 - Train Loss: 0.081535 | Val Loss: 0.552663\n",
      "Validation loss decreased (0.554692 --> 0.552663).  Saving model ...\n",
      "Epoch 77/500 - Train Loss: 0.080550 | Val Loss: 0.575901\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 78/500 - Train Loss: 0.078911 | Val Loss: 0.560622\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 79/500 - Train Loss: 0.077504 | Val Loss: 0.551553\n",
      "Validation loss decreased (0.552663 --> 0.551553).  Saving model ...\n",
      "Epoch 80/500 - Train Loss: 0.076356 | Val Loss: 0.548160\n",
      "Validation loss decreased (0.551553 --> 0.548160).  Saving model ...\n",
      "Epoch 81/500 - Train Loss: 0.074987 | Val Loss: 0.553857\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 82/500 - Train Loss: 0.073883 | Val Loss: 0.556338\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 83/500 - Train Loss: 0.072603 | Val Loss: 0.560962\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 84/500 - Train Loss: 0.071364 | Val Loss: 0.559134\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 85/500 - Train Loss: 0.070394 | Val Loss: 0.571158\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Extracting Latent Representations for OC-SVM...\n",
      "Training One-Class SVM (nu=0.01)...\n",
      "\n",
      "Evaluating on rest of Day 7...\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Evaluating model...\n",
      "Day 7 Results: AUROC=1.0000, AUPRC=1.0000, F4=0.9575\n",
      "\n",
      "================================================================================\n",
      "Final Day (Test Day): 2015-01-13-TOTF.PA-book.csv.gz\n",
      "--------------------------------------------------------------------------------\n",
      "Loading data from ../data/TOTF.PA-book\\2015-01-13-TOTF.PA-book.csv.gz...\n",
      "Successfully loaded 1211246 rows.\n",
      "Filtered 2059 pre-market samples.\n",
      "Morning samples: 163357, Rest of day: 1045830\n",
      "\n",
      "Evaluating on Final Day Morning...\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Evaluating model...\n",
      "Final Day Morning: AUROC=1.0000, AUPRC=1.0000, F4=0.9942\n",
      "\n",
      "Evaluating on Final Day Rest...\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Evaluating model...\n",
      "Final Day Rest: AUROC=1.0000, AUPRC=1.0000, F4=0.9526\n",
      "\n",
      "================================================================================\n",
      "Sequential Training Complete!\n",
      "\n",
      "Saved Transformer+OCSVM model to ../models/TOTF_box-cox_transformer_ocsvm_sequential\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAINING: Transformer + OC-SVM\")\n",
    "\n",
    "results_transformer, pipeline_transformer = sequential_training_pipeline(\n",
    "    data_dir=DATA_DIR,\n",
    "    num_days=NUM_DAYS,\n",
    "    first_hour_minutes=FIRST_HOUR_MINUTES,\n",
    "    train_block_minutes=TRAIN_BLOCK_MINUTES,\n",
    "    val_block_minutes=VAL_BLOCK_MINUTES,\n",
    "    model_type='transformer_ocsvm',\n",
    "    feature_sets=FEATURE_SETS,\n",
    "    epochs=EPOCHS,\n",
    "    lr=LR,\n",
    "    nu=0.01,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    patience=PATIENCE,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Save results\n",
    "with open('../models/TOTF_sequential_transformer_ocsvm_results.json', 'w') as f:\n",
    "    json.dump(results_transformer, f, indent=2)\n",
    "\n",
    "# Save trained model\n",
    "base_filename = f\"../models/TOTF_{SCALER_TYPE}_transformer_ocsvm_sequential\"\n",
    "torch.save(pipeline_transformer.model.state_dict(), f\"{base_filename}_weights.pth\")\n",
    "joblib.dump(pipeline_transformer.scaler, f\"{base_filename}_scaler.pkl\")\n",
    "if pipeline_transformer.detector is not None:\n",
    "    joblib.dump(pipeline_transformer.detector, f\"{base_filename}_ocsvm_detector.pkl\")\n",
    "    joblib.dump(pipeline_transformer.latent_scaler, f\"{base_filename}_latent_scaler.pkl\")\n",
    "\n",
    "config = {\n",
    "    'dataset': 'TOTF',\n",
    "    'model_type': 'transformer_ocsvm',\n",
    "    'scaler_type': SCALER_TYPE,\n",
    "    'training_type': 'sequential',\n",
    "    'num_days': NUM_DAYS,\n",
    "    'seq_length': SEQ_LENGTH,\n",
    "    'input_dim': len(pipeline_transformer.feature_names),\n",
    "    'hidden_dim': HIDDEN_DIM,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'feature_names': pipeline_transformer.feature_names\n",
    "}\n",
    "with open(f\"{base_filename}_config.json\", 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved Transformer+OCSVM model to {base_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f711b7cb",
   "metadata": {},
   "source": [
    "### 2. Probabilistic Robust Autoencoder (PRAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "571cdb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING: PRAE\n",
      "Pipeline initialized on device: cuda\n",
      "Starting sequential training on 7 days...\n",
      "Train block: 10 min, Val block: 10 min\n",
      "================================================================================\n",
      "Day 1/7: 2015-01-02-TOTF.PA-book.csv.gz\n",
      "--------------------------------------------------------------------------------\n",
      "Loading data from ../data/TOTF.PA-book\\2015-01-02-TOTF.PA-book.csv.gz...\n",
      "Successfully loaded 640429 rows.\n",
      "Filtered 2801 pre-market samples.\n",
      "First hour samples: 74476, Rest of day: 563152\n",
      "Training on 3 time blocks...\n",
      "Block 1/3: Train=21255, Val=12380\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Initializing prae model...\n",
      "Initializing Probabilistic Robust Autoencoder (PRAE)...\n",
      "Training PRAE (lambda=0.100000, Max Epochs=1000)...\n",
      "Epoch 1/1000 - Train Loss: 0.447761 | Val MSE: 0.596596\n",
      "Validation loss decreased (inf --> 0.596596).  Saving model ...\n",
      "Epoch 2/1000 - Train Loss: 0.300614 | Val MSE: 0.645875\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 3/1000 - Train Loss: 0.250935 | Val MSE: 0.691657\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 4/1000 - Train Loss: 0.210169 | Val MSE: 0.628766\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 5/1000 - Train Loss: 0.186079 | Val MSE: 0.681419\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 6/1000 - Train Loss: 0.166069 | Val MSE: 0.563547\n",
      "Validation loss decreased (0.596596 --> 0.563547).  Saving model ...\n",
      "Epoch 7/1000 - Train Loss: 0.139812 | Val MSE: 0.544455\n",
      "Validation loss decreased (0.563547 --> 0.544455).  Saving model ...\n",
      "Epoch 8/1000 - Train Loss: 0.115439 | Val MSE: 0.525993\n",
      "Validation loss decreased (0.544455 --> 0.525993).  Saving model ...\n",
      "Epoch 9/1000 - Train Loss: 0.094700 | Val MSE: 0.515346\n",
      "Validation loss decreased (0.525993 --> 0.515346).  Saving model ...\n",
      "Epoch 10/1000 - Train Loss: 0.081696 | Val MSE: 0.475214\n",
      "Validation loss decreased (0.515346 --> 0.475214).  Saving model ...\n",
      "Epoch 11/1000 - Train Loss: 0.066503 | Val MSE: 0.484753\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 12/1000 - Train Loss: 0.054574 | Val MSE: 0.415051\n",
      "Validation loss decreased (0.475214 --> 0.415051).  Saving model ...\n",
      "Epoch 13/1000 - Train Loss: 0.045645 | Val MSE: 0.435117\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 14/1000 - Train Loss: 0.038062 | Val MSE: 0.376975\n",
      "Validation loss decreased (0.415051 --> 0.376975).  Saving model ...\n",
      "Epoch 15/1000 - Train Loss: 0.030866 | Val MSE: 0.397844\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 16/1000 - Train Loss: 0.027538 | Val MSE: 0.366515\n",
      "Validation loss decreased (0.376975 --> 0.366515).  Saving model ...\n",
      "Epoch 17/1000 - Train Loss: 0.020602 | Val MSE: 0.358198\n",
      "Validation loss decreased (0.366515 --> 0.358198).  Saving model ...\n",
      "Epoch 18/1000 - Train Loss: 0.016802 | Val MSE: 0.354398\n",
      "Validation loss decreased (0.358198 --> 0.354398).  Saving model ...\n",
      "Epoch 19/1000 - Train Loss: 0.013337 | Val MSE: 0.342930\n",
      "Validation loss decreased (0.354398 --> 0.342930).  Saving model ...\n",
      "Epoch 20/1000 - Train Loss: 0.010716 | Val MSE: 0.353109\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 21/1000 - Train Loss: 0.007834 | Val MSE: 0.335446\n",
      "Validation loss decreased (0.342930 --> 0.335446).  Saving model ...\n",
      "Epoch 22/1000 - Train Loss: 0.006884 | Val MSE: 0.336080\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 23/1000 - Train Loss: 0.003693 | Val MSE: 0.329294\n",
      "Validation loss decreased (0.335446 --> 0.329294).  Saving model ...\n",
      "Epoch 24/1000 - Train Loss: 0.001216 | Val MSE: 0.328838\n",
      "Validation loss decreased (0.329294 --> 0.328838).  Saving model ...\n",
      "Epoch 25/1000 - Train Loss: -0.000223 | Val MSE: 0.317202\n",
      "Validation loss decreased (0.328838 --> 0.317202).  Saving model ...\n",
      "Epoch 26/1000 - Train Loss: -0.001561 | Val MSE: 0.323450\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 27/1000 - Train Loss: -0.001096 | Val MSE: 0.316235\n",
      "Validation loss decreased (0.317202 --> 0.316235).  Saving model ...\n",
      "Epoch 28/1000 - Train Loss: -0.004373 | Val MSE: 0.321751\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 29/1000 - Train Loss: -0.005490 | Val MSE: 0.317584\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 30/1000 - Train Loss: -0.006260 | Val MSE: 0.308598\n",
      "Validation loss decreased (0.316235 --> 0.308598).  Saving model ...\n",
      "Epoch 31/1000 - Train Loss: -0.007605 | Val MSE: 0.313112\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 32/1000 - Train Loss: -0.008661 | Val MSE: 0.309285\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 33/1000 - Train Loss: -0.009776 | Val MSE: 0.302724\n",
      "Validation loss decreased (0.308598 --> 0.302724).  Saving model ...\n",
      "Epoch 34/1000 - Train Loss: -0.010566 | Val MSE: 0.296365\n",
      "Validation loss decreased (0.302724 --> 0.296365).  Saving model ...\n",
      "Epoch 35/1000 - Train Loss: -0.011208 | Val MSE: 0.299848\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 36/1000 - Train Loss: -0.010547 | Val MSE: 0.295162\n",
      "Validation loss decreased (0.296365 --> 0.295162).  Saving model ...\n",
      "Epoch 37/1000 - Train Loss: -0.012763 | Val MSE: 0.290588\n",
      "Validation loss decreased (0.295162 --> 0.290588).  Saving model ...\n",
      "Epoch 38/1000 - Train Loss: -0.013227 | Val MSE: 0.295234\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 39/1000 - Train Loss: -0.014623 | Val MSE: 0.297187\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 40/1000 - Train Loss: -0.015877 | Val MSE: 0.297209\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 41/1000 - Train Loss: -0.017154 | Val MSE: 0.292766\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 42/1000 - Train Loss: -0.018226 | Val MSE: 0.290420\n",
      "Validation loss decreased (0.290588 --> 0.290420).  Saving model ...\n",
      "Epoch 43/1000 - Train Loss: -0.018193 | Val MSE: 0.287745\n",
      "Validation loss decreased (0.290420 --> 0.287745).  Saving model ...\n",
      "Epoch 44/1000 - Train Loss: -0.018968 | Val MSE: 0.282432\n",
      "Validation loss decreased (0.287745 --> 0.282432).  Saving model ...\n",
      "Epoch 45/1000 - Train Loss: -0.019970 | Val MSE: 0.278449\n",
      "Validation loss decreased (0.282432 --> 0.278449).  Saving model ...\n",
      "Epoch 46/1000 - Train Loss: -0.021195 | Val MSE: 0.279046\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 47/1000 - Train Loss: -0.021623 | Val MSE: 0.278281\n",
      "Validation loss decreased (0.278449 --> 0.278281).  Saving model ...\n",
      "Epoch 48/1000 - Train Loss: -0.022218 | Val MSE: 0.280765\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 49/1000 - Train Loss: -0.022685 | Val MSE: 0.280345\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 50/1000 - Train Loss: -0.023462 | Val MSE: 0.280559\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 51/1000 - Train Loss: -0.023674 | Val MSE: 0.275898\n",
      "Validation loss decreased (0.278281 --> 0.275898).  Saving model ...\n",
      "Epoch 52/1000 - Train Loss: -0.023739 | Val MSE: 0.279100\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 53/1000 - Train Loss: -0.024404 | Val MSE: 0.275448\n",
      "Validation loss decreased (0.275898 --> 0.275448).  Saving model ...\n",
      "Epoch 54/1000 - Train Loss: -0.025288 | Val MSE: 0.271638\n",
      "Validation loss decreased (0.275448 --> 0.271638).  Saving model ...\n",
      "Epoch 55/1000 - Train Loss: -0.026147 | Val MSE: 0.274140\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 56/1000 - Train Loss: -0.026001 | Val MSE: 0.280718\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 57/1000 - Train Loss: -0.026322 | Val MSE: 0.275509\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 58/1000 - Train Loss: -0.027185 | Val MSE: 0.271225\n",
      "Validation loss decreased (0.271638 --> 0.271225).  Saving model ...\n",
      "Epoch 59/1000 - Train Loss: -0.027290 | Val MSE: 0.279451\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 60/1000 - Train Loss: -0.027914 | Val MSE: 0.273133\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 61/1000 - Train Loss: -0.028326 | Val MSE: 0.270209\n",
      "Validation loss decreased (0.271225 --> 0.270209).  Saving model ...\n",
      "Epoch 62/1000 - Train Loss: -0.028583 | Val MSE: 0.274612\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 63/1000 - Train Loss: -0.028546 | Val MSE: 0.269114\n",
      "Validation loss decreased (0.270209 --> 0.269114).  Saving model ...\n",
      "Epoch 64/1000 - Train Loss: -0.029409 | Val MSE: 0.268503\n",
      "Validation loss decreased (0.269114 --> 0.268503).  Saving model ...\n",
      "Epoch 65/1000 - Train Loss: -0.029728 | Val MSE: 0.266627\n",
      "Validation loss decreased (0.268503 --> 0.266627).  Saving model ...\n",
      "Epoch 66/1000 - Train Loss: -0.030036 | Val MSE: 0.265988\n",
      "Validation loss decreased (0.266627 --> 0.265988).  Saving model ...\n",
      "Epoch 67/1000 - Train Loss: -0.029829 | Val MSE: 0.269894\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 68/1000 - Train Loss: -0.030218 | Val MSE: 0.273271\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 69/1000 - Train Loss: -0.030937 | Val MSE: 0.265275\n",
      "Validation loss decreased (0.265988 --> 0.265275).  Saving model ...\n",
      "Epoch 70/1000 - Train Loss: -0.031275 | Val MSE: 0.270798\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 71/1000 - Train Loss: -0.031515 | Val MSE: 0.267444\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 72/1000 - Train Loss: -0.031793 | Val MSE: 0.263739\n",
      "Validation loss decreased (0.265275 --> 0.263739).  Saving model ...\n",
      "Epoch 73/1000 - Train Loss: -0.032139 | Val MSE: 0.264044\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 74/1000 - Train Loss: -0.032278 | Val MSE: 0.266072\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 75/1000 - Train Loss: -0.032798 | Val MSE: 0.266410\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 76/1000 - Train Loss: -0.032918 | Val MSE: 0.267850\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 77/1000 - Train Loss: -0.033093 | Val MSE: 0.264452\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Block 2/3: Train=13265, Val=7183\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Robust Autoencoder (PRAE)...\n",
      "Training PRAE (lambda=0.100000, Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 0.503798 | Val MSE: 0.766334\n",
      "Validation loss decreased (inf --> 0.766334).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 0.340076 | Val MSE: 0.533050\n",
      "Validation loss decreased (0.766334 --> 0.533050).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 0.283823 | Val MSE: 0.501796\n",
      "Validation loss decreased (0.533050 --> 0.501796).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 0.257363 | Val MSE: 0.480538\n",
      "Validation loss decreased (0.501796 --> 0.480538).  Saving model ...\n",
      "Epoch 5/500 - Train Loss: 0.234840 | Val MSE: 0.454325\n",
      "Validation loss decreased (0.480538 --> 0.454325).  Saving model ...\n",
      "Epoch 6/500 - Train Loss: 0.205902 | Val MSE: 0.433098\n",
      "Validation loss decreased (0.454325 --> 0.433098).  Saving model ...\n",
      "Epoch 7/500 - Train Loss: 0.185818 | Val MSE: 0.410549\n",
      "Validation loss decreased (0.433098 --> 0.410549).  Saving model ...\n",
      "Epoch 8/500 - Train Loss: 0.166624 | Val MSE: 0.398231\n",
      "Validation loss decreased (0.410549 --> 0.398231).  Saving model ...\n",
      "Epoch 9/500 - Train Loss: 0.152913 | Val MSE: 0.376868\n",
      "Validation loss decreased (0.398231 --> 0.376868).  Saving model ...\n",
      "Epoch 10/500 - Train Loss: 0.129355 | Val MSE: 0.344952\n",
      "Validation loss decreased (0.376868 --> 0.344952).  Saving model ...\n",
      "Epoch 11/500 - Train Loss: 0.108418 | Val MSE: 0.319947\n",
      "Validation loss decreased (0.344952 --> 0.319947).  Saving model ...\n",
      "Epoch 12/500 - Train Loss: 0.094638 | Val MSE: 0.309272\n",
      "Validation loss decreased (0.319947 --> 0.309272).  Saving model ...\n",
      "Epoch 13/500 - Train Loss: 0.082557 | Val MSE: 0.294977\n",
      "Validation loss decreased (0.309272 --> 0.294977).  Saving model ...\n",
      "Epoch 14/500 - Train Loss: 0.070128 | Val MSE: 0.286534\n",
      "Validation loss decreased (0.294977 --> 0.286534).  Saving model ...\n",
      "Epoch 15/500 - Train Loss: 0.061437 | Val MSE: 0.276356\n",
      "Validation loss decreased (0.286534 --> 0.276356).  Saving model ...\n",
      "Epoch 16/500 - Train Loss: 0.053614 | Val MSE: 0.270316\n",
      "Validation loss decreased (0.276356 --> 0.270316).  Saving model ...\n",
      "Epoch 17/500 - Train Loss: 0.046703 | Val MSE: 0.265472\n",
      "Validation loss decreased (0.270316 --> 0.265472).  Saving model ...\n",
      "Epoch 18/500 - Train Loss: 0.040412 | Val MSE: 0.264104\n",
      "Validation loss decreased (0.265472 --> 0.264104).  Saving model ...\n",
      "Epoch 19/500 - Train Loss: 0.033785 | Val MSE: 0.255005\n",
      "Validation loss decreased (0.264104 --> 0.255005).  Saving model ...\n",
      "Epoch 20/500 - Train Loss: 0.029806 | Val MSE: 0.258425\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 21/500 - Train Loss: 0.024028 | Val MSE: 0.252702\n",
      "Validation loss decreased (0.255005 --> 0.252702).  Saving model ...\n",
      "Epoch 22/500 - Train Loss: 0.020064 | Val MSE: 0.256488\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 23/500 - Train Loss: 0.016270 | Val MSE: 0.245654\n",
      "Validation loss decreased (0.252702 --> 0.245654).  Saving model ...\n",
      "Epoch 24/500 - Train Loss: 0.011748 | Val MSE: 0.239034\n",
      "Validation loss decreased (0.245654 --> 0.239034).  Saving model ...\n",
      "Epoch 25/500 - Train Loss: 0.008211 | Val MSE: 0.239528\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 26/500 - Train Loss: 0.005494 | Val MSE: 0.238421\n",
      "Validation loss decreased (0.239034 --> 0.238421).  Saving model ...\n",
      "Epoch 27/500 - Train Loss: 0.003022 | Val MSE: 0.231157\n",
      "Validation loss decreased (0.238421 --> 0.231157).  Saving model ...\n",
      "Epoch 28/500 - Train Loss: 0.000063 | Val MSE: 0.230756\n",
      "Validation loss decreased (0.231157 --> 0.230756).  Saving model ...\n",
      "Epoch 29/500 - Train Loss: -0.002071 | Val MSE: 0.229344\n",
      "Validation loss decreased (0.230756 --> 0.229344).  Saving model ...\n",
      "Epoch 30/500 - Train Loss: -0.004290 | Val MSE: 0.230631\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 31/500 - Train Loss: -0.006618 | Val MSE: 0.227684\n",
      "Validation loss decreased (0.229344 --> 0.227684).  Saving model ...\n",
      "Epoch 32/500 - Train Loss: -0.008050 | Val MSE: 0.226806\n",
      "Validation loss decreased (0.227684 --> 0.226806).  Saving model ...\n",
      "Epoch 33/500 - Train Loss: -0.009888 | Val MSE: 0.224907\n",
      "Validation loss decreased (0.226806 --> 0.224907).  Saving model ...\n",
      "Epoch 34/500 - Train Loss: -0.011496 | Val MSE: 0.225993\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 35/500 - Train Loss: -0.012942 | Val MSE: 0.221553\n",
      "Validation loss decreased (0.224907 --> 0.221553).  Saving model ...\n",
      "Epoch 36/500 - Train Loss: -0.014372 | Val MSE: 0.222999\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 37/500 - Train Loss: -0.015956 | Val MSE: 0.223099\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 38/500 - Train Loss: -0.016871 | Val MSE: 0.223355\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 39/500 - Train Loss: -0.018132 | Val MSE: 0.226550\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 40/500 - Train Loss: -0.019238 | Val MSE: 0.224478\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Block 3/3: Train=7918, Val=12475\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Robust Autoencoder (PRAE)...\n",
      "Training PRAE (lambda=0.100000, Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 1.280641 | Val MSE: 5.465430\n",
      "Validation loss decreased (inf --> 5.465430).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 0.643579 | Val MSE: 3.829174\n",
      "Validation loss decreased (5.465430 --> 3.829174).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 0.462405 | Val MSE: 3.157842\n",
      "Validation loss decreased (3.829174 --> 3.157842).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 0.423877 | Val MSE: 2.890838\n",
      "Validation loss decreased (3.157842 --> 2.890838).  Saving model ...\n",
      "Epoch 5/500 - Train Loss: 0.366137 | Val MSE: 2.581265\n",
      "Validation loss decreased (2.890838 --> 2.581265).  Saving model ...\n",
      "Epoch 6/500 - Train Loss: 0.339527 | Val MSE: 2.457125\n",
      "Validation loss decreased (2.581265 --> 2.457125).  Saving model ...\n",
      "Epoch 7/500 - Train Loss: 0.302017 | Val MSE: 2.382166\n",
      "Validation loss decreased (2.457125 --> 2.382166).  Saving model ...\n",
      "Epoch 8/500 - Train Loss: 0.272169 | Val MSE: 2.313016\n",
      "Validation loss decreased (2.382166 --> 2.313016).  Saving model ...\n",
      "Epoch 9/500 - Train Loss: 0.241390 | Val MSE: 2.142804\n",
      "Validation loss decreased (2.313016 --> 2.142804).  Saving model ...\n",
      "Epoch 10/500 - Train Loss: 0.215964 | Val MSE: 2.246875\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 11/500 - Train Loss: 0.189454 | Val MSE: 2.056494\n",
      "Validation loss decreased (2.142804 --> 2.056494).  Saving model ...\n",
      "Epoch 12/500 - Train Loss: 0.177729 | Val MSE: 2.008787\n",
      "Validation loss decreased (2.056494 --> 2.008787).  Saving model ...\n",
      "Epoch 13/500 - Train Loss: 0.163858 | Val MSE: 2.029526\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 14/500 - Train Loss: 0.154265 | Val MSE: 2.023665\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 15/500 - Train Loss: 0.143444 | Val MSE: 2.018279\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 16/500 - Train Loss: 0.130419 | Val MSE: 2.006059\n",
      "Validation loss decreased (2.008787 --> 2.006059).  Saving model ...\n",
      "Epoch 17/500 - Train Loss: 0.123483 | Val MSE: 2.037440\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 18/500 - Train Loss: 0.111715 | Val MSE: 1.966692\n",
      "Validation loss decreased (2.006059 --> 1.966692).  Saving model ...\n",
      "Epoch 19/500 - Train Loss: 0.102594 | Val MSE: 1.966948\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 20/500 - Train Loss: 0.096419 | Val MSE: 1.953546\n",
      "Validation loss decreased (1.966692 --> 1.953546).  Saving model ...\n",
      "Epoch 21/500 - Train Loss: 0.089200 | Val MSE: 1.938926\n",
      "Validation loss decreased (1.953546 --> 1.938926).  Saving model ...\n",
      "Epoch 22/500 - Train Loss: 0.084913 | Val MSE: 2.040612\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 23/500 - Train Loss: 0.080281 | Val MSE: 1.995123\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 24/500 - Train Loss: 0.075535 | Val MSE: 1.956181\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 25/500 - Train Loss: 0.072349 | Val MSE: 1.930882\n",
      "Validation loss decreased (1.938926 --> 1.930882).  Saving model ...\n",
      "Epoch 26/500 - Train Loss: 0.065061 | Val MSE: 1.960039\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 27/500 - Train Loss: 0.061128 | Val MSE: 1.962843\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 28/500 - Train Loss: 0.055228 | Val MSE: 1.914839\n",
      "Validation loss decreased (1.930882 --> 1.914839).  Saving model ...\n",
      "Epoch 29/500 - Train Loss: 0.052104 | Val MSE: 1.924098\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 30/500 - Train Loss: 0.048615 | Val MSE: 1.984686\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 31/500 - Train Loss: 0.045748 | Val MSE: 1.906893\n",
      "Validation loss decreased (1.914839 --> 1.906893).  Saving model ...\n",
      "Epoch 32/500 - Train Loss: 0.042238 | Val MSE: 1.936500\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 33/500 - Train Loss: 0.039464 | Val MSE: 1.960519\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 34/500 - Train Loss: 0.034600 | Val MSE: 1.956911\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 35/500 - Train Loss: 0.032967 | Val MSE: 1.933622\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 36/500 - Train Loss: 0.028980 | Val MSE: 1.907628\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "\n",
      "Evaluating on rest of Day 1...\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Evaluating model...\n",
      "Day 1 Results: AUROC=0.9975, AUPRC=0.9866, F4=0.0022\n",
      "Day 2/7: 2015-01-05-TOTF.PA-book.csv.gz\n",
      "--------------------------------------------------------------------------------\n",
      "Loading data from ../data/TOTF.PA-book\\2015-01-05-TOTF.PA-book.csv.gz...\n",
      "Successfully loaded 1202740 rows.\n",
      "Filtered 1920 pre-market samples.\n",
      "First hour samples: 113192, Rest of day: 1087628\n",
      "Training on 3 time blocks...\n",
      "Block 1/3: Train=24352, Val=30164\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Robust Autoencoder (PRAE)...\n",
      "Training PRAE (lambda=0.100000, Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 20.530875 | Val MSE: 13.096389\n",
      "Validation loss decreased (inf --> 13.096389).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 5.372814 | Val MSE: 2.105700\n",
      "Validation loss decreased (13.096389 --> 2.105700).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 1.357415 | Val MSE: 0.949048\n",
      "Validation loss decreased (2.105700 --> 0.949048).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 0.898011 | Val MSE: 0.917082\n",
      "Validation loss decreased (0.949048 --> 0.917082).  Saving model ...\n",
      "Epoch 5/500 - Train Loss: 0.727104 | Val MSE: 0.901974\n",
      "Validation loss decreased (0.917082 --> 0.901974).  Saving model ...\n",
      "Epoch 6/500 - Train Loss: 0.622692 | Val MSE: 0.871100\n",
      "Validation loss decreased (0.901974 --> 0.871100).  Saving model ...\n",
      "Epoch 7/500 - Train Loss: 0.554048 | Val MSE: 0.874848\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 8/500 - Train Loss: 0.499435 | Val MSE: 0.826671\n",
      "Validation loss decreased (0.871100 --> 0.826671).  Saving model ...\n",
      "Epoch 9/500 - Train Loss: 0.445933 | Val MSE: 0.815429\n",
      "Validation loss decreased (0.826671 --> 0.815429).  Saving model ...\n",
      "Epoch 10/500 - Train Loss: 0.400026 | Val MSE: 0.763937\n",
      "Validation loss decreased (0.815429 --> 0.763937).  Saving model ...\n",
      "Epoch 11/500 - Train Loss: 0.361908 | Val MSE: 0.744759\n",
      "Validation loss decreased (0.763937 --> 0.744759).  Saving model ...\n",
      "Epoch 12/500 - Train Loss: 0.327616 | Val MSE: 0.713851\n",
      "Validation loss decreased (0.744759 --> 0.713851).  Saving model ...\n",
      "Epoch 13/500 - Train Loss: 0.294766 | Val MSE: 0.713050\n",
      "Validation loss decreased (0.713851 --> 0.713050).  Saving model ...\n",
      "Epoch 14/500 - Train Loss: 0.263341 | Val MSE: 0.680047\n",
      "Validation loss decreased (0.713050 --> 0.680047).  Saving model ...\n",
      "Epoch 15/500 - Train Loss: 0.240834 | Val MSE: 0.672234\n",
      "Validation loss decreased (0.680047 --> 0.672234).  Saving model ...\n",
      "Epoch 16/500 - Train Loss: 0.218383 | Val MSE: 0.667717\n",
      "Validation loss decreased (0.672234 --> 0.667717).  Saving model ...\n",
      "Epoch 17/500 - Train Loss: 0.194692 | Val MSE: 0.633998\n",
      "Validation loss decreased (0.667717 --> 0.633998).  Saving model ...\n",
      "Epoch 18/500 - Train Loss: 0.173357 | Val MSE: 0.619999\n",
      "Validation loss decreased (0.633998 --> 0.619999).  Saving model ...\n",
      "Epoch 19/500 - Train Loss: 0.160609 | Val MSE: 0.596938\n",
      "Validation loss decreased (0.619999 --> 0.596938).  Saving model ...\n",
      "Epoch 20/500 - Train Loss: 0.144142 | Val MSE: 0.595383\n",
      "Validation loss decreased (0.596938 --> 0.595383).  Saving model ...\n",
      "Epoch 21/500 - Train Loss: 0.132204 | Val MSE: 0.609478\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 22/500 - Train Loss: 0.121892 | Val MSE: 0.573279\n",
      "Validation loss decreased (0.595383 --> 0.573279).  Saving model ...\n",
      "Epoch 23/500 - Train Loss: 0.109924 | Val MSE: 0.550045\n",
      "Validation loss decreased (0.573279 --> 0.550045).  Saving model ...\n",
      "Epoch 24/500 - Train Loss: 0.101653 | Val MSE: 0.558483\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 25/500 - Train Loss: 0.096829 | Val MSE: 0.559991\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 26/500 - Train Loss: 0.088099 | Val MSE: 0.540763\n",
      "Validation loss decreased (0.550045 --> 0.540763).  Saving model ...\n",
      "Epoch 27/500 - Train Loss: 0.078976 | Val MSE: 0.533150\n",
      "Validation loss decreased (0.540763 --> 0.533150).  Saving model ...\n",
      "Epoch 28/500 - Train Loss: 0.076744 | Val MSE: 0.538993\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 29/500 - Train Loss: 0.073323 | Val MSE: 0.513148\n",
      "Validation loss decreased (0.533150 --> 0.513148).  Saving model ...\n",
      "Epoch 30/500 - Train Loss: 0.063195 | Val MSE: 0.501349\n",
      "Validation loss decreased (0.513148 --> 0.501349).  Saving model ...\n",
      "Epoch 31/500 - Train Loss: 0.058087 | Val MSE: 0.506149\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 32/500 - Train Loss: 0.053855 | Val MSE: 0.500809\n",
      "Validation loss decreased (0.501349 --> 0.500809).  Saving model ...\n",
      "Epoch 33/500 - Train Loss: 0.051347 | Val MSE: 0.507489\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 34/500 - Train Loss: 0.047637 | Val MSE: 0.525734\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 35/500 - Train Loss: 0.044707 | Val MSE: 0.498083\n",
      "Validation loss decreased (0.500809 --> 0.498083).  Saving model ...\n",
      "Epoch 36/500 - Train Loss: 0.043723 | Val MSE: 0.501597\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 37/500 - Train Loss: 0.042771 | Val MSE: 0.495777\n",
      "Validation loss decreased (0.498083 --> 0.495777).  Saving model ...\n",
      "Epoch 38/500 - Train Loss: 0.037045 | Val MSE: 0.491505\n",
      "Validation loss decreased (0.495777 --> 0.491505).  Saving model ...\n",
      "Epoch 39/500 - Train Loss: 0.038332 | Val MSE: 0.500399\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 40/500 - Train Loss: 0.036514 | Val MSE: 0.487925\n",
      "Validation loss decreased (0.491505 --> 0.487925).  Saving model ...\n",
      "Epoch 41/500 - Train Loss: 0.029120 | Val MSE: 0.479049\n",
      "Validation loss decreased (0.487925 --> 0.479049).  Saving model ...\n",
      "Epoch 42/500 - Train Loss: 0.028000 | Val MSE: 0.488946\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 43/500 - Train Loss: 0.025842 | Val MSE: 0.486920\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 44/500 - Train Loss: 0.021774 | Val MSE: 0.478952\n",
      "Validation loss decreased (0.479049 --> 0.478952).  Saving model ...\n",
      "Epoch 45/500 - Train Loss: 0.019622 | Val MSE: 0.496125\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 46/500 - Train Loss: 0.017715 | Val MSE: 0.486589\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 47/500 - Train Loss: 0.018468 | Val MSE: 0.469523\n",
      "Validation loss decreased (0.478952 --> 0.469523).  Saving model ...\n",
      "Epoch 48/500 - Train Loss: 0.014444 | Val MSE: 0.464142\n",
      "Validation loss decreased (0.469523 --> 0.464142).  Saving model ...\n",
      "Epoch 49/500 - Train Loss: 0.012644 | Val MSE: 0.473215\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 50/500 - Train Loss: 0.010977 | Val MSE: 0.462989\n",
      "Validation loss decreased (0.464142 --> 0.462989).  Saving model ...\n",
      "Epoch 51/500 - Train Loss: 0.010498 | Val MSE: 0.477032\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 52/500 - Train Loss: 0.008706 | Val MSE: 0.462421\n",
      "Validation loss decreased (0.462989 --> 0.462421).  Saving model ...\n",
      "Epoch 53/500 - Train Loss: 0.007532 | Val MSE: 0.453616\n",
      "Validation loss decreased (0.462421 --> 0.453616).  Saving model ...\n",
      "Epoch 54/500 - Train Loss: 0.011646 | Val MSE: 0.468299\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 55/500 - Train Loss: 0.007321 | Val MSE: 0.516147\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 56/500 - Train Loss: 0.016783 | Val MSE: 0.461300\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 57/500 - Train Loss: 0.003598 | Val MSE: 0.455754\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 58/500 - Train Loss: 0.000973 | Val MSE: 0.449800\n",
      "Validation loss decreased (0.453616 --> 0.449800).  Saving model ...\n",
      "Epoch 59/500 - Train Loss: 0.001189 | Val MSE: 0.455213\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 60/500 - Train Loss: -0.000177 | Val MSE: 0.455897\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 61/500 - Train Loss: -0.001539 | Val MSE: 0.452121\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 62/500 - Train Loss: 0.005426 | Val MSE: 0.471421\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 63/500 - Train Loss: 0.004097 | Val MSE: 0.467592\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Block 2/3: Train=20778, Val=12683\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Robust Autoencoder (PRAE)...\n",
      "Training PRAE (lambda=0.100000, Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 13.356718 | Val MSE: 6.711791\n",
      "Validation loss decreased (inf --> 6.711791).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 3.461779 | Val MSE: 0.849882\n",
      "Validation loss decreased (6.711791 --> 0.849882).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 0.763405 | Val MSE: 0.583548\n",
      "Validation loss decreased (0.849882 --> 0.583548).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 0.532490 | Val MSE: 0.684327\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 5/500 - Train Loss: 0.520094 | Val MSE: 0.643362\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 6/500 - Train Loss: 0.415623 | Val MSE: 0.596405\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 7/500 - Train Loss: 0.351356 | Val MSE: 0.571957\n",
      "Validation loss decreased (0.583548 --> 0.571957).  Saving model ...\n",
      "Epoch 8/500 - Train Loss: 0.333554 | Val MSE: 0.560374\n",
      "Validation loss decreased (0.571957 --> 0.560374).  Saving model ...\n",
      "Epoch 9/500 - Train Loss: 0.320071 | Val MSE: 0.565419\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 10/500 - Train Loss: 0.289965 | Val MSE: 0.520039\n",
      "Validation loss decreased (0.560374 --> 0.520039).  Saving model ...\n",
      "Epoch 11/500 - Train Loss: 0.267768 | Val MSE: 0.490442\n",
      "Validation loss decreased (0.520039 --> 0.490442).  Saving model ...\n",
      "Epoch 12/500 - Train Loss: 0.250504 | Val MSE: 0.470255\n",
      "Validation loss decreased (0.490442 --> 0.470255).  Saving model ...\n",
      "Epoch 13/500 - Train Loss: 0.230999 | Val MSE: 0.448222\n",
      "Validation loss decreased (0.470255 --> 0.448222).  Saving model ...\n",
      "Epoch 14/500 - Train Loss: 0.212940 | Val MSE: 0.413159\n",
      "Validation loss decreased (0.448222 --> 0.413159).  Saving model ...\n",
      "Epoch 15/500 - Train Loss: 0.193901 | Val MSE: 0.403821\n",
      "Validation loss decreased (0.413159 --> 0.403821).  Saving model ...\n",
      "Epoch 16/500 - Train Loss: 0.176997 | Val MSE: 0.388592\n",
      "Validation loss decreased (0.403821 --> 0.388592).  Saving model ...\n",
      "Epoch 17/500 - Train Loss: 0.162100 | Val MSE: 0.364353\n",
      "Validation loss decreased (0.388592 --> 0.364353).  Saving model ...\n",
      "Epoch 18/500 - Train Loss: 0.147214 | Val MSE: 0.351751\n",
      "Validation loss decreased (0.364353 --> 0.351751).  Saving model ...\n",
      "Epoch 19/500 - Train Loss: 0.133683 | Val MSE: 0.338637\n",
      "Validation loss decreased (0.351751 --> 0.338637).  Saving model ...\n",
      "Epoch 20/500 - Train Loss: 0.123925 | Val MSE: 0.330986\n",
      "Validation loss decreased (0.338637 --> 0.330986).  Saving model ...\n",
      "Epoch 21/500 - Train Loss: 0.111680 | Val MSE: 0.325837\n",
      "Validation loss decreased (0.330986 --> 0.325837).  Saving model ...\n",
      "Epoch 22/500 - Train Loss: 0.104431 | Val MSE: 0.318010\n",
      "Validation loss decreased (0.325837 --> 0.318010).  Saving model ...\n",
      "Epoch 23/500 - Train Loss: 0.094863 | Val MSE: 0.294960\n",
      "Validation loss decreased (0.318010 --> 0.294960).  Saving model ...\n",
      "Epoch 24/500 - Train Loss: 0.086460 | Val MSE: 0.284685\n",
      "Validation loss decreased (0.294960 --> 0.284685).  Saving model ...\n",
      "Epoch 25/500 - Train Loss: 0.078022 | Val MSE: 0.285464\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 26/500 - Train Loss: 0.069119 | Val MSE: 0.268360\n",
      "Validation loss decreased (0.284685 --> 0.268360).  Saving model ...\n",
      "Epoch 27/500 - Train Loss: 0.060677 | Val MSE: 0.250377\n",
      "Validation loss decreased (0.268360 --> 0.250377).  Saving model ...\n",
      "Epoch 28/500 - Train Loss: 0.053284 | Val MSE: 0.248636\n",
      "Validation loss decreased (0.250377 --> 0.248636).  Saving model ...\n",
      "Epoch 29/500 - Train Loss: 0.046719 | Val MSE: 0.244135\n",
      "Validation loss decreased (0.248636 --> 0.244135).  Saving model ...\n",
      "Epoch 30/500 - Train Loss: 0.041035 | Val MSE: 0.243472\n",
      "Validation loss decreased (0.244135 --> 0.243472).  Saving model ...\n",
      "Epoch 31/500 - Train Loss: 0.035932 | Val MSE: 0.230877\n",
      "Validation loss decreased (0.243472 --> 0.230877).  Saving model ...\n",
      "Epoch 32/500 - Train Loss: 0.030915 | Val MSE: 0.232168\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 33/500 - Train Loss: 0.026451 | Val MSE: 0.218866\n",
      "Validation loss decreased (0.230877 --> 0.218866).  Saving model ...\n",
      "Epoch 34/500 - Train Loss: 0.022206 | Val MSE: 0.210368\n",
      "Validation loss decreased (0.218866 --> 0.210368).  Saving model ...\n",
      "Epoch 35/500 - Train Loss: 0.018118 | Val MSE: 0.209828\n",
      "Validation loss decreased (0.210368 --> 0.209828).  Saving model ...\n",
      "Epoch 36/500 - Train Loss: 0.014905 | Val MSE: 0.193991\n",
      "Validation loss decreased (0.209828 --> 0.193991).  Saving model ...\n",
      "Epoch 37/500 - Train Loss: 0.012681 | Val MSE: 0.195732\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 38/500 - Train Loss: 0.008814 | Val MSE: 0.194578\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 39/500 - Train Loss: 0.006694 | Val MSE: 0.205761\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 40/500 - Train Loss: 0.004568 | Val MSE: 0.191293\n",
      "Validation loss decreased (0.193991 --> 0.191293).  Saving model ...\n",
      "Epoch 41/500 - Train Loss: 0.002254 | Val MSE: 0.189843\n",
      "Validation loss decreased (0.191293 --> 0.189843).  Saving model ...\n",
      "Epoch 42/500 - Train Loss: 0.000322 | Val MSE: 0.193790\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 43/500 - Train Loss: -0.001457 | Val MSE: 0.182693\n",
      "Validation loss decreased (0.189843 --> 0.182693).  Saving model ...\n",
      "Epoch 44/500 - Train Loss: -0.003652 | Val MSE: 0.177988\n",
      "Validation loss decreased (0.182693 --> 0.177988).  Saving model ...\n",
      "Epoch 45/500 - Train Loss: -0.005415 | Val MSE: 0.184806\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 46/500 - Train Loss: -0.006988 | Val MSE: 0.171947\n",
      "Validation loss decreased (0.177988 --> 0.171947).  Saving model ...\n",
      "Epoch 47/500 - Train Loss: -0.009237 | Val MSE: 0.165172\n",
      "Validation loss decreased (0.171947 --> 0.165172).  Saving model ...\n",
      "Epoch 48/500 - Train Loss: -0.010401 | Val MSE: 0.176298\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 49/500 - Train Loss: -0.012035 | Val MSE: 0.172473\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 50/500 - Train Loss: -0.013350 | Val MSE: 0.189034\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 51/500 - Train Loss: -0.014236 | Val MSE: 0.179159\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 52/500 - Train Loss: -0.015560 | Val MSE: 0.170912\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Block 3/3: Train=13412, Val=11803\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Robust Autoencoder (PRAE)...\n",
      "Training PRAE (lambda=0.100000, Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 11.072474 | Val MSE: 12.496542\n",
      "Validation loss decreased (inf --> 12.496542).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 5.055709 | Val MSE: 5.325858\n",
      "Validation loss decreased (12.496542 --> 5.325858).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 1.790583 | Val MSE: 2.012952\n",
      "Validation loss decreased (5.325858 --> 2.012952).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 0.742336 | Val MSE: 1.031063\n",
      "Validation loss decreased (2.012952 --> 1.031063).  Saving model ...\n",
      "Epoch 5/500 - Train Loss: 0.547945 | Val MSE: 0.810123\n",
      "Validation loss decreased (1.031063 --> 0.810123).  Saving model ...\n",
      "Epoch 6/500 - Train Loss: 0.509503 | Val MSE: 0.732983\n",
      "Validation loss decreased (0.810123 --> 0.732983).  Saving model ...\n",
      "Epoch 7/500 - Train Loss: 0.481316 | Val MSE: 0.716924\n",
      "Validation loss decreased (0.732983 --> 0.716924).  Saving model ...\n",
      "Epoch 8/500 - Train Loss: 0.448490 | Val MSE: 0.657492\n",
      "Validation loss decreased (0.716924 --> 0.657492).  Saving model ...\n",
      "Epoch 9/500 - Train Loss: 0.417542 | Val MSE: 0.678404\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 10/500 - Train Loss: 0.394502 | Val MSE: 0.651818\n",
      "Validation loss decreased (0.657492 --> 0.651818).  Saving model ...\n",
      "Epoch 11/500 - Train Loss: 0.365341 | Val MSE: 0.601748\n",
      "Validation loss decreased (0.651818 --> 0.601748).  Saving model ...\n",
      "Epoch 12/500 - Train Loss: 0.339482 | Val MSE: 0.554177\n",
      "Validation loss decreased (0.601748 --> 0.554177).  Saving model ...\n",
      "Epoch 13/500 - Train Loss: 0.315737 | Val MSE: 0.579819\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 14/500 - Train Loss: 0.290246 | Val MSE: 0.564889\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 15/500 - Train Loss: 0.273704 | Val MSE: 0.545233\n",
      "Validation loss decreased (0.554177 --> 0.545233).  Saving model ...\n",
      "Epoch 16/500 - Train Loss: 0.260303 | Val MSE: 0.475656\n",
      "Validation loss decreased (0.545233 --> 0.475656).  Saving model ...\n",
      "Epoch 17/500 - Train Loss: 0.236538 | Val MSE: 0.488461\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 18/500 - Train Loss: 0.211917 | Val MSE: 0.461462\n",
      "Validation loss decreased (0.475656 --> 0.461462).  Saving model ...\n",
      "Epoch 19/500 - Train Loss: 0.192174 | Val MSE: 0.472677\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 20/500 - Train Loss: 0.173822 | Val MSE: 0.469741\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 21/500 - Train Loss: 0.158723 | Val MSE: 0.436542\n",
      "Validation loss decreased (0.461462 --> 0.436542).  Saving model ...\n",
      "Epoch 22/500 - Train Loss: 0.147165 | Val MSE: 0.449838\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 23/500 - Train Loss: 0.131621 | Val MSE: 0.447642\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 24/500 - Train Loss: 0.121881 | Val MSE: 0.411631\n",
      "Validation loss decreased (0.436542 --> 0.411631).  Saving model ...\n",
      "Epoch 25/500 - Train Loss: 0.111335 | Val MSE: 0.420195\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 26/500 - Train Loss: 0.101483 | Val MSE: 0.426718\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 27/500 - Train Loss: 0.092510 | Val MSE: 0.438107\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 28/500 - Train Loss: 0.083347 | Val MSE: 0.401627\n",
      "Validation loss decreased (0.411631 --> 0.401627).  Saving model ...\n",
      "Epoch 29/500 - Train Loss: 0.074712 | Val MSE: 0.395787\n",
      "Validation loss decreased (0.401627 --> 0.395787).  Saving model ...\n",
      "Epoch 30/500 - Train Loss: 0.069854 | Val MSE: 0.385130\n",
      "Validation loss decreased (0.395787 --> 0.385130).  Saving model ...\n",
      "Epoch 31/500 - Train Loss: 0.065249 | Val MSE: 0.396748\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 32/500 - Train Loss: 0.060110 | Val MSE: 0.385193\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 33/500 - Train Loss: 0.055294 | Val MSE: 0.392895\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 34/500 - Train Loss: 0.050342 | Val MSE: 0.387462\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 35/500 - Train Loss: 0.045248 | Val MSE: 0.389155\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "\n",
      "Evaluating on rest of Day 2...\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Evaluating model...\n",
      "Day 2 Results: AUROC=0.9984, AUPRC=0.9906, F4=0.0017\n",
      "Day 3/7: 2015-01-06-TOTF.PA-book.csv.gz\n",
      "--------------------------------------------------------------------------------\n",
      "Loading data from ../data/TOTF.PA-book\\2015-01-06-TOTF.PA-book.csv.gz...\n",
      "Successfully loaded 1272556 rows.\n",
      "Filtered 1896 pre-market samples.\n",
      "First hour samples: 170233, Rest of day: 1100427\n",
      "Training on 3 time blocks...\n",
      "Block 1/3: Train=30901, Val=36877\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Robust Autoencoder (PRAE)...\n",
      "Training PRAE (lambda=0.100000, Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 155.594313 | Val MSE: 168.004205\n",
      "Validation loss decreased (inf --> 168.004205).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 71.506136 | Val MSE: 63.327689\n",
      "Validation loss decreased (168.004205 --> 63.327689).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 19.452750 | Val MSE: 15.347227\n",
      "Validation loss decreased (63.327689 --> 15.347227).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 3.314413 | Val MSE: 4.091082\n",
      "Validation loss decreased (15.347227 --> 4.091082).  Saving model ...\n",
      "Epoch 5/500 - Train Loss: 0.992844 | Val MSE: 2.409848\n",
      "Validation loss decreased (4.091082 --> 2.409848).  Saving model ...\n",
      "Epoch 6/500 - Train Loss: 0.809182 | Val MSE: 2.182506\n",
      "Validation loss decreased (2.409848 --> 2.182506).  Saving model ...\n",
      "Epoch 7/500 - Train Loss: 0.798412 | Val MSE: 2.147573\n",
      "Validation loss decreased (2.182506 --> 2.147573).  Saving model ...\n",
      "Epoch 8/500 - Train Loss: 0.772050 | Val MSE: 1.947726\n",
      "Validation loss decreased (2.147573 --> 1.947726).  Saving model ...\n",
      "Epoch 9/500 - Train Loss: 0.679024 | Val MSE: 1.704208\n",
      "Validation loss decreased (1.947726 --> 1.704208).  Saving model ...\n",
      "Epoch 10/500 - Train Loss: 0.629060 | Val MSE: 1.591333\n",
      "Validation loss decreased (1.704208 --> 1.591333).  Saving model ...\n",
      "Epoch 11/500 - Train Loss: 0.579467 | Val MSE: 1.653695\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 12/500 - Train Loss: 0.540202 | Val MSE: 1.619889\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 13/500 - Train Loss: 0.509706 | Val MSE: 1.694178\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 14/500 - Train Loss: 0.490439 | Val MSE: 1.714386\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 15/500 - Train Loss: 0.463973 | Val MSE: 1.744718\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Block 2/3: Train=23621, Val=28400\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Robust Autoencoder (PRAE)...\n",
      "Training PRAE (lambda=0.100000, Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 174.040487 | Val MSE: 239.511359\n",
      "Validation loss decreased (inf --> 239.511359).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 107.129705 | Val MSE: 141.689016\n",
      "Validation loss decreased (239.511359 --> 141.689016).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 51.494341 | Val MSE: 67.332026\n",
      "Validation loss decreased (141.689016 --> 67.332026).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 17.813642 | Val MSE: 27.018223\n",
      "Validation loss decreased (67.332026 --> 27.018223).  Saving model ...\n",
      "Epoch 5/500 - Train Loss: 4.432292 | Val MSE: 11.419526\n",
      "Validation loss decreased (27.018223 --> 11.419526).  Saving model ...\n",
      "Epoch 6/500 - Train Loss: 1.101094 | Val MSE: 6.602543\n",
      "Validation loss decreased (11.419526 --> 6.602543).  Saving model ...\n",
      "Epoch 7/500 - Train Loss: 0.551316 | Val MSE: 5.251450\n",
      "Validation loss decreased (6.602543 --> 5.251450).  Saving model ...\n",
      "Epoch 8/500 - Train Loss: 0.480865 | Val MSE: 4.873172\n",
      "Validation loss decreased (5.251450 --> 4.873172).  Saving model ...\n",
      "Epoch 9/500 - Train Loss: 0.474963 | Val MSE: 4.826566\n",
      "Validation loss decreased (4.873172 --> 4.826566).  Saving model ...\n",
      "Epoch 10/500 - Train Loss: 0.472119 | Val MSE: 4.755783\n",
      "Validation loss decreased (4.826566 --> 4.755783).  Saving model ...\n",
      "Epoch 11/500 - Train Loss: 0.474012 | Val MSE: 4.751700\n",
      "Validation loss decreased (4.755783 --> 4.751700).  Saving model ...\n",
      "Epoch 12/500 - Train Loss: 0.473006 | Val MSE: 4.722043\n",
      "Validation loss decreased (4.751700 --> 4.722043).  Saving model ...\n",
      "Epoch 13/500 - Train Loss: 0.472646 | Val MSE: 4.747438\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 14/500 - Train Loss: 0.472077 | Val MSE: 4.701095\n",
      "Validation loss decreased (4.722043 --> 4.701095).  Saving model ...\n",
      "Epoch 15/500 - Train Loss: 0.474406 | Val MSE: 4.759378\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 16/500 - Train Loss: 0.471126 | Val MSE: 4.747887\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 17/500 - Train Loss: 0.473510 | Val MSE: 4.800651\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 18/500 - Train Loss: 0.472238 | Val MSE: 4.732204\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 19/500 - Train Loss: 0.472274 | Val MSE: 4.733813\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Block 3/3: Train=30334, Val=20100\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Robust Autoencoder (PRAE)...\n",
      "Training PRAE (lambda=0.100000, Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 234.715804 | Val MSE: 220.091388\n",
      "Validation loss decreased (inf --> 220.091388).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 125.801648 | Val MSE: 92.716988\n",
      "Validation loss decreased (220.091388 --> 92.716988).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 45.416838 | Val MSE: 22.087430\n",
      "Validation loss decreased (92.716988 --> 22.087430).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 9.926223 | Val MSE: 2.747006\n",
      "Validation loss decreased (22.087430 --> 2.747006).  Saving model ...\n",
      "Epoch 5/500 - Train Loss: 1.702216 | Val MSE: 0.767466\n",
      "Validation loss decreased (2.747006 --> 0.767466).  Saving model ...\n",
      "Epoch 6/500 - Train Loss: 0.661076 | Val MSE: 0.887155\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 7/500 - Train Loss: 0.572136 | Val MSE: 0.957029\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 8/500 - Train Loss: 0.567364 | Val MSE: 0.960895\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 9/500 - Train Loss: 0.552754 | Val MSE: 0.843969\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 10/500 - Train Loss: 0.515629 | Val MSE: 0.818632\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "\n",
      "Evaluating on rest of Day 3...\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Evaluating model...\n",
      "Day 3 Results: AUROC=0.9987, AUPRC=0.9925, F4=0.0013\n",
      "Day 4/7: 2015-01-07-TOTF.PA-book.csv.gz\n",
      "--------------------------------------------------------------------------------\n",
      "Loading data from ../data/TOTF.PA-book\\2015-01-07-TOTF.PA-book.csv.gz...\n",
      "Successfully loaded 1124864 rows.\n",
      "Filtered 2053 pre-market samples.\n",
      "First hour samples: 128092, Rest of day: 994719\n",
      "Training on 3 time blocks...\n",
      "Block 1/3: Train=19569, Val=27024\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Robust Autoencoder (PRAE)...\n",
      "Training PRAE (lambda=0.100000, Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 213.500273 | Val MSE: 237.776525\n",
      "Validation loss decreased (inf --> 237.776525).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 152.525872 | Val MSE: 158.847102\n",
      "Validation loss decreased (237.776525 --> 158.847102).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 96.114249 | Val MSE: 89.497151\n",
      "Validation loss decreased (158.847102 --> 89.497151).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 50.116611 | Val MSE: 41.082465\n",
      "Validation loss decreased (89.497151 --> 41.082465).  Saving model ...\n",
      "Epoch 5/500 - Train Loss: 21.326358 | Val MSE: 14.975951\n",
      "Validation loss decreased (41.082465 --> 14.975951).  Saving model ...\n",
      "Epoch 6/500 - Train Loss: 7.394525 | Val MSE: 4.486306\n",
      "Validation loss decreased (14.975951 --> 4.486306).  Saving model ...\n",
      "Epoch 7/500 - Train Loss: 2.281869 | Val MSE: 1.457009\n",
      "Validation loss decreased (4.486306 --> 1.457009).  Saving model ...\n",
      "Epoch 8/500 - Train Loss: 0.839955 | Val MSE: 0.836839\n",
      "Validation loss decreased (1.457009 --> 0.836839).  Saving model ...\n",
      "Epoch 9/500 - Train Loss: 0.496099 | Val MSE: 0.806149\n",
      "Validation loss decreased (0.836839 --> 0.806149).  Saving model ...\n",
      "Epoch 10/500 - Train Loss: 0.424439 | Val MSE: 0.788936\n",
      "Validation loss decreased (0.806149 --> 0.788936).  Saving model ...\n",
      "Epoch 11/500 - Train Loss: 0.399900 | Val MSE: 0.728801\n",
      "Validation loss decreased (0.788936 --> 0.728801).  Saving model ...\n",
      "Epoch 12/500 - Train Loss: 0.364072 | Val MSE: 0.702741\n",
      "Validation loss decreased (0.728801 --> 0.702741).  Saving model ...\n",
      "Epoch 13/500 - Train Loss: 0.343318 | Val MSE: 0.670388\n",
      "Validation loss decreased (0.702741 --> 0.670388).  Saving model ...\n",
      "Epoch 14/500 - Train Loss: 0.334776 | Val MSE: 0.649928\n",
      "Validation loss decreased (0.670388 --> 0.649928).  Saving model ...\n",
      "Epoch 15/500 - Train Loss: 0.331507 | Val MSE: 0.694190\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 16/500 - Train Loss: 0.326796 | Val MSE: 0.647412\n",
      "Validation loss decreased (0.649928 --> 0.647412).  Saving model ...\n",
      "Epoch 17/500 - Train Loss: 0.312233 | Val MSE: 0.737448\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 18/500 - Train Loss: 0.297405 | Val MSE: 0.670521\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 19/500 - Train Loss: 0.285812 | Val MSE: 0.652966\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 20/500 - Train Loss: 0.273884 | Val MSE: 0.660613\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 21/500 - Train Loss: 0.262773 | Val MSE: 0.642890\n",
      "Validation loss decreased (0.647412 --> 0.642890).  Saving model ...\n",
      "Epoch 22/500 - Train Loss: 0.248866 | Val MSE: 0.636532\n",
      "Validation loss decreased (0.642890 --> 0.636532).  Saving model ...\n",
      "Epoch 23/500 - Train Loss: 0.236673 | Val MSE: 0.657230\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 24/500 - Train Loss: 0.224712 | Val MSE: 0.633568\n",
      "Validation loss decreased (0.636532 --> 0.633568).  Saving model ...\n",
      "Epoch 25/500 - Train Loss: 0.216793 | Val MSE: 0.643281\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 26/500 - Train Loss: 0.208160 | Val MSE: 0.647411\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 27/500 - Train Loss: 0.198034 | Val MSE: 0.640913\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 28/500 - Train Loss: 0.190122 | Val MSE: 0.639658\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 29/500 - Train Loss: 0.179195 | Val MSE: 0.620348\n",
      "Validation loss decreased (0.633568 --> 0.620348).  Saving model ...\n",
      "Epoch 30/500 - Train Loss: 0.171534 | Val MSE: 0.616986\n",
      "Validation loss decreased (0.620348 --> 0.616986).  Saving model ...\n",
      "Epoch 31/500 - Train Loss: 0.164209 | Val MSE: 0.624138\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 32/500 - Train Loss: 0.156933 | Val MSE: 0.627682\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 33/500 - Train Loss: 0.148164 | Val MSE: 0.651500\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 34/500 - Train Loss: 0.140614 | Val MSE: 0.643203\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 35/500 - Train Loss: 0.135556 | Val MSE: 0.641197\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Block 2/3: Train=27589, Val=28408\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Robust Autoencoder (PRAE)...\n",
      "Training PRAE (lambda=0.100000, Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 164.052823 | Val MSE: 132.237732\n",
      "Validation loss decreased (inf --> 132.237732).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 87.400969 | Val MSE: 51.131694\n",
      "Validation loss decreased (132.237732 --> 51.131694).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 31.581392 | Val MSE: 10.318874\n",
      "Validation loss decreased (51.131694 --> 10.318874).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 7.268552 | Val MSE: 1.855391\n",
      "Validation loss decreased (10.318874 --> 1.855391).  Saving model ...\n",
      "Epoch 5/500 - Train Loss: 1.564072 | Val MSE: 2.502766\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 6/500 - Train Loss: 0.798979 | Val MSE: 3.178996\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 7/500 - Train Loss: 0.728670 | Val MSE: 3.406840\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 8/500 - Train Loss: 0.721780 | Val MSE: 3.394236\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 9/500 - Train Loss: 0.716626 | Val MSE: 3.302914\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Block 3/3: Train=14055, Val=11447\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Robust Autoencoder (PRAE)...\n",
      "Training PRAE (lambda=0.100000, Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 172.506729 | Val MSE: 207.246800\n",
      "Validation loss decreased (inf --> 207.246800).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 138.430688 | Val MSE: 160.799810\n",
      "Validation loss decreased (207.246800 --> 160.799810).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 103.222136 | Val MSE: 115.843032\n",
      "Validation loss decreased (160.799810 --> 115.843032).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 71.362185 | Val MSE: 76.589241\n",
      "Validation loss decreased (115.843032 --> 76.589241).  Saving model ...\n",
      "Epoch 5/500 - Train Loss: 44.998824 | Val MSE: 45.869829\n",
      "Validation loss decreased (76.589241 --> 45.869829).  Saving model ...\n",
      "Epoch 6/500 - Train Loss: 25.650736 | Val MSE: 24.704383\n",
      "Validation loss decreased (45.869829 --> 24.704383).  Saving model ...\n",
      "Epoch 7/500 - Train Loss: 13.030565 | Val MSE: 12.024229\n",
      "Validation loss decreased (24.704383 --> 12.024229).  Saving model ...\n",
      "Epoch 8/500 - Train Loss: 6.007228 | Val MSE: 5.495163\n",
      "Validation loss decreased (12.024229 --> 5.495163).  Saving model ...\n",
      "Epoch 9/500 - Train Loss: 2.633662 | Val MSE: 2.601225\n",
      "Validation loss decreased (5.495163 --> 2.601225).  Saving model ...\n",
      "Epoch 10/500 - Train Loss: 1.214644 | Val MSE: 1.479494\n",
      "Validation loss decreased (2.601225 --> 1.479494).  Saving model ...\n",
      "Epoch 11/500 - Train Loss: 0.712390 | Val MSE: 1.079426\n",
      "Validation loss decreased (1.479494 --> 1.079426).  Saving model ...\n",
      "Epoch 12/500 - Train Loss: 0.538668 | Val MSE: 0.957843\n",
      "Validation loss decreased (1.079426 --> 0.957843).  Saving model ...\n",
      "Epoch 13/500 - Train Loss: 0.484121 | Val MSE: 0.897327\n",
      "Validation loss decreased (0.957843 --> 0.897327).  Saving model ...\n",
      "Epoch 14/500 - Train Loss: 0.452047 | Val MSE: 0.871028\n",
      "Validation loss decreased (0.897327 --> 0.871028).  Saving model ...\n",
      "Epoch 15/500 - Train Loss: 0.433575 | Val MSE: 0.856765\n",
      "Validation loss decreased (0.871028 --> 0.856765).  Saving model ...\n",
      "Epoch 16/500 - Train Loss: 0.420503 | Val MSE: 0.848210\n",
      "Validation loss decreased (0.856765 --> 0.848210).  Saving model ...\n",
      "Epoch 17/500 - Train Loss: 0.417276 | Val MSE: 0.852126\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 18/500 - Train Loss: 0.411259 | Val MSE: 0.852303\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 19/500 - Train Loss: 0.407264 | Val MSE: 0.853144\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 20/500 - Train Loss: 0.404884 | Val MSE: 0.852611\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 21/500 - Train Loss: 0.393739 | Val MSE: 0.832190\n",
      "Validation loss decreased (0.848210 --> 0.832190).  Saving model ...\n",
      "Epoch 22/500 - Train Loss: 0.386244 | Val MSE: 0.823148\n",
      "Validation loss decreased (0.832190 --> 0.823148).  Saving model ...\n",
      "Epoch 23/500 - Train Loss: 0.366679 | Val MSE: 0.816933\n",
      "Validation loss decreased (0.823148 --> 0.816933).  Saving model ...\n",
      "Epoch 24/500 - Train Loss: 0.350686 | Val MSE: 0.785781\n",
      "Validation loss decreased (0.816933 --> 0.785781).  Saving model ...\n",
      "Epoch 25/500 - Train Loss: 0.332121 | Val MSE: 0.779834\n",
      "Validation loss decreased (0.785781 --> 0.779834).  Saving model ...\n",
      "Epoch 26/500 - Train Loss: 0.320429 | Val MSE: 0.771433\n",
      "Validation loss decreased (0.779834 --> 0.771433).  Saving model ...\n",
      "Epoch 27/500 - Train Loss: 0.304044 | Val MSE: 0.735353\n",
      "Validation loss decreased (0.771433 --> 0.735353).  Saving model ...\n",
      "Epoch 28/500 - Train Loss: 0.290307 | Val MSE: 0.741386\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 29/500 - Train Loss: 0.276474 | Val MSE: 0.703655\n",
      "Validation loss decreased (0.735353 --> 0.703655).  Saving model ...\n",
      "Epoch 30/500 - Train Loss: 0.267811 | Val MSE: 0.685734\n",
      "Validation loss decreased (0.703655 --> 0.685734).  Saving model ...\n",
      "Epoch 31/500 - Train Loss: 0.253091 | Val MSE: 0.687114\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 32/500 - Train Loss: 0.242230 | Val MSE: 0.671798\n",
      "Validation loss decreased (0.685734 --> 0.671798).  Saving model ...\n",
      "Epoch 33/500 - Train Loss: 0.232807 | Val MSE: 0.678324\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 34/500 - Train Loss: 0.223237 | Val MSE: 0.651355\n",
      "Validation loss decreased (0.671798 --> 0.651355).  Saving model ...\n",
      "Epoch 35/500 - Train Loss: 0.214412 | Val MSE: 0.641952\n",
      "Validation loss decreased (0.651355 --> 0.641952).  Saving model ...\n",
      "Epoch 36/500 - Train Loss: 0.205090 | Val MSE: 0.640621\n",
      "Validation loss decreased (0.641952 --> 0.640621).  Saving model ...\n",
      "Epoch 37/500 - Train Loss: 0.195082 | Val MSE: 0.620275\n",
      "Validation loss decreased (0.640621 --> 0.620275).  Saving model ...\n",
      "Epoch 38/500 - Train Loss: 0.187714 | Val MSE: 0.614154\n",
      "Validation loss decreased (0.620275 --> 0.614154).  Saving model ...\n",
      "Epoch 39/500 - Train Loss: 0.179846 | Val MSE: 0.606706\n",
      "Validation loss decreased (0.614154 --> 0.606706).  Saving model ...\n",
      "Epoch 40/500 - Train Loss: 0.167183 | Val MSE: 0.579646\n",
      "Validation loss decreased (0.606706 --> 0.579646).  Saving model ...\n",
      "Epoch 41/500 - Train Loss: 0.157913 | Val MSE: 0.588110\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 42/500 - Train Loss: 0.146161 | Val MSE: 0.560925\n",
      "Validation loss decreased (0.579646 --> 0.560925).  Saving model ...\n",
      "Epoch 43/500 - Train Loss: 0.141719 | Val MSE: 0.540396\n",
      "Validation loss decreased (0.560925 --> 0.540396).  Saving model ...\n",
      "Epoch 44/500 - Train Loss: 0.132509 | Val MSE: 0.533190\n",
      "Validation loss decreased (0.540396 --> 0.533190).  Saving model ...\n",
      "Epoch 45/500 - Train Loss: 0.126011 | Val MSE: 0.555535\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 46/500 - Train Loss: 0.117420 | Val MSE: 0.503252\n",
      "Validation loss decreased (0.533190 --> 0.503252).  Saving model ...\n",
      "Epoch 47/500 - Train Loss: 0.112126 | Val MSE: 0.517894\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 48/500 - Train Loss: 0.106104 | Val MSE: 0.516690\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 49/500 - Train Loss: 0.098592 | Val MSE: 0.513507\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 50/500 - Train Loss: 0.092681 | Val MSE: 0.485607\n",
      "Validation loss decreased (0.503252 --> 0.485607).  Saving model ...\n",
      "Epoch 51/500 - Train Loss: 0.088172 | Val MSE: 0.493302\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 52/500 - Train Loss: 0.081290 | Val MSE: 0.500168\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 53/500 - Train Loss: 0.076947 | Val MSE: 0.475637\n",
      "Validation loss decreased (0.485607 --> 0.475637).  Saving model ...\n",
      "Epoch 54/500 - Train Loss: 0.073485 | Val MSE: 0.501129\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 55/500 - Train Loss: 0.069888 | Val MSE: 0.486528\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 56/500 - Train Loss: 0.064959 | Val MSE: 0.473729\n",
      "Validation loss decreased (0.475637 --> 0.473729).  Saving model ...\n",
      "Epoch 57/500 - Train Loss: 0.060947 | Val MSE: 0.449008\n",
      "Validation loss decreased (0.473729 --> 0.449008).  Saving model ...\n",
      "Epoch 58/500 - Train Loss: 0.057951 | Val MSE: 0.450280\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 59/500 - Train Loss: 0.053489 | Val MSE: 0.449642\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 60/500 - Train Loss: 0.050505 | Val MSE: 0.464187\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 61/500 - Train Loss: 0.046718 | Val MSE: 0.453662\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 62/500 - Train Loss: 0.043357 | Val MSE: 0.455612\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "\n",
      "Evaluating on rest of Day 4...\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Evaluating model...\n",
      "Day 4 Results: AUROC=0.9975, AUPRC=0.9868, F4=0.0026\n",
      "Day 5/7: 2015-01-08-TOTF.PA-book.csv.gz\n",
      "--------------------------------------------------------------------------------\n",
      "Loading data from ../data/TOTF.PA-book\\2015-01-08-TOTF.PA-book.csv.gz...\n",
      "Successfully loaded 1101381 rows.\n",
      "Filtered 1868 pre-market samples.\n",
      "First hour samples: 125163, Rest of day: 974350\n",
      "Training on 3 time blocks...\n",
      "Block 1/3: Train=17618, Val=32995\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Robust Autoencoder (PRAE)...\n",
      "Training PRAE (lambda=0.100000, Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 61.171684 | Val MSE: 81.144852\n",
      "Validation loss decreased (inf --> 81.144852).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 36.368501 | Val MSE: 46.407636\n",
      "Validation loss decreased (81.144852 --> 46.407636).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 17.204744 | Val MSE: 21.958781\n",
      "Validation loss decreased (46.407636 --> 21.958781).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 6.319150 | Val MSE: 9.329943\n",
      "Validation loss decreased (21.958781 --> 9.329943).  Saving model ...\n",
      "Epoch 5/500 - Train Loss: 2.062882 | Val MSE: 4.385510\n",
      "Validation loss decreased (9.329943 --> 4.385510).  Saving model ...\n",
      "Epoch 6/500 - Train Loss: 0.933140 | Val MSE: 2.736455\n",
      "Validation loss decreased (4.385510 --> 2.736455).  Saving model ...\n",
      "Epoch 7/500 - Train Loss: 0.666000 | Val MSE: 3.013335\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 8/500 - Train Loss: 0.581215 | Val MSE: 1.945171\n",
      "Validation loss decreased (2.736455 --> 1.945171).  Saving model ...\n",
      "Epoch 9/500 - Train Loss: 0.516908 | Val MSE: 1.736587\n",
      "Validation loss decreased (1.945171 --> 1.736587).  Saving model ...\n",
      "Epoch 10/500 - Train Loss: 0.491795 | Val MSE: 1.761635\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 11/500 - Train Loss: 0.474757 | Val MSE: 1.731532\n",
      "Validation loss decreased (1.736587 --> 1.731532).  Saving model ...\n",
      "Epoch 12/500 - Train Loss: 0.446729 | Val MSE: 1.989107\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 13/500 - Train Loss: 0.438183 | Val MSE: 1.477595\n",
      "Validation loss decreased (1.731532 --> 1.477595).  Saving model ...\n",
      "Epoch 14/500 - Train Loss: 0.421779 | Val MSE: 1.623567\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 15/500 - Train Loss: 0.407074 | Val MSE: 1.736473\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 16/500 - Train Loss: 0.398466 | Val MSE: 1.731271\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 17/500 - Train Loss: 0.370793 | Val MSE: 1.575347\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 18/500 - Train Loss: 0.361447 | Val MSE: 1.700003\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Block 2/3: Train=32355, Val=16660\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Robust Autoencoder (PRAE)...\n",
      "Training PRAE (lambda=0.100000, Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 53.930910 | Val MSE: 26.619934\n",
      "Validation loss decreased (inf --> 26.619934).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 13.549960 | Val MSE: 1.906661\n",
      "Validation loss decreased (26.619934 --> 1.906661).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 1.436890 | Val MSE: 1.178385\n",
      "Validation loss decreased (1.906661 --> 1.178385).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 0.401529 | Val MSE: 1.651925\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 5/500 - Train Loss: 0.357969 | Val MSE: 1.855056\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 6/500 - Train Loss: 0.304634 | Val MSE: 1.759117\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 7/500 - Train Loss: 0.267655 | Val MSE: 1.431379\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 8/500 - Train Loss: 0.242436 | Val MSE: 1.692272\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Block 3/3: Train=11364, Val=14171\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Robust Autoencoder (PRAE)...\n",
      "Training PRAE (lambda=0.100000, Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 70.108359 | Val MSE: 73.055428\n",
      "Validation loss decreased (inf --> 73.055428).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 55.441029 | Val MSE: 54.495298\n",
      "Validation loss decreased (73.055428 --> 54.495298).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 40.568552 | Val MSE: 37.513839\n",
      "Validation loss decreased (54.495298 --> 37.513839).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 27.513085 | Val MSE: 23.423313\n",
      "Validation loss decreased (37.513839 --> 23.423313).  Saving model ...\n",
      "Epoch 5/500 - Train Loss: 17.135445 | Val MSE: 13.011602\n",
      "Validation loss decreased (23.423313 --> 13.011602).  Saving model ...\n",
      "Epoch 6/500 - Train Loss: 9.699083 | Val MSE: 6.323538\n",
      "Validation loss decreased (13.011602 --> 6.323538).  Saving model ...\n",
      "Epoch 7/500 - Train Loss: 4.899168 | Val MSE: 2.740542\n",
      "Validation loss decreased (6.323538 --> 2.740542).  Saving model ...\n",
      "Epoch 8/500 - Train Loss: 2.329962 | Val MSE: 1.172326\n",
      "Validation loss decreased (2.740542 --> 1.172326).  Saving model ...\n",
      "Epoch 9/500 - Train Loss: 1.068431 | Val MSE: 0.688087\n",
      "Validation loss decreased (1.172326 --> 0.688087).  Saving model ...\n",
      "Epoch 10/500 - Train Loss: 0.550226 | Val MSE: 0.643783\n",
      "Validation loss decreased (0.688087 --> 0.643783).  Saving model ...\n",
      "Epoch 11/500 - Train Loss: 0.352676 | Val MSE: 0.718043\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 12/500 - Train Loss: 0.281508 | Val MSE: 0.797590\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 13/500 - Train Loss: 0.260002 | Val MSE: 0.844529\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 14/500 - Train Loss: 0.251801 | Val MSE: 0.866094\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 15/500 - Train Loss: 0.240280 | Val MSE: 0.899352\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "\n",
      "Evaluating on rest of Day 5...\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Evaluating model...\n",
      "Day 5 Results: AUROC=0.9982, AUPRC=0.9901, F4=0.0019\n",
      "Day 6/7: 2015-01-09-TOTF.PA-book.csv.gz\n",
      "--------------------------------------------------------------------------------\n",
      "Loading data from ../data/TOTF.PA-book\\2015-01-09-TOTF.PA-book.csv.gz...\n",
      "Successfully loaded 1278519 rows.\n",
      "Filtered 2045 pre-market samples.\n",
      "First hour samples: 107573, Rest of day: 1168901\n",
      "Training on 3 time blocks...\n",
      "Block 1/3: Train=19152, Val=20617\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Robust Autoencoder (PRAE)...\n",
      "Training PRAE (lambda=0.100000, Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 13.446333 | Val MSE: 8.011591\n",
      "Validation loss decreased (inf --> 8.011591).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 4.085964 | Val MSE: 1.518092\n",
      "Validation loss decreased (8.011591 --> 1.518092).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 1.098721 | Val MSE: 0.669451\n",
      "Validation loss decreased (1.518092 --> 0.669451).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 0.653255 | Val MSE: 0.644190\n",
      "Validation loss decreased (0.669451 --> 0.644190).  Saving model ...\n",
      "Epoch 5/500 - Train Loss: 0.564267 | Val MSE: 0.608476\n",
      "Validation loss decreased (0.644190 --> 0.608476).  Saving model ...\n",
      "Epoch 6/500 - Train Loss: 0.508032 | Val MSE: 0.600737\n",
      "Validation loss decreased (0.608476 --> 0.600737).  Saving model ...\n",
      "Epoch 7/500 - Train Loss: 0.454156 | Val MSE: 0.575724\n",
      "Validation loss decreased (0.600737 --> 0.575724).  Saving model ...\n",
      "Epoch 8/500 - Train Loss: 0.421692 | Val MSE: 0.591397\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 9/500 - Train Loss: 0.388017 | Val MSE: 0.574239\n",
      "Validation loss decreased (0.575724 --> 0.574239).  Saving model ...\n",
      "Epoch 10/500 - Train Loss: 0.357140 | Val MSE: 0.557176\n",
      "Validation loss decreased (0.574239 --> 0.557176).  Saving model ...\n",
      "Epoch 11/500 - Train Loss: 0.322792 | Val MSE: 0.556332\n",
      "Validation loss decreased (0.557176 --> 0.556332).  Saving model ...\n",
      "Epoch 12/500 - Train Loss: 0.300215 | Val MSE: 0.534214\n",
      "Validation loss decreased (0.556332 --> 0.534214).  Saving model ...\n",
      "Epoch 13/500 - Train Loss: 0.272196 | Val MSE: 0.548651\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 14/500 - Train Loss: 0.254948 | Val MSE: 0.527847\n",
      "Validation loss decreased (0.534214 --> 0.527847).  Saving model ...\n",
      "Epoch 15/500 - Train Loss: 0.231210 | Val MSE: 0.491111\n",
      "Validation loss decreased (0.527847 --> 0.491111).  Saving model ...\n",
      "Epoch 16/500 - Train Loss: 0.208075 | Val MSE: 0.477771\n",
      "Validation loss decreased (0.491111 --> 0.477771).  Saving model ...\n",
      "Epoch 17/500 - Train Loss: 0.186339 | Val MSE: 0.469771\n",
      "Validation loss decreased (0.477771 --> 0.469771).  Saving model ...\n",
      "Epoch 18/500 - Train Loss: 0.170298 | Val MSE: 0.452785\n",
      "Validation loss decreased (0.469771 --> 0.452785).  Saving model ...\n",
      "Epoch 19/500 - Train Loss: 0.153267 | Val MSE: 0.449134\n",
      "Validation loss decreased (0.452785 --> 0.449134).  Saving model ...\n",
      "Epoch 20/500 - Train Loss: 0.139427 | Val MSE: 0.427241\n",
      "Validation loss decreased (0.449134 --> 0.427241).  Saving model ...\n",
      "Epoch 21/500 - Train Loss: 0.129567 | Val MSE: 0.426616\n",
      "Validation loss decreased (0.427241 --> 0.426616).  Saving model ...\n",
      "Epoch 22/500 - Train Loss: 0.117435 | Val MSE: 0.427306\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 23/500 - Train Loss: 0.105862 | Val MSE: 0.424881\n",
      "Validation loss decreased (0.426616 --> 0.424881).  Saving model ...\n",
      "Epoch 24/500 - Train Loss: 0.096639 | Val MSE: 0.401750\n",
      "Validation loss decreased (0.424881 --> 0.401750).  Saving model ...\n",
      "Epoch 25/500 - Train Loss: 0.088288 | Val MSE: 0.413329\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 26/500 - Train Loss: 0.080331 | Val MSE: 0.410857\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 27/500 - Train Loss: 0.077092 | Val MSE: 0.409360\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 28/500 - Train Loss: 0.069539 | Val MSE: 0.409399\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 29/500 - Train Loss: 0.064144 | Val MSE: 0.388651\n",
      "Validation loss decreased (0.401750 --> 0.388651).  Saving model ...\n",
      "Epoch 30/500 - Train Loss: 0.059044 | Val MSE: 0.397595\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 31/500 - Train Loss: 0.052275 | Val MSE: 0.380420\n",
      "Validation loss decreased (0.388651 --> 0.380420).  Saving model ...\n",
      "Epoch 32/500 - Train Loss: 0.045683 | Val MSE: 0.384373\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 33/500 - Train Loss: 0.040952 | Val MSE: 0.378252\n",
      "Validation loss decreased (0.380420 --> 0.378252).  Saving model ...\n",
      "Epoch 34/500 - Train Loss: 0.036987 | Val MSE: 0.383905\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 35/500 - Train Loss: 0.033137 | Val MSE: 0.373880\n",
      "Validation loss decreased (0.378252 --> 0.373880).  Saving model ...\n",
      "Epoch 36/500 - Train Loss: 0.031457 | Val MSE: 0.380649\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 37/500 - Train Loss: 0.028646 | Val MSE: 0.376481\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 38/500 - Train Loss: 0.024857 | Val MSE: 0.362851\n",
      "Validation loss decreased (0.373880 --> 0.362851).  Saving model ...\n",
      "Epoch 39/500 - Train Loss: 0.022654 | Val MSE: 0.363211\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 40/500 - Train Loss: 0.019492 | Val MSE: 0.369428\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 41/500 - Train Loss: 0.017946 | Val MSE: 0.370117\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 42/500 - Train Loss: 0.016293 | Val MSE: 0.378012\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 43/500 - Train Loss: 0.014199 | Val MSE: 0.358324\n",
      "Validation loss decreased (0.362851 --> 0.358324).  Saving model ...\n",
      "Epoch 44/500 - Train Loss: 0.012994 | Val MSE: 0.361480\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 45/500 - Train Loss: 0.011275 | Val MSE: 0.370966\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 46/500 - Train Loss: 0.010563 | Val MSE: 0.368101\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 47/500 - Train Loss: 0.008876 | Val MSE: 0.359844\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 48/500 - Train Loss: 0.007343 | Val MSE: 0.367105\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Block 2/3: Train=16206, Val=22140\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Robust Autoencoder (PRAE)...\n",
      "Training PRAE (lambda=0.100000, Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 11.828492 | Val MSE: 5.703031\n",
      "Validation loss decreased (inf --> 5.703031).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 4.258703 | Val MSE: 1.138323\n",
      "Validation loss decreased (5.703031 --> 1.138323).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 1.018164 | Val MSE: 0.745069\n",
      "Validation loss decreased (1.138323 --> 0.745069).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 0.359492 | Val MSE: 1.091573\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 5/500 - Train Loss: 0.289110 | Val MSE: 1.140053\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 6/500 - Train Loss: 0.265443 | Val MSE: 1.140137\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 7/500 - Train Loss: 0.241699 | Val MSE: 1.124876\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 8/500 - Train Loss: 0.215620 | Val MSE: 1.080268\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Block 3/3: Train=13494, Val=15964\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Robust Autoencoder (PRAE)...\n",
      "Training PRAE (lambda=0.100000, Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 17.576827 | Val MSE: 15.789635\n",
      "Validation loss decreased (inf --> 15.789635).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 9.308537 | Val MSE: 6.995399\n",
      "Validation loss decreased (15.789635 --> 6.995399).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 3.808381 | Val MSE: 2.352643\n",
      "Validation loss decreased (6.995399 --> 2.352643).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 1.317589 | Val MSE: 0.826505\n",
      "Validation loss decreased (2.352643 --> 0.826505).  Saving model ...\n",
      "Epoch 5/500 - Train Loss: 0.572383 | Val MSE: 0.573458\n",
      "Validation loss decreased (0.826505 --> 0.573458).  Saving model ...\n",
      "Epoch 6/500 - Train Loss: 0.428446 | Val MSE: 0.574405\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 7/500 - Train Loss: 0.407918 | Val MSE: 0.580335\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 8/500 - Train Loss: 0.409577 | Val MSE: 0.585546\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 9/500 - Train Loss: 0.375167 | Val MSE: 0.608527\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 10/500 - Train Loss: 0.355986 | Val MSE: 0.618766\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "\n",
      "Evaluating on rest of Day 6...\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Evaluating model...\n",
      "Day 6 Results: AUROC=0.9987, AUPRC=0.9924, F4=0.0014\n",
      "Day 7/7: 2015-01-12-TOTF.PA-book.csv.gz\n",
      "--------------------------------------------------------------------------------\n",
      "Loading data from ../data/TOTF.PA-book\\2015-01-12-TOTF.PA-book.csv.gz...\n",
      "Successfully loaded 1178388 rows.\n",
      "Filtered 2074 pre-market samples.\n",
      "First hour samples: 97467, Rest of day: 1078847\n",
      "Training on 3 time blocks...\n",
      "Block 1/3: Train=13396, Val=19727\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Robust Autoencoder (PRAE)...\n",
      "Training PRAE (lambda=0.100000, Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 72.682763 | Val MSE: 99.054554\n",
      "Validation loss decreased (inf --> 99.054554).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 54.187193 | Val MSE: 71.153177\n",
      "Validation loss decreased (99.054554 --> 71.153177).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 35.403625 | Val MSE: 46.384942\n",
      "Validation loss decreased (71.153177 --> 46.384942).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 20.943261 | Val MSE: 27.288882\n",
      "Validation loss decreased (46.384942 --> 27.288882).  Saving model ...\n",
      "Epoch 5/500 - Train Loss: 10.510939 | Val MSE: 14.680214\n",
      "Validation loss decreased (27.288882 --> 14.680214).  Saving model ...\n",
      "Epoch 6/500 - Train Loss: 4.663313 | Val MSE: 7.599405\n",
      "Validation loss decreased (14.680214 --> 7.599405).  Saving model ...\n",
      "Epoch 7/500 - Train Loss: 1.899983 | Val MSE: 4.147911\n",
      "Validation loss decreased (7.599405 --> 4.147911).  Saving model ...\n",
      "Epoch 8/500 - Train Loss: 0.809311 | Val MSE: 2.642626\n",
      "Validation loss decreased (4.147911 --> 2.642626).  Saving model ...\n",
      "Epoch 9/500 - Train Loss: 0.460352 | Val MSE: 2.026805\n",
      "Validation loss decreased (2.642626 --> 2.026805).  Saving model ...\n",
      "Epoch 10/500 - Train Loss: 0.361598 | Val MSE: 1.779801\n",
      "Validation loss decreased (2.026805 --> 1.779801).  Saving model ...\n",
      "Epoch 11/500 - Train Loss: 0.335178 | Val MSE: 1.686353\n",
      "Validation loss decreased (1.779801 --> 1.686353).  Saving model ...\n",
      "Epoch 12/500 - Train Loss: 0.330251 | Val MSE: 1.654887\n",
      "Validation loss decreased (1.686353 --> 1.654887).  Saving model ...\n",
      "Epoch 13/500 - Train Loss: 0.328151 | Val MSE: 1.662420\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 14/500 - Train Loss: 0.304350 | Val MSE: 1.564639\n",
      "Validation loss decreased (1.654887 --> 1.564639).  Saving model ...\n",
      "Epoch 15/500 - Train Loss: 0.283392 | Val MSE: 1.530573\n",
      "Validation loss decreased (1.564639 --> 1.530573).  Saving model ...\n",
      "Epoch 16/500 - Train Loss: 0.274296 | Val MSE: 1.764513\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 17/500 - Train Loss: 0.270646 | Val MSE: 1.607443\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 18/500 - Train Loss: 0.266184 | Val MSE: 1.556427\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 19/500 - Train Loss: 0.263954 | Val MSE: 1.571926\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 20/500 - Train Loss: 0.255831 | Val MSE: 1.741692\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Block 2/3: Train=18530, Val=17941\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Robust Autoencoder (PRAE)...\n",
      "Training PRAE (lambda=0.100000, Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 78.691761 | Val MSE: 68.359262\n",
      "Validation loss decreased (inf --> 68.359262).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 48.247733 | Val MSE: 34.178837\n",
      "Validation loss decreased (68.359262 --> 34.178837).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 23.043561 | Val MSE: 12.012627\n",
      "Validation loss decreased (34.178837 --> 12.012627).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 8.403699 | Val MSE: 2.593720\n",
      "Validation loss decreased (12.012627 --> 2.593720).  Saving model ...\n",
      "Epoch 5/500 - Train Loss: 2.417866 | Val MSE: 0.638436\n",
      "Validation loss decreased (2.593720 --> 0.638436).  Saving model ...\n",
      "Epoch 6/500 - Train Loss: 0.752888 | Val MSE: 0.841736\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 7/500 - Train Loss: 0.447259 | Val MSE: 1.125582\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 8/500 - Train Loss: 0.395214 | Val MSE: 1.228393\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 9/500 - Train Loss: 0.374127 | Val MSE: 1.221017\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 10/500 - Train Loss: 0.349961 | Val MSE: 1.341920\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Block 3/3: Train=12909, Val=14964\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Robust Autoencoder (PRAE)...\n",
      "Training PRAE (lambda=0.100000, Max Epochs=500)...\n",
      "Epoch 1/500 - Train Loss: 73.649502 | Val MSE: 91.577302\n",
      "Validation loss decreased (inf --> 91.577302).  Saving model ...\n",
      "Epoch 2/500 - Train Loss: 55.337891 | Val MSE: 66.087278\n",
      "Validation loss decreased (91.577302 --> 66.087278).  Saving model ...\n",
      "Epoch 3/500 - Train Loss: 37.525134 | Val MSE: 43.402508\n",
      "Validation loss decreased (66.087278 --> 43.402508).  Saving model ...\n",
      "Epoch 4/500 - Train Loss: 22.873766 | Val MSE: 25.630711\n",
      "Validation loss decreased (43.402508 --> 25.630711).  Saving model ...\n",
      "Epoch 5/500 - Train Loss: 12.345003 | Val MSE: 13.565413\n",
      "Validation loss decreased (25.630711 --> 13.565413).  Saving model ...\n",
      "Epoch 6/500 - Train Loss: 5.776276 | Val MSE: 6.635718\n",
      "Validation loss decreased (13.565413 --> 6.635718).  Saving model ...\n",
      "Epoch 7/500 - Train Loss: 2.432341 | Val MSE: 3.233869\n",
      "Validation loss decreased (6.635718 --> 3.233869).  Saving model ...\n",
      "Epoch 8/500 - Train Loss: 1.005766 | Val MSE: 1.784817\n",
      "Validation loss decreased (3.233869 --> 1.784817).  Saving model ...\n",
      "Epoch 9/500 - Train Loss: 0.488809 | Val MSE: 1.227827\n",
      "Validation loss decreased (1.784817 --> 1.227827).  Saving model ...\n",
      "Epoch 10/500 - Train Loss: 0.323209 | Val MSE: 1.020840\n",
      "Validation loss decreased (1.227827 --> 1.020840).  Saving model ...\n",
      "Epoch 11/500 - Train Loss: 0.279899 | Val MSE: 0.944968\n",
      "Validation loss decreased (1.020840 --> 0.944968).  Saving model ...\n",
      "Epoch 12/500 - Train Loss: 0.269369 | Val MSE: 0.910501\n",
      "Validation loss decreased (0.944968 --> 0.910501).  Saving model ...\n",
      "Epoch 13/500 - Train Loss: 0.262677 | Val MSE: 0.890819\n",
      "Validation loss decreased (0.910501 --> 0.890819).  Saving model ...\n",
      "Epoch 14/500 - Train Loss: 0.251765 | Val MSE: 0.876532\n",
      "Validation loss decreased (0.890819 --> 0.876532).  Saving model ...\n",
      "Epoch 15/500 - Train Loss: 0.238194 | Val MSE: 0.874624\n",
      "Validation loss decreased (0.876532 --> 0.874624).  Saving model ...\n",
      "Epoch 16/500 - Train Loss: 0.227873 | Val MSE: 0.864406\n",
      "Validation loss decreased (0.874624 --> 0.864406).  Saving model ...\n",
      "Epoch 17/500 - Train Loss: 0.218203 | Val MSE: 0.823642\n",
      "Validation loss decreased (0.864406 --> 0.823642).  Saving model ...\n",
      "Epoch 18/500 - Train Loss: 0.212318 | Val MSE: 0.843350\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 19/500 - Train Loss: 0.202219 | Val MSE: 0.852718\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 20/500 - Train Loss: 0.193645 | Val MSE: 0.847145\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 21/500 - Train Loss: 0.184215 | Val MSE: 0.842912\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 22/500 - Train Loss: 0.179012 | Val MSE: 0.834847\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "\n",
      "Evaluating on rest of Day 7...\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Evaluating model...\n",
      "Day 7 Results: AUROC=0.9984, AUPRC=0.9909, F4=0.0017\n",
      "\n",
      "================================================================================\n",
      "Final Day (Test Day): 2015-01-13-TOTF.PA-book.csv.gz\n",
      "--------------------------------------------------------------------------------\n",
      "Loading data from ../data/TOTF.PA-book\\2015-01-13-TOTF.PA-book.csv.gz...\n",
      "Successfully loaded 1211246 rows.\n",
      "Filtered 2059 pre-market samples.\n",
      "Morning samples: 163357, Rest of day: 1045830\n",
      "\n",
      "Evaluating on Final Day Morning...\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Evaluating model...\n",
      "Final Day Morning: AUROC=1.0000, AUPRC=1.0000, F4=1.0000\n",
      "\n",
      "Evaluating on Final Day Rest...\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Evaluating model...\n",
      "Final Day Rest: AUROC=0.9983, AUPRC=0.9901, F4=0.0012\n",
      "\n",
      "================================================================================\n",
      "Sequential Training Complete!\n",
      "\n",
      "Saved PRAE model to ../models/TOTF_box-cox_prae_sequential\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAINING: PRAE\")\n",
    "\n",
    "results_prae, pipeline_prae = sequential_training_pipeline(\n",
    "    data_dir=DATA_DIR,\n",
    "    num_days=NUM_DAYS,\n",
    "    first_hour_minutes=FIRST_HOUR_MINUTES,\n",
    "    train_block_minutes=TRAIN_BLOCK_MINUTES,\n",
    "    val_block_minutes=VAL_BLOCK_MINUTES,\n",
    "    model_type='prae',\n",
    "    feature_sets=FEATURE_SETS,\n",
    "    epochs=EPOCHS,\n",
    "    lr=LR,\n",
    "    lambda_reg=0.1,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    patience=PATIENCE,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Save results\n",
    "with open('../models/TOTF_sequential_prae_results.json', 'w') as f:\n",
    "    json.dump(results_prae, f, indent=2)\n",
    "\n",
    "# Save trained model\n",
    "base_filename = f\"../models/TOTF_{SCALER_TYPE}_prae_sequential\"\n",
    "torch.save(pipeline_prae.model.state_dict(), f\"{base_filename}_weights.pth\")\n",
    "joblib.dump(pipeline_prae.scaler, f\"{base_filename}_scaler.pkl\")\n",
    "\n",
    "config = {\n",
    "    'dataset': 'TOTF',\n",
    "    'model_type': 'prae',\n",
    "    'scaler_type': SCALER_TYPE,\n",
    "    'training_type': 'sequential',\n",
    "    'num_days': NUM_DAYS,\n",
    "    'seq_length': SEQ_LENGTH,\n",
    "    'input_dim': len(pipeline_prae.feature_names),\n",
    "    'hidden_dim': HIDDEN_DIM,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'feature_names': pipeline_prae.feature_names\n",
    "}\n",
    "with open(f\"{base_filename}_config.json\", 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved PRAE model to {base_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48958a62",
   "metadata": {},
   "source": [
    "### 3. Probabilistic Neural Network (PNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d74f287a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING: PNN\n",
      "Pipeline initialized on device: cuda\n",
      "Starting sequential training on 7 days...\n",
      "Train block: 10 min, Val block: 10 min\n",
      "================================================================================\n",
      "Day 1/7: 2015-01-02-TOTF.PA-book.csv.gz\n",
      "--------------------------------------------------------------------------------\n",
      "Loading data from ../data/TOTF.PA-book\\2015-01-02-TOTF.PA-book.csv.gz...\n",
      "Successfully loaded 640429 rows.\n",
      "Filtered 2801 pre-market samples.\n",
      "First hour samples: 74476, Rest of day: 563152\n",
      "Training on 3 time blocks...\n",
      "Block 1/3: Train=21255, Val=12380\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Initializing pnn model...\n",
      "Initializing Probabilistic Neural Network (PNN)...\n",
      "Training PNN (Max Epochs=1000)...\n",
      "Epoch 1/1000 - Train: 1.225301 | Val: 1.228680\n",
      "Validation loss decreased (inf --> 1.228680).  Saving model ...\n",
      "Epoch 2/1000 - Train: 0.717334 | Val: 1.100810\n",
      "Validation loss decreased (1.228680 --> 1.100810).  Saving model ...\n",
      "Epoch 3/1000 - Train: 0.338182 | Val: 0.947514\n",
      "Validation loss decreased (1.100810 --> 0.947514).  Saving model ...\n",
      "Epoch 4/1000 - Train: 0.067278 | Val: 1.143514\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 5/1000 - Train: 0.103231 | Val: 1.228635\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 6/1000 - Train: 0.420423 | Val: 1.112632\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 7/1000 - Train: 1.413047 | Val: 2.514790\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 8/1000 - Train: 1.949151 | Val: 1.985959\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Block 2/3: Train=13265, Val=7183\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Neural Network (PNN)...\n",
      "Training PNN (Max Epochs=500)...\n",
      "Epoch 1/500 - Train: 1.077006 | Val: 0.722236\n",
      "Validation loss decreased (inf --> 0.722236).  Saving model ...\n",
      "Epoch 2/500 - Train: 0.691225 | Val: 0.557890\n",
      "Validation loss decreased (0.722236 --> 0.557890).  Saving model ...\n",
      "Epoch 3/500 - Train: 0.530150 | Val: 0.502096\n",
      "Validation loss decreased (0.557890 --> 0.502096).  Saving model ...\n",
      "Epoch 4/500 - Train: 0.384561 | Val: 0.453654\n",
      "Validation loss decreased (0.502096 --> 0.453654).  Saving model ...\n",
      "Epoch 5/500 - Train: 0.412884 | Val: 0.552243\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 6/500 - Train: 0.689603 | Val: 1.014562\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 7/500 - Train: 0.689011 | Val: 0.436159\n",
      "Validation loss decreased (0.453654 --> 0.436159).  Saving model ...\n",
      "Epoch 8/500 - Train: 0.307412 | Val: 0.443601\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 9/500 - Train: 0.226855 | Val: 0.390540\n",
      "Validation loss decreased (0.436159 --> 0.390540).  Saving model ...\n",
      "Epoch 10/500 - Train: 0.113713 | Val: 0.377430\n",
      "Validation loss decreased (0.390540 --> 0.377430).  Saving model ...\n",
      "Epoch 11/500 - Train: 0.525122 | Val: 1.900905\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 12/500 - Train: 0.995545 | Val: 0.591859\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 13/500 - Train: 0.308781 | Val: 0.502711\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 14/500 - Train: 0.207212 | Val: 0.453071\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 15/500 - Train: 0.094736 | Val: 0.442148\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Block 3/3: Train=7918, Val=12475\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Neural Network (PNN)...\n",
      "Training PNN (Max Epochs=500)...\n",
      "Epoch 1/500 - Train: 1.857747 | Val: 1.467515\n",
      "Validation loss decreased (inf --> 1.467515).  Saving model ...\n",
      "Epoch 2/500 - Train: 1.039818 | Val: 1.113934\n",
      "Validation loss decreased (1.467515 --> 1.113934).  Saving model ...\n",
      "Epoch 3/500 - Train: 0.846794 | Val: 1.154013\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 4/500 - Train: 0.739516 | Val: 1.194891\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 5/500 - Train: 0.646992 | Val: 0.988457\n",
      "Validation loss decreased (1.113934 --> 0.988457).  Saving model ...\n",
      "Epoch 6/500 - Train: 0.571506 | Val: 1.002355\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 7/500 - Train: 0.553667 | Val: 1.220751\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 8/500 - Train: 0.369954 | Val: 1.949741\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 9/500 - Train: 0.354889 | Val: 1.709230\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 10/500 - Train: 0.234147 | Val: 1.963421\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "\n",
      "Evaluating on rest of Day 1...\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Evaluating model...\n",
      "Day 1 Results: AUROC=0.9589, AUPRC=0.9473, F4=0.8845\n",
      "Day 2/7: 2015-01-05-TOTF.PA-book.csv.gz\n",
      "--------------------------------------------------------------------------------\n",
      "Loading data from ../data/TOTF.PA-book\\2015-01-05-TOTF.PA-book.csv.gz...\n",
      "Successfully loaded 1202740 rows.\n",
      "Filtered 1920 pre-market samples.\n",
      "First hour samples: 113192, Rest of day: 1087628\n",
      "Training on 3 time blocks...\n",
      "Block 1/3: Train=24352, Val=30164\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Neural Network (PNN)...\n",
      "Training PNN (Max Epochs=500)...\n",
      "Epoch 1/500 - Train: 2.870834 | Val: 1.542136\n",
      "Validation loss decreased (inf --> 1.542136).  Saving model ...\n",
      "Epoch 2/500 - Train: 1.945714 | Val: 3.917303\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 3/500 - Train: 2.874283 | Val: 1.538908\n",
      "Validation loss decreased (1.542136 --> 1.538908).  Saving model ...\n",
      "Epoch 4/500 - Train: 1.293654 | Val: 1.312096\n",
      "Validation loss decreased (1.538908 --> 1.312096).  Saving model ...\n",
      "Epoch 5/500 - Train: 2.748407 | Val: 3.295069\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 6/500 - Train: 3.104897 | Val: 3.778582\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 7/500 - Train: 3.313561 | Val: 2.535257\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 8/500 - Train: 1.907331 | Val: 1.649524\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 9/500 - Train: 3.305340 | Val: 3.985219\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Block 2/3: Train=20778, Val=12683\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Neural Network (PNN)...\n",
      "Training PNN (Max Epochs=500)...\n",
      "Epoch 1/500 - Train: 22.869188 | Val: 23.025850\n",
      "Validation loss decreased (inf --> 23.025850).  Saving model ...\n",
      "Epoch 2/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 3/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 4/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 5/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 6/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 7/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 8/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 9/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 10/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 11/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 12/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 13/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 14/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 15/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 16/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 17/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 18/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 19/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 20/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 21/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 22/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 23/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 24/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 25/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 26/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 27/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 28/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 29/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 30/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 31/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 32/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 33/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 34/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 35/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 36/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 37/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 38/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 39/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 40/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 41/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 42/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 43/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 44/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 45/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 46/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 47/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 48/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 49/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 50/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 51/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 52/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 53/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 54/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 55/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 56/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 57/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 58/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 59/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 60/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 61/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 62/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 63/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 64/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 65/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 66/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 67/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 68/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 69/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 70/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 71/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 72/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 73/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 74/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 75/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 76/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 77/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 78/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 79/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 80/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 81/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 82/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 83/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 84/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 85/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 86/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 87/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 88/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 89/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 90/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 91/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 92/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 93/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 94/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 95/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 96/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 97/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 98/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 99/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 100/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 101/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 102/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 103/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 104/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 105/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 106/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 107/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 108/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 109/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 110/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 111/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 112/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 113/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 114/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 115/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 116/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 117/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 118/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 119/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 120/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 121/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 122/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 123/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 124/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 125/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 126/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 127/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 128/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 129/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 130/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 131/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 132/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 133/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 134/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 135/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 136/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 137/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 138/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 139/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 140/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 141/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 142/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 143/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 144/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 145/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 146/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 147/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 148/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 149/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 150/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 151/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 152/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 153/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 154/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 155/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 156/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 157/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 158/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 159/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 160/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 161/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 162/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 163/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 164/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 165/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 166/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 167/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 168/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 169/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 170/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 171/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 172/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 173/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 174/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 175/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 176/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 177/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 178/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 179/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 180/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 181/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 182/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 183/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 184/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 185/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 186/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 187/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 188/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 189/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 190/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 191/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 192/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 193/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 194/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 195/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 196/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 197/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 198/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 199/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 200/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 201/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 202/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 203/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 204/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 205/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 206/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 207/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 208/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 209/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 210/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 211/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 212/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 213/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 214/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 215/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 216/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 217/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 218/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 219/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 220/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 221/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 222/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 223/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 224/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 225/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 226/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 227/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 228/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 229/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 230/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 231/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 232/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 233/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 234/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 235/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 236/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 237/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 238/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 239/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 240/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 241/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 242/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 243/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 244/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 245/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 246/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 247/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 248/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 249/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 250/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 251/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 252/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 253/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 254/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 255/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 256/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 257/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 258/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 259/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 260/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 261/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 262/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 263/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 264/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 265/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 266/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 267/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 268/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 269/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 270/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 271/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 272/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 273/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 274/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 275/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 276/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 277/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 278/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 279/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 280/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 281/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 282/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 283/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 284/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 285/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 286/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 287/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 288/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 289/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 290/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 291/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 292/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 293/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 294/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 295/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 296/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 297/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 298/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 299/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 300/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 301/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 302/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 303/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 304/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 305/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 306/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 307/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 308/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 309/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 310/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 311/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 312/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 313/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 314/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 315/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 316/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 317/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 318/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 319/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 320/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 321/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 322/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 323/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 324/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 325/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 326/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 327/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 328/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 329/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 330/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 331/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 332/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 333/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 334/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 335/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 336/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 337/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 338/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 339/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 340/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 341/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 342/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 343/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 344/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 345/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 346/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 347/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 348/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 349/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 350/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 351/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 352/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 353/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 354/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 355/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 356/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 357/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 358/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 359/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 360/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 361/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 362/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 363/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 364/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 365/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 366/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 367/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 368/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 369/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 370/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 371/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 372/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 373/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 374/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 375/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 376/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 377/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 378/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 379/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 380/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 381/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 382/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 383/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 384/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 385/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 386/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 387/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 388/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 389/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 390/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 391/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 392/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 393/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 394/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 395/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 396/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 397/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 398/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 399/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 400/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 401/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 402/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 403/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 404/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 405/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 406/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 407/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 408/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 409/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 410/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 411/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 412/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 413/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 414/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 415/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 416/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 417/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 418/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 419/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 420/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 421/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 422/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 423/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 424/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 425/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 426/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 427/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 428/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 429/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 430/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 431/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 432/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 433/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 434/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 435/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 436/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 437/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 438/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 439/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 440/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 441/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 442/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 443/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 444/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 445/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 446/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 447/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 448/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 449/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 450/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 451/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 452/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 453/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 454/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 455/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 456/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 457/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 458/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 459/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 460/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 461/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 462/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 463/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 464/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 465/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 466/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 467/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 468/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 469/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 470/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 471/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 472/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 473/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 474/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 475/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 476/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 477/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 478/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 479/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 480/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 481/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 482/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 483/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 484/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 485/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 486/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 487/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 488/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 489/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 490/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 491/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 492/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 493/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 494/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 495/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 496/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 497/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 498/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 499/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 500/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Block 3/3: Train=13412, Val=11803\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Neural Network (PNN)...\n",
      "Training PNN (Max Epochs=500)...\n",
      "Epoch 1/500 - Train: 1.954404 | Val: 1.332813\n",
      "Validation loss decreased (inf --> 1.332813).  Saving model ...\n",
      "Epoch 2/500 - Train: 1.943831 | Val: 1.779871\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 3/500 - Train: 2.240270 | Val: 3.057358\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 4/500 - Train: 2.507512 | Val: 3.096104\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 5/500 - Train: 2.140355 | Val: 0.942255\n",
      "Validation loss decreased (1.332813 --> 0.942255).  Saving model ...\n",
      "Epoch 6/500 - Train: 1.623615 | Val: 2.577245\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 7/500 - Train: 1.685177 | Val: 0.960107\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 8/500 - Train: 2.273524 | Val: 2.940197\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 9/500 - Train: 1.565842 | Val: 0.793439\n",
      "Validation loss decreased (0.942255 --> 0.793439).  Saving model ...\n",
      "Epoch 10/500 - Train: 1.098748 | Val: 1.185511\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 11/500 - Train: 1.187737 | Val: 2.460363\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 12/500 - Train: 2.289559 | Val: 2.259556\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 13/500 - Train: 1.151025 | Val: 0.752841\n",
      "Validation loss decreased (0.793439 --> 0.752841).  Saving model ...\n",
      "Epoch 14/500 - Train: 1.651313 | Val: 1.204504\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 15/500 - Train: 0.863628 | Val: 1.418110\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 16/500 - Train: 1.098657 | Val: 0.866736\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 17/500 - Train: 0.712136 | Val: 0.652229\n",
      "Validation loss decreased (0.752841 --> 0.652229).  Saving model ...\n",
      "Epoch 18/500 - Train: 1.899886 | Val: 2.595228\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 19/500 - Train: 1.653196 | Val: 0.774677\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 20/500 - Train: 0.698198 | Val: 0.855376\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 21/500 - Train: 0.723159 | Val: 0.973246\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 22/500 - Train: 0.646895 | Val: 0.846770\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "\n",
      "Evaluating on rest of Day 2...\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Evaluating model...\n",
      "Day 2 Results: AUROC=0.9215, AUPRC=0.8944, F4=0.0000\n",
      "Day 3/7: 2015-01-06-TOTF.PA-book.csv.gz\n",
      "--------------------------------------------------------------------------------\n",
      "Loading data from ../data/TOTF.PA-book\\2015-01-06-TOTF.PA-book.csv.gz...\n",
      "Successfully loaded 1272556 rows.\n",
      "Filtered 1896 pre-market samples.\n",
      "First hour samples: 170233, Rest of day: 1100427\n",
      "Training on 3 time blocks...\n",
      "Block 1/3: Train=30901, Val=36877\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Neural Network (PNN)...\n",
      "Training PNN (Max Epochs=500)...\n",
      "Epoch 1/500 - Train: 23.024660 | Val: 23.025850\n",
      "Validation loss decreased (inf --> 23.025850).  Saving model ...\n",
      "Epoch 2/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 3/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 4/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 5/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 6/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 7/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 8/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 9/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 10/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 11/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 12/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 13/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 14/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 15/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 16/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 17/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 18/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 19/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 20/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 21/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 22/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 23/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 24/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 25/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 26/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 27/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 28/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 29/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 30/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 31/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 32/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 33/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 34/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 35/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 36/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 37/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 38/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 39/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 40/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 41/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 42/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 43/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 44/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 45/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 46/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 47/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 48/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 49/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 50/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 51/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 52/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 53/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 54/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 55/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 56/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 57/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 58/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 59/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 60/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 61/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 62/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 63/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 64/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 65/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 66/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 67/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 68/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 69/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 70/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 71/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 72/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 73/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 74/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 75/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 76/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 77/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 78/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 79/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 80/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 81/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 82/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 83/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 84/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 85/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 86/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 87/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 88/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 89/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 90/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 91/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 92/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 93/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 94/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 95/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 96/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 97/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 98/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 99/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 100/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 101/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 102/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 103/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 104/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 105/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 106/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 107/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 108/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 109/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 110/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 111/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 112/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 113/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 114/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 115/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 116/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 117/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 118/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 119/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 120/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 121/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 122/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 123/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 124/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 125/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 126/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 127/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 128/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 129/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 130/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 131/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 132/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 133/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 134/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 135/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 136/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 137/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 138/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 139/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 140/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 141/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 142/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 143/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 144/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 145/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 146/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 147/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 148/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 149/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 150/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 151/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 152/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 153/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 154/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 155/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 156/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 157/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 158/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 159/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 160/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 161/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 162/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 163/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 164/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 165/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 166/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 167/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 168/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 169/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 170/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 171/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 172/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 173/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 174/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 175/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 176/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 177/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 178/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 179/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 180/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 181/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 182/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 183/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 184/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 185/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 186/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 187/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 188/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 189/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 190/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 191/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 192/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 193/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 194/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 195/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 196/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 197/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 198/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 199/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 200/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 201/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 202/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 203/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 204/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 205/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 206/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 207/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 208/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 209/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 210/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 211/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 212/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 213/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 214/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 215/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 216/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 217/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 218/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 219/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 220/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 221/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 222/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 223/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 224/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 225/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 226/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 227/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 228/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 229/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 230/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 231/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 232/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 233/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 234/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 235/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 236/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 237/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 238/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 239/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 240/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 241/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 242/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 243/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 244/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 245/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 246/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 247/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 248/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 249/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 250/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 251/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 252/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 253/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 254/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 255/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 256/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 257/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 258/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 259/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 260/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 261/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 262/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 263/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 264/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 265/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 266/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 267/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 268/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 269/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 270/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 271/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 272/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 273/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 274/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 275/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 276/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 277/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 278/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 279/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 280/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 281/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 282/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 283/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 284/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 285/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 286/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 287/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 288/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 289/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 290/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 291/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 292/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 293/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 294/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 295/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 296/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 297/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 298/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 299/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 300/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 301/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 302/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 303/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 304/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 305/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 306/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 307/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 308/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 309/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 310/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 311/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 312/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 313/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 314/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 315/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 316/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 317/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 318/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 319/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 320/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 321/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 322/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 323/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 324/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 325/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 326/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 327/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 328/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 329/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 330/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 331/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 332/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 333/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 334/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 335/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 336/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 337/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 338/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 339/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 340/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 341/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 342/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 343/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 344/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 345/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 346/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 347/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 348/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 349/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 350/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 351/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 352/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 353/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 354/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 355/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 356/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 357/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 358/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 359/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 360/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 361/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 362/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 363/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 364/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 365/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 366/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 367/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 368/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 369/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 370/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 371/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 372/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 373/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 374/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 375/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 376/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 377/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 378/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 379/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 380/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 381/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 382/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 383/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 384/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 385/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 386/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 387/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 388/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 389/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 390/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 391/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 392/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 393/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 394/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 395/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 396/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 397/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 398/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 399/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 400/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 401/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 402/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 403/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 404/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 405/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 406/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 407/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 408/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 409/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 410/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 411/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 412/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 413/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 414/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 415/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 416/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 417/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 418/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 419/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 420/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 421/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 422/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 423/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 424/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 425/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 426/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 427/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 428/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 429/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 430/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 431/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 432/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 433/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 434/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 435/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 436/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 437/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 438/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 439/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 440/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 441/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 442/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 443/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 444/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 445/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 446/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 447/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 448/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 449/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 450/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 451/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 452/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 453/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 454/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 455/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 456/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 457/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 458/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 459/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 460/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 461/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 462/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 463/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 464/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 465/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 466/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 467/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 468/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 469/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 470/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 471/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 472/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 473/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 474/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 475/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 476/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 477/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 478/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 479/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 480/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 481/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 482/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 483/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 484/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 485/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 486/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 487/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 488/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 489/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 490/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 491/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 492/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 493/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 494/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 495/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 496/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 497/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 498/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 499/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 500/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Block 2/3: Train=23621, Val=28400\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Neural Network (PNN)...\n",
      "Training PNN (Max Epochs=500)...\n",
      "Epoch 1/500 - Train: 23.024755 | Val: 23.025850\n",
      "Validation loss decreased (inf --> 23.025850).  Saving model ...\n",
      "Epoch 2/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 3/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 4/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 5/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 6/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 7/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 8/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 9/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 10/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 11/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 12/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 13/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 14/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 15/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 16/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 17/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 18/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 19/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 20/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 21/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 22/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 23/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 24/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 25/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 26/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 27/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 28/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 29/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 30/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 31/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 32/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 33/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 34/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 35/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 36/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 37/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 38/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 39/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 40/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 41/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 42/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 43/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 44/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 45/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 46/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 47/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 48/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 49/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 50/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 51/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 52/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 53/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 54/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 55/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 56/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 57/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 58/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 59/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 60/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 61/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 62/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 63/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 64/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 65/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 66/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 67/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 68/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 69/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 70/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 71/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 72/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 73/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 74/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 75/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 76/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 77/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 78/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 79/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 80/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 81/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 82/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 83/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 84/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 85/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 86/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 87/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 88/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 89/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 90/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 91/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 92/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 93/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 94/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 95/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 96/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 97/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 98/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 99/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 100/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 101/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 102/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 103/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 104/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 105/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 106/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 107/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 108/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 109/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 110/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 111/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 112/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 113/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 114/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 115/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 116/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 117/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 118/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 119/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 120/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 121/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 122/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 123/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 124/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 125/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 126/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 127/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 128/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 129/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 130/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 131/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 132/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 133/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 134/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 135/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 136/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 137/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 138/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 139/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 140/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 141/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 142/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 143/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 144/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 145/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 146/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 147/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 148/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 149/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 150/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 151/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 152/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 153/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 154/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 155/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 156/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 157/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 158/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 159/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 160/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 161/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 162/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 163/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 164/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 165/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 166/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 167/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 168/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 169/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 170/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 171/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 172/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 173/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 174/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 175/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 176/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 177/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 178/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 179/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 180/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 181/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 182/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 183/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 184/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 185/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 186/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 187/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 188/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 189/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 190/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 191/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 192/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 193/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 194/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 195/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 196/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 197/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 198/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 199/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 200/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 201/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 202/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 203/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 204/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 205/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 206/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 207/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 208/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 209/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 210/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 211/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 212/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 213/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 214/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 215/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 216/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 217/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 218/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 219/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 220/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 221/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 222/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 223/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 224/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 225/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 226/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 227/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 228/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 229/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 230/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 231/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 232/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 233/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 234/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 235/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 236/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 237/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 238/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 239/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 240/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 241/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 242/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 243/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 244/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 245/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 246/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 247/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 248/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 249/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 250/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 251/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 252/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 253/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 254/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 255/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 256/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 257/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 258/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 259/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 260/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 261/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 262/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 263/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 264/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 265/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 266/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 267/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 268/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 269/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 270/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 271/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 272/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 273/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 274/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 275/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 276/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 277/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 278/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 279/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 280/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 281/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 282/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 283/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 284/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 285/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 286/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 287/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 288/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 289/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 290/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 291/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 292/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 293/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 294/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 295/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 296/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 297/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 298/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 299/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 300/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 301/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 302/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 303/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 304/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 305/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 306/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 307/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 308/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 309/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 310/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 311/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 312/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 313/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 314/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 315/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 316/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 317/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 318/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 319/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 320/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 321/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 322/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 323/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 324/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 325/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 326/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 327/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 328/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 329/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 330/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 331/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 332/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 333/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 334/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 335/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 336/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 337/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 338/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 339/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 340/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 341/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 342/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 343/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 344/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 345/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 346/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 347/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 348/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 349/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 350/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 351/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 352/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 353/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 354/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 355/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 356/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 357/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 358/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 359/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 360/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 361/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 362/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 363/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 364/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 365/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 366/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 367/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 368/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 369/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 370/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 371/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 372/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 373/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 374/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 375/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 376/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 377/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 378/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 379/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 380/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 381/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 382/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 383/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 384/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 385/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 386/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 387/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 388/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 389/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 390/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 391/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 392/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 393/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 394/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 395/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 396/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 397/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 398/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 399/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 400/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 401/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 402/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 403/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 404/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 405/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 406/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 407/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 408/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 409/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 410/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 411/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 412/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 413/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 414/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 415/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 416/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 417/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 418/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 419/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 420/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 421/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 422/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 423/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 424/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 425/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 426/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 427/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 428/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 429/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 430/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 431/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 432/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 433/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 434/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 435/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 436/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 437/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 438/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 439/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 440/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 441/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 442/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 443/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 444/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 445/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 446/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 447/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 448/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 449/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 450/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 451/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 452/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 453/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 454/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 455/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 456/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 457/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 458/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 459/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 460/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 461/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 462/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 463/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 464/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 465/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 466/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 467/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 468/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 469/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 470/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 471/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 472/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 473/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 474/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 475/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 476/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 477/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 478/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 479/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 480/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 481/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 482/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 483/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 484/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 485/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 486/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 487/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 488/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 489/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 490/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 491/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 492/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 493/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 494/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 495/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 496/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 497/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 498/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 499/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 500/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Block 3/3: Train=30334, Val=20100\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Neural Network (PNN)...\n",
      "Training PNN (Max Epochs=500)...\n",
      "Epoch 1/500 - Train: 23.025091 | Val: 23.025850\n",
      "Validation loss decreased (inf --> 23.025850).  Saving model ...\n",
      "Epoch 2/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 3/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 4/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 5/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 6/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 7/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 8/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 9/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 10/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 11/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 12/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 13/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 14/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 15/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 16/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 17/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 18/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 19/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 20/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 21/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 22/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 23/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 24/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 25/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 26/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 27/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 28/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 29/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 30/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 31/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 32/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 33/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 34/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 35/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 36/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 37/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 38/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 39/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 40/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 41/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 42/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 43/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 44/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 45/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 46/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 47/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 48/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 49/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 50/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 51/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 52/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 53/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 54/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 55/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 56/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 57/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 58/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 59/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 60/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 61/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 62/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 63/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 64/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 65/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 66/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 67/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 68/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 69/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 70/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 71/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 72/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 73/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 74/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 75/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 76/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 77/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 78/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 79/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 80/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 81/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 82/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 83/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 84/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 85/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 86/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 87/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 88/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 89/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 90/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 91/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 92/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 93/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 94/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 95/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 96/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 97/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 98/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 99/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 100/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 101/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 102/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 103/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 104/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 105/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 106/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 107/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 108/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 109/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 110/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 111/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 112/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 113/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 114/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 115/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 116/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 117/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 118/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 119/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 120/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 121/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 122/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 123/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 124/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 125/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 126/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 127/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 128/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 129/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 130/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 131/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 132/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 133/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 134/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 135/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 136/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 137/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 138/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 139/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 140/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 141/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 142/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 143/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 144/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 145/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 146/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 147/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 148/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 149/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 150/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 151/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 152/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 153/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 154/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 155/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 156/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 157/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 158/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 159/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 160/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 161/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 162/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 163/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 164/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 165/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 166/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 167/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 168/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 169/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 170/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 171/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 172/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 173/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 174/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 175/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 176/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 177/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 178/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 179/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 180/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 181/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 182/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 183/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 184/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 185/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 186/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 187/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 188/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 189/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 190/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 191/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 192/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 193/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 194/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 195/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 196/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 197/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 198/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 199/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 200/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 201/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 202/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 203/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 204/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 205/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 206/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 207/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 208/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 209/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 210/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 211/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 212/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 213/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 214/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 215/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 216/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 217/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 218/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 219/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 220/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 221/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 222/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 223/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 224/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 225/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 226/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 227/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 228/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 229/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 230/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 231/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 232/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 233/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 234/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 235/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 236/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 237/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 238/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 239/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 240/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 241/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 242/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 243/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 244/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 245/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 246/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 247/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 248/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 249/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 250/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 251/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 252/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 253/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 254/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 255/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 256/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 257/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 258/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 259/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 260/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 261/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 262/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 263/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 264/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 265/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 266/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 267/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 268/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 269/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 270/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 271/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 272/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 273/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 274/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 275/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 276/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 277/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 278/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 279/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 280/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 281/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 282/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 283/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 284/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 285/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 286/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 287/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 288/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 289/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 290/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 291/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 292/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 293/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 294/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 295/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 296/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 297/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 298/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 299/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 300/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 301/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 302/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 303/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 304/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 305/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 306/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 307/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 308/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 309/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 310/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 311/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 312/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 313/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 314/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 315/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 316/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 317/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 318/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 319/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 320/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 321/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 322/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 323/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 324/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 325/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 326/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 327/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 328/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 329/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 330/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 331/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 332/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 333/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 334/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 335/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 336/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 337/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 338/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 339/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 340/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 341/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 342/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 343/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 344/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 345/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 346/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 347/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 348/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 349/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 350/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 351/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 352/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 353/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 354/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 355/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 356/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 357/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 358/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 359/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 360/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 361/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 362/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 363/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 364/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 365/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 366/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 367/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 368/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 369/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 370/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 371/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 372/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 373/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 374/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 375/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 376/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 377/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 378/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 379/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 380/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 381/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 382/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 383/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 384/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 385/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 386/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 387/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 388/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 389/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 390/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 391/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 392/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 393/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 394/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 395/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 396/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 397/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 398/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 399/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 400/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 401/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 402/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 403/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 404/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 405/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 406/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 407/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 408/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 409/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 410/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 411/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 412/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 413/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 414/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 415/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 416/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 417/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 418/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 419/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 420/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 421/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 422/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 423/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 424/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 425/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 426/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 427/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 428/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 429/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 430/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 431/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 432/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 433/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 434/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 435/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 436/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 437/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 438/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 439/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 440/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 441/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 442/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 443/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 444/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 445/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 446/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 447/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 448/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 449/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 450/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 451/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 452/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 453/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 454/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 455/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 456/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 457/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 458/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 459/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 460/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 461/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 462/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 463/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 464/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 465/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 466/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 467/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 468/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 469/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 470/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 471/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 472/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 473/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 474/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 475/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 476/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 477/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 478/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 479/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 480/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 481/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 482/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 483/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 484/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 485/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 486/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 487/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 488/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 489/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 490/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 491/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 492/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 493/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 494/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 495/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 496/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 497/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 498/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 499/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 500/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "\n",
      "Evaluating on rest of Day 3...\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Evaluating model...\n",
      "Day 3 Results: AUROC=0.5000, AUPRC=0.5000, F4=0.0000\n",
      "Day 4/7: 2015-01-07-TOTF.PA-book.csv.gz\n",
      "--------------------------------------------------------------------------------\n",
      "Loading data from ../data/TOTF.PA-book\\2015-01-07-TOTF.PA-book.csv.gz...\n",
      "Successfully loaded 1124864 rows.\n",
      "Filtered 2053 pre-market samples.\n",
      "First hour samples: 128092, Rest of day: 994719\n",
      "Training on 3 time blocks...\n",
      "Block 1/3: Train=19569, Val=27024\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Neural Network (PNN)...\n",
      "Training PNN (Max Epochs=500)...\n",
      "Epoch 1/500 - Train: 8.414357 | Val: 23.025850\n",
      "Validation loss decreased (inf --> 23.025850).  Saving model ...\n",
      "Epoch 2/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 3/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 4/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 5/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 6/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 7/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 8/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 9/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 10/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 11/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 12/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 13/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 14/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 15/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 16/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 17/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 18/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 19/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 20/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 21/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 22/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 23/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 24/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 25/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 26/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 27/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 28/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 29/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 30/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 31/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 32/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 33/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 34/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 35/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 36/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 37/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 38/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 39/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 40/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 41/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 42/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 43/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 44/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 45/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 46/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 47/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 48/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 49/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 50/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 51/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 52/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 53/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 54/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 55/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 56/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 57/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 58/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 59/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 60/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 61/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 62/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 63/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 64/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 65/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 66/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 67/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 68/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 69/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 70/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 71/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 72/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 73/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 74/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 75/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 76/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 77/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 78/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 79/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 80/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 81/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 82/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 83/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 84/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 85/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 86/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 87/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 88/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 89/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 90/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 91/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 92/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 93/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 94/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 95/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 96/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 97/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 98/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 99/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 100/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 101/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 102/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 103/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 104/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 105/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 106/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 107/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 108/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 109/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 110/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 111/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 112/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 113/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 114/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 115/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 116/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 117/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 118/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 119/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 120/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 121/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 122/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 123/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 124/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 125/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 126/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 127/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 128/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 129/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 130/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 131/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 132/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 133/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 134/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 135/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 136/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 137/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 138/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 139/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 140/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 141/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 142/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 143/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 144/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 145/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 146/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 147/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 148/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 149/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 150/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 151/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 152/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 153/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 154/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 155/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 156/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 157/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 158/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 159/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 160/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 161/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 162/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 163/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 164/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 165/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 166/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 167/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 168/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 169/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 170/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 171/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 172/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 173/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 174/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 175/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 176/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 177/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 178/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 179/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 180/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 181/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 182/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 183/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 184/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 185/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 186/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 187/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 188/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 189/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 190/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 191/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 192/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 193/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 194/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 195/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 196/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 197/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 198/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 199/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 200/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 201/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 202/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 203/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 204/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 205/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 206/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 207/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 208/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 209/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 210/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 211/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 212/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 213/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 214/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 215/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 216/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 217/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 218/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 219/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 220/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 221/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 222/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 223/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 224/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 225/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 226/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 227/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 228/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 229/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 230/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 231/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 232/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 233/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 234/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 235/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 236/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 237/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 238/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 239/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 240/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 241/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 242/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 243/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 244/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 245/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 246/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 247/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 248/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 249/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 250/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 251/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 252/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 253/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 254/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 255/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 256/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 257/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 258/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 259/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 260/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 261/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 262/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 263/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 264/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 265/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 266/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 267/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 268/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 269/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 270/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 271/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 272/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 273/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 274/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 275/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 276/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 277/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 278/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 279/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 280/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 281/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 282/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 283/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 284/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 285/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 286/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 287/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 288/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 289/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 290/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 291/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 292/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 293/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 294/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 295/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 296/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 297/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 298/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 299/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 300/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 301/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 302/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 303/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 304/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 305/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 306/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 307/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 308/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 309/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 310/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 311/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 312/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 313/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 314/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 315/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 316/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 317/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 318/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 319/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 320/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 321/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 322/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 323/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 324/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 325/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 326/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 327/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 328/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 329/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 330/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 331/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 332/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 333/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 334/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 335/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 336/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 337/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 338/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 339/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 340/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 341/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 342/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 343/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 344/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 345/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 346/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 347/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 348/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 349/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 350/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 351/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 352/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 353/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 354/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 355/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 356/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 357/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 358/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 359/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 360/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 361/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 362/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 363/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 364/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 365/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 366/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 367/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 368/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 369/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 370/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 371/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 372/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 373/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 374/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 375/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 376/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 377/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 378/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 379/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 380/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 381/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 382/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 383/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 384/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 385/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 386/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 387/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 388/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 389/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 390/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 391/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 392/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 393/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 394/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 395/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 396/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 397/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 398/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 399/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 400/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 401/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 402/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 403/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 404/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 405/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 406/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 407/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 408/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 409/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 410/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 411/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 412/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 413/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 414/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 415/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 416/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 417/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 418/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 419/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 420/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 421/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 422/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 423/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 424/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 425/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 426/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 427/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 428/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 429/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 430/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 431/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 432/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 433/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 434/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 435/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 436/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 437/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 438/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 439/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 440/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 441/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 442/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 443/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 444/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 445/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 446/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 447/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 448/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 449/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 450/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 451/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 452/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 453/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 454/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 455/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 456/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 457/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 458/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 459/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 460/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 461/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 462/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 463/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 464/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 465/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 466/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 467/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 468/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 469/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 470/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 471/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 472/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 473/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 474/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 475/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 476/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 477/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 478/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 479/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 480/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 481/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 482/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 483/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 484/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 485/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 486/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 487/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 488/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 489/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 490/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 491/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 492/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 493/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 494/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 495/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 496/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 497/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 498/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 499/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 500/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Block 2/3: Train=27589, Val=28408\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Neural Network (PNN)...\n",
      "Training PNN (Max Epochs=500)...\n",
      "Epoch 1/500 - Train: 22.918898 | Val: 23.025850\n",
      "Validation loss decreased (inf --> 23.025850).  Saving model ...\n",
      "Epoch 2/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 3/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 4/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 5/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 6/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 7/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 8/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 9/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 10/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 11/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 12/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 13/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 14/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 15/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 16/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 17/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 18/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 19/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 20/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 21/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 22/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 23/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 24/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 25/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 26/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 27/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 28/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 29/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 30/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 31/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 32/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 33/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 34/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 35/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 36/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 37/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 38/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 39/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 40/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 41/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 42/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 43/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 44/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 45/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 46/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 47/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 48/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 49/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 50/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 51/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 52/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 53/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 54/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 55/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 56/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 57/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 58/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 59/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 60/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 61/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 62/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 63/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 64/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 65/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 66/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 67/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 68/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 69/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 70/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 71/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 72/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 73/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 74/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 75/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 76/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 77/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 78/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 79/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 80/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 81/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 82/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 83/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 84/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 85/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 86/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 87/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 88/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 89/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 90/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 91/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 92/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 93/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 94/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 95/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 96/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 97/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 98/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 99/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 100/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 101/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 102/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 103/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 104/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 105/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 106/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 107/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 108/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 109/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 110/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 111/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 112/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 113/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 114/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 115/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 116/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 117/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 118/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 119/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 120/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 121/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 122/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 123/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 124/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 125/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 126/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 127/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 128/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 129/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 130/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 131/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 132/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 133/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 134/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 135/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 136/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 137/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 138/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 139/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 140/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 141/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 142/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 143/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 144/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 145/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 146/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 147/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 148/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 149/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 150/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 151/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 152/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 153/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 154/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 155/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 156/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 157/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 158/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 159/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 160/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 161/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 162/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 163/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 164/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 165/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 166/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 167/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 168/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 169/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 170/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 171/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 172/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 173/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 174/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 175/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 176/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 177/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 178/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 179/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 180/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 181/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 182/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 183/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 184/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 185/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 186/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 187/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 188/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 189/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 190/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 191/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 192/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 193/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 194/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 195/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 196/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 197/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 198/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 199/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 200/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 201/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 202/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 203/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 204/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 205/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 206/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 207/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 208/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 209/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 210/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 211/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 212/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 213/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 214/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 215/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 216/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 217/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 218/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 219/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 220/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 221/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 222/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 223/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 224/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 225/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 226/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 227/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 228/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 229/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 230/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 231/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 232/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 233/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 234/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 235/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 236/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 237/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 238/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 239/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 240/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 241/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 242/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 243/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 244/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 245/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 246/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 247/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 248/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 249/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 250/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 251/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 252/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 253/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 254/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 255/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 256/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 257/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 258/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 259/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 260/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 261/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 262/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 263/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 264/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 265/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 266/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 267/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 268/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 269/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 270/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 271/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 272/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 273/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 274/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 275/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 276/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 277/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 278/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 279/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 280/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 281/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 282/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 283/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 284/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 285/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 286/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 287/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 288/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 289/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 290/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 291/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 292/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 293/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 294/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 295/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 296/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 297/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 298/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 299/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 300/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 301/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 302/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 303/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 304/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 305/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 306/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 307/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 308/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 309/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 310/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 311/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 312/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 313/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 314/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 315/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 316/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 317/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 318/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 319/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 320/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 321/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 322/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 323/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 324/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 325/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 326/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 327/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 328/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 329/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 330/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 331/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 332/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 333/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 334/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 335/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 336/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 337/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 338/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 339/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 340/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 341/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 342/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 343/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 344/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 345/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 346/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 347/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 348/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 349/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 350/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 351/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 352/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 353/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 354/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 355/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 356/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 357/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 358/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 359/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 360/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 361/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 362/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 363/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 364/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 365/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 366/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 367/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 368/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 369/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 370/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 371/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 372/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 373/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 374/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 375/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 376/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 377/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 378/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 379/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 380/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 381/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 382/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 383/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 384/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 385/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 386/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 387/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 388/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 389/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 390/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 391/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 392/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 393/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 394/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 395/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 396/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 397/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 398/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 399/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 400/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 401/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 402/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 403/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 404/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 405/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 406/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 407/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 408/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 409/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 410/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 411/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 412/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 413/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 414/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 415/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 416/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 417/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 418/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 419/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 420/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 421/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 422/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 423/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 424/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 425/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 426/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 427/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 428/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 429/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 430/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 431/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 432/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 433/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 434/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 435/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 436/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 437/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 438/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 439/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 440/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 441/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 442/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 443/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 444/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 445/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 446/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 447/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 448/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 449/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 450/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 451/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 452/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 453/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 454/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 455/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 456/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 457/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 458/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 459/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 460/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 461/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 462/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 463/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 464/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 465/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 466/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 467/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 468/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 469/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 470/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 471/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 472/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 473/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 474/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 475/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 476/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 477/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 478/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 479/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 480/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 481/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 482/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 483/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 484/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 485/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 486/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 487/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 488/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 489/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 490/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 491/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 492/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 493/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 494/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 495/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 496/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 497/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 498/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 499/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 500/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Block 3/3: Train=14055, Val=11447\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Neural Network (PNN)...\n",
      "Training PNN (Max Epochs=500)...\n",
      "Epoch 1/500 - Train: 4.263388 | Val: 4.835800\n",
      "Validation loss decreased (inf --> 4.835800).  Saving model ...\n",
      "Epoch 2/500 - Train: 4.759611 | Val: 4.678414\n",
      "Validation loss decreased (4.835800 --> 4.678414).  Saving model ...\n",
      "Epoch 3/500 - Train: 4.547029 | Val: 4.408082\n",
      "Validation loss decreased (4.678414 --> 4.408082).  Saving model ...\n",
      "Epoch 4/500 - Train: 4.139469 | Val: 3.732125\n",
      "Validation loss decreased (4.408082 --> 3.732125).  Saving model ...\n",
      "Epoch 5/500 - Train: 4.321787 | Val: 5.070240\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 6/500 - Train: 4.983183 | Val: 4.902101\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 7/500 - Train: 4.766449 | Val: 4.626993\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 8/500 - Train: 4.377272 | Val: 4.041558\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 9/500 - Train: 3.285073 | Val: 3.258862\n",
      "Validation loss decreased (3.732125 --> 3.258862).  Saving model ...\n",
      "Epoch 10/500 - Train: 5.645285 | Val: 5.481300\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 11/500 - Train: 5.428636 | Val: 5.398197\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 12/500 - Train: 5.336954 | Val: 5.296229\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 13/500 - Train: 5.222746 | Val: 5.167131\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 14/500 - Train: 5.074684 | Val: 4.994574\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "\n",
      "Evaluating on rest of Day 4...\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Evaluating model...\n",
      "Day 4 Results: AUROC=0.8843, AUPRC=0.7688, F4=0.1124\n",
      "Day 5/7: 2015-01-08-TOTF.PA-book.csv.gz\n",
      "--------------------------------------------------------------------------------\n",
      "Loading data from ../data/TOTF.PA-book\\2015-01-08-TOTF.PA-book.csv.gz...\n",
      "Successfully loaded 1101381 rows.\n",
      "Filtered 1868 pre-market samples.\n",
      "First hour samples: 125163, Rest of day: 974350\n",
      "Training on 3 time blocks...\n",
      "Block 1/3: Train=17618, Val=32995\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Neural Network (PNN)...\n",
      "Training PNN (Max Epochs=500)...\n",
      "Epoch 1/500 - Train: 4.159868 | Val: 4.496092\n",
      "Validation loss decreased (inf --> 4.496092).  Saving model ...\n",
      "Epoch 2/500 - Train: 4.387020 | Val: 4.413376\n",
      "Validation loss decreased (4.496092 --> 4.413376).  Saving model ...\n",
      "Epoch 3/500 - Train: 4.291407 | Val: 4.302420\n",
      "Validation loss decreased (4.413376 --> 4.302420).  Saving model ...\n",
      "Epoch 4/500 - Train: 4.163838 | Val: 4.155036\n",
      "Validation loss decreased (4.302420 --> 4.155036).  Saving model ...\n",
      "Epoch 5/500 - Train: 3.969075 | Val: 3.875071\n",
      "Validation loss decreased (4.155036 --> 3.875071).  Saving model ...\n",
      "Epoch 6/500 - Train: 3.134729 | Val: 1.890090\n",
      "Validation loss decreased (3.875071 --> 1.890090).  Saving model ...\n",
      "Epoch 7/500 - Train: 2.114206 | Val: 2.299508\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 8/500 - Train: 1.897693 | Val: 5.121547\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 9/500 - Train: 5.099153 | Val: 5.177863\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 10/500 - Train: 5.070959 | Val: 5.141100\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 11/500 - Train: 5.031407 | Val: 5.099990\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Block 2/3: Train=32355, Val=16660\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Neural Network (PNN)...\n",
      "Training PNN (Max Epochs=500)...\n",
      "Epoch 1/500 - Train: 2.110818 | Val: 1.179074\n",
      "Validation loss decreased (inf --> 1.179074).  Saving model ...\n",
      "Epoch 2/500 - Train: 1.235663 | Val: 1.054535\n",
      "Validation loss decreased (1.179074 --> 1.054535).  Saving model ...\n",
      "Epoch 3/500 - Train: 1.113544 | Val: 0.780102\n",
      "Validation loss decreased (1.054535 --> 0.780102).  Saving model ...\n",
      "Epoch 4/500 - Train: 1.087635 | Val: 1.177370\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 5/500 - Train: 1.468507 | Val: 1.181102\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 6/500 - Train: 1.459624 | Val: 1.185088\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 7/500 - Train: 1.460959 | Val: 1.188874\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 8/500 - Train: 1.460354 | Val: 1.192109\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Block 3/3: Train=11364, Val=14171\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Neural Network (PNN)...\n",
      "Training PNN (Max Epochs=500)...\n",
      "Epoch 1/500 - Train: 22.830988 | Val: 23.025850\n",
      "Validation loss decreased (inf --> 23.025850).  Saving model ...\n",
      "Epoch 2/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 3/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 4/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 5/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 6/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 7/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 8/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 9/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 10/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 11/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 12/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 13/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 14/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 15/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 16/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 17/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 18/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 19/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 20/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 21/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 22/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 23/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 24/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 25/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 26/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 27/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 28/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 29/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 30/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 31/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 32/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 33/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 34/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 35/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 36/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 37/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 38/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 39/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 40/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 41/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 42/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 43/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 44/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 45/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 46/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 47/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 48/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 49/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 50/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 51/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 52/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 53/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 54/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 55/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 56/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 57/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 58/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 59/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 60/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 61/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 62/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 63/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 64/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 65/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 66/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 67/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 68/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 69/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 70/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 71/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 72/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 73/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 74/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 75/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 76/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 77/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 78/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 79/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 80/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 81/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 82/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 83/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 84/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 85/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 86/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 87/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 88/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 89/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 90/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 91/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 92/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 93/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 94/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 95/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 96/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 97/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 98/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 99/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 100/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 101/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 102/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 103/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 104/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 105/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 106/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 107/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 108/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 109/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 110/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 111/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 112/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 113/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 114/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 115/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 116/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 117/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 118/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 119/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 120/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 121/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 122/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 123/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 124/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 125/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 126/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 127/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 128/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 129/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 130/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 131/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 132/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 133/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 134/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 135/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 136/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 137/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 138/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 139/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 140/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 141/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 142/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 143/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 144/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 145/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 146/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 147/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 148/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 149/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 150/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 151/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 152/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 153/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 154/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 155/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 156/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 157/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 158/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 159/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 160/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 161/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 162/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 163/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 164/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 165/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 166/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 167/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 168/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 169/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 170/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 171/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 172/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 173/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 174/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 175/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 176/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 177/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 178/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 179/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 180/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 181/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 182/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 183/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 184/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 185/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 186/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 187/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 188/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 189/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 190/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 191/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 192/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 193/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 194/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 195/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 196/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 197/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 198/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 199/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 200/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 201/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 202/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 203/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 204/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 205/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 206/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 207/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 208/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 209/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 210/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 211/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 212/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 213/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 214/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 215/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 216/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 217/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 218/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 219/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 220/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 221/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 222/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 223/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 224/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 225/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 226/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 227/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 228/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 229/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 230/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 231/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 232/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 233/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 234/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 235/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 236/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 237/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 238/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 239/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 240/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 241/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 242/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 243/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 244/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 245/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 246/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 247/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 248/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 249/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 250/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 251/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 252/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 253/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 254/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 255/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 256/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 257/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 258/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 259/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 260/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 261/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 262/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 263/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 264/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 265/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 266/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 267/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 268/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 269/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 270/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 271/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 272/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 273/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 274/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 275/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 276/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 277/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 278/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 279/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 280/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 281/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 282/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 283/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 284/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 285/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 286/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 287/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 288/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 289/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 290/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 291/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 292/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 293/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 294/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 295/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 296/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 297/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 298/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 299/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 300/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 301/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 302/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 303/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 304/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 305/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 306/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 307/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 308/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 309/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 310/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 311/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 312/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 313/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 314/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 315/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 316/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 317/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 318/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 319/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 320/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 321/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 322/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 323/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 324/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 325/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 326/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 327/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 328/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 329/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 330/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 331/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 332/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 333/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 334/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 335/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 336/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 337/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 338/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 339/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 340/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 341/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 342/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 343/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 344/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 345/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 346/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 347/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 348/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 349/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 350/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 351/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 352/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 353/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 354/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 355/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 356/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 357/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 358/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 359/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 360/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 361/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 362/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 363/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 364/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 365/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 366/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 367/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 368/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 369/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 370/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 371/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 372/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 373/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 374/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 375/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 376/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 377/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 378/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 379/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 380/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 381/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 382/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 383/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 384/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 385/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 386/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 387/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 388/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 389/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 390/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 391/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 392/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 393/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 394/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 395/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 396/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 397/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 398/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 399/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 400/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 401/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 402/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 403/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 404/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 405/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 406/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 407/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 408/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 409/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 410/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 411/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 412/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 413/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 414/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 415/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 416/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 417/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 418/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 419/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 420/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 421/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 422/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 423/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 424/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 425/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 426/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 427/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 428/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 429/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 430/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 431/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 432/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 433/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 434/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 435/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 436/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 437/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 438/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 439/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 440/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 441/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 442/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 443/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 444/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 445/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 446/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 447/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 448/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 449/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 450/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 451/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 452/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 453/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 454/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 455/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 456/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 457/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 458/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 459/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 460/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 461/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 462/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 463/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 464/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 465/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 466/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 467/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 468/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 469/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 470/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 471/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 472/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 473/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 474/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 475/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 476/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 477/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 478/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 479/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 480/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 481/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 482/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 483/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 484/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 485/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 486/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 487/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 488/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 489/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 490/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 491/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 492/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 493/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 494/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 495/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 496/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 497/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 498/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 499/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 500/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "\n",
      "Evaluating on rest of Day 5...\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Evaluating model...\n",
      "Day 5 Results: AUROC=0.5000, AUPRC=0.5000, F4=0.0000\n",
      "Day 6/7: 2015-01-09-TOTF.PA-book.csv.gz\n",
      "--------------------------------------------------------------------------------\n",
      "Loading data from ../data/TOTF.PA-book\\2015-01-09-TOTF.PA-book.csv.gz...\n",
      "Successfully loaded 1278519 rows.\n",
      "Filtered 2045 pre-market samples.\n",
      "First hour samples: 107573, Rest of day: 1168901\n",
      "Training on 3 time blocks...\n",
      "Block 1/3: Train=19152, Val=20617\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Neural Network (PNN)...\n",
      "Training PNN (Max Epochs=500)...\n",
      "Epoch 1/500 - Train: 22.854437 | Val: 23.025850\n",
      "Validation loss decreased (inf --> 23.025850).  Saving model ...\n",
      "Epoch 2/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 3/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 4/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 5/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 6/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 7/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 8/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 9/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 10/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 11/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 12/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 13/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 14/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 15/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 16/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 17/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 18/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 19/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 20/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 21/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 22/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 23/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 24/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 25/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 26/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 27/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 28/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 29/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 30/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 31/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 32/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 33/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 34/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 35/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 36/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 37/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 38/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 39/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 40/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 41/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 42/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 43/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 44/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 45/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 46/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 47/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 48/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 49/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 50/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 51/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 52/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 53/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 54/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 55/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 56/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 57/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 58/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 59/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 60/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 61/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 62/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 63/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 64/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 65/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 66/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 67/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 68/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 69/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 70/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 71/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 72/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 73/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 74/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 75/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 76/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 77/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 78/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 79/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 80/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 81/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 82/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 83/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 84/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 85/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 86/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 87/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 88/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 89/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 90/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 91/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 92/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 93/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 94/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 95/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 96/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 97/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 98/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 99/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 100/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 101/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 102/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 103/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 104/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 105/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 106/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 107/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 108/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 109/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 110/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 111/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 112/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 113/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 114/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 115/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 116/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 117/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 118/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 119/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 120/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 121/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 122/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 123/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 124/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 125/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 126/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 127/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 128/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 129/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 130/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 131/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 132/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 133/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 134/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 135/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 136/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 137/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 138/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 139/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 140/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 141/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 142/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 143/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 144/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 145/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 146/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 147/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 148/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 149/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 150/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 151/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 152/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 153/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 154/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 155/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 156/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 157/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 158/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 159/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 160/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 161/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 162/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 163/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 164/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 165/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 166/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 167/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 168/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 169/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 170/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 171/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 172/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 173/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 174/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 175/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 176/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 177/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 178/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 179/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 180/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 181/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 182/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 183/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 184/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 185/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 186/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 187/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 188/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 189/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 190/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 191/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 192/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 193/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 194/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 195/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 196/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 197/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 198/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 199/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 200/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 201/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 202/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 203/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 204/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 205/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 206/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 207/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 208/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 209/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 210/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 211/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 212/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 213/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 214/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 215/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 216/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 217/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 218/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 219/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 220/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 221/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 222/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 223/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 224/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 225/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 226/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 227/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 228/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 229/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 230/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 231/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 232/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 233/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 234/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 235/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 236/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 237/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 238/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 239/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 240/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 241/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 242/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 243/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 244/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 245/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 246/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 247/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 248/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 249/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 250/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 251/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 252/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 253/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 254/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 255/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 256/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 257/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 258/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 259/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 260/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 261/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 262/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 263/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 264/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 265/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 266/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 267/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 268/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 269/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 270/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 271/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 272/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 273/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 274/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 275/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 276/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 277/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 278/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 279/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 280/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 281/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 282/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 283/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 284/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 285/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 286/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 287/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 288/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 289/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 290/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 291/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 292/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 293/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 294/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 295/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 296/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 297/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 298/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 299/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 300/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 301/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 302/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 303/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 304/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 305/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 306/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 307/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 308/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 309/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 310/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 311/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 312/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 313/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 314/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 315/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 316/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 317/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 318/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 319/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 320/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 321/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 322/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 323/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 324/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 325/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 326/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 327/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 328/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 329/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 330/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 331/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 332/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 333/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 334/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 335/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 336/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 337/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 338/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 339/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 340/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 341/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 342/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 343/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 344/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 345/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 346/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 347/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 348/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 349/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 350/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 351/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 352/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 353/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 354/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 355/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 356/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 357/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 358/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 359/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 360/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 361/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 362/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 363/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 364/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 365/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 366/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 367/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 368/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 369/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 370/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 371/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 372/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 373/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 374/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 375/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 376/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 377/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 378/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 379/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 380/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 381/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 382/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 383/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 384/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 385/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 386/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 387/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 388/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 389/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 390/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 391/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 392/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 393/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 394/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 395/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 396/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 397/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 398/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 399/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 400/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 401/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 402/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 403/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 404/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 405/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 406/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 407/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 408/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 409/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 410/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 411/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 412/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 413/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 414/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 415/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 416/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 417/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 418/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 419/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 420/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 421/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 422/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 423/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 424/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 425/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 426/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 427/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 428/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 429/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 430/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 431/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 432/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 433/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 434/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 435/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 436/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 437/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 438/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 439/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 440/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 441/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 442/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 443/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 444/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 445/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 446/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 447/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 448/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 449/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 450/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 451/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 452/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 453/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 454/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 455/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 456/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 457/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 458/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 459/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 460/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 461/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 462/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 463/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 464/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 465/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 466/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 467/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 468/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 469/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 470/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 471/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 472/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 473/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 474/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 475/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 476/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 477/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 478/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 479/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 480/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 481/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 482/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 483/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 484/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 485/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 486/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 487/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 488/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 489/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 490/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 491/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 492/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 493/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 494/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 495/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 496/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 497/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 498/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 499/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 500/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Block 2/3: Train=16206, Val=22140\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Neural Network (PNN)...\n",
      "Training PNN (Max Epochs=500)...\n",
      "Epoch 1/500 - Train: 3.186831 | Val: 2.331925\n",
      "Validation loss decreased (inf --> 2.331925).  Saving model ...\n",
      "Epoch 2/500 - Train: 2.443251 | Val: 3.056723\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 3/500 - Train: 2.724788 | Val: 1.866522\n",
      "Validation loss decreased (2.331925 --> 1.866522).  Saving model ...\n",
      "Epoch 4/500 - Train: 1.866531 | Val: 3.792691\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 5/500 - Train: 4.769993 | Val: 4.636475\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 6/500 - Train: 4.770812 | Val: 4.574687\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 7/500 - Train: 4.716712 | Val: 4.526649\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 8/500 - Train: 4.672820 | Val: 4.485931\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Block 3/3: Train=13494, Val=15964\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Neural Network (PNN)...\n",
      "Training PNN (Max Epochs=500)...\n",
      "Epoch 1/500 - Train: 22.766037 | Val: 23.025850\n",
      "Validation loss decreased (inf --> 23.025850).  Saving model ...\n",
      "Epoch 2/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 3/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 4/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 5/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 6/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 7/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 8/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 9/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 10/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 11/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 12/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 13/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 14/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 15/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 16/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 17/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 18/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 19/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 20/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 21/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 22/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 23/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 24/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 25/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 26/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 27/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 28/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 29/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 30/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 31/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 32/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 33/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 34/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 35/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 36/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 37/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 38/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 39/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 40/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 41/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 42/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 43/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 44/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 45/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 46/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 47/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 48/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 49/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 50/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 51/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 52/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 53/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 54/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 55/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 56/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 57/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 58/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 59/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 60/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 61/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 62/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 63/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 64/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 65/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 66/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 67/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 68/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 69/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 70/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 71/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 72/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 73/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 74/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 75/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 76/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 77/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 78/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 79/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 80/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 81/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 82/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 83/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 84/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 85/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 86/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 87/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 88/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 89/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 90/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 91/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 92/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 93/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 94/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 95/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 96/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 97/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 98/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 99/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 100/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 101/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 102/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 103/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 104/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 105/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 106/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 107/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 108/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 109/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 110/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 111/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 112/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 113/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 114/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 115/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 116/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 117/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 118/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 119/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 120/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 121/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 122/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 123/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 124/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 125/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 126/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 127/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 128/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 129/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 130/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 131/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 132/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 133/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 134/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 135/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 136/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 137/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 138/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 139/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 140/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 141/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 142/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 143/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 144/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 145/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 146/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 147/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 148/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 149/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 150/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 151/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 152/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 153/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 154/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 155/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 156/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 157/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 158/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 159/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 160/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 161/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 162/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 163/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 164/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 165/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 166/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 167/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 168/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 169/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 170/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 171/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 172/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 173/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 174/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 175/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 176/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 177/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 178/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 179/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 180/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 181/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 182/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 183/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 184/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 185/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 186/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 187/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 188/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 189/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 190/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 191/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 192/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 193/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 194/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 195/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 196/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 197/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 198/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 199/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 200/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 201/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 202/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 203/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 204/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 205/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 206/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 207/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 208/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 209/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 210/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 211/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 212/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 213/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 214/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 215/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 216/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 217/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 218/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 219/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 220/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 221/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 222/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 223/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 224/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 225/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 226/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 227/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 228/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 229/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 230/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 231/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 232/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 233/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 234/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 235/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 236/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 237/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 238/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 239/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 240/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 241/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 242/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 243/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 244/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 245/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 246/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 247/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 248/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 249/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 250/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 251/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 252/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 253/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 254/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 255/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 256/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 257/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 258/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 259/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 260/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 261/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 262/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 263/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 264/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 265/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 266/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 267/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 268/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 269/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 270/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 271/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 272/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 273/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 274/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 275/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 276/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 277/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 278/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 279/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 280/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 281/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 282/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 283/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 284/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 285/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 286/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 287/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 288/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 289/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 290/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 291/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 292/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 293/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 294/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 295/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 296/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 297/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 298/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 299/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 300/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 301/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 302/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 303/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 304/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 305/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 306/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 307/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 308/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 309/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 310/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 311/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 312/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 313/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 314/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 315/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 316/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 317/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 318/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 319/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 320/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 321/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 322/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 323/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 324/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 325/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 326/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 327/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 328/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 329/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 330/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 331/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 332/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 333/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 334/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 335/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 336/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 337/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 338/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 339/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 340/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 341/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 342/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 343/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 344/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 345/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 346/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 347/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 348/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 349/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 350/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 351/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 352/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 353/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 354/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 355/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 356/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 357/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 358/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 359/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 360/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 361/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 362/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 363/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 364/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 365/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 366/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 367/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 368/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 369/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 370/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 371/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 372/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 373/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 374/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 375/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 376/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 377/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 378/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 379/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 380/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 381/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 382/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 383/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 384/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 385/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 386/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 387/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 388/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 389/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 390/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 391/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 392/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 393/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 394/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 395/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 396/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 397/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 398/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 399/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 400/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 401/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 402/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 403/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 404/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 405/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 406/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 407/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 408/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 409/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 410/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 411/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 412/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 413/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 414/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 415/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 416/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 417/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 418/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 419/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 420/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 421/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 422/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 423/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 424/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 425/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 426/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 427/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 428/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 429/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 430/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 431/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 432/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 433/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 434/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 435/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 436/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 437/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 438/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 439/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 440/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 441/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 442/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 443/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 444/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 445/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 446/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 447/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 448/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 449/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 450/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 451/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 452/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 453/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 454/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 455/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 456/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 457/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 458/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 459/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 460/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 461/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 462/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 463/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 464/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 465/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 466/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 467/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 468/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 469/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 470/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 471/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 472/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 473/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 474/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 475/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 476/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 477/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 478/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 479/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 480/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 481/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 482/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 483/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 484/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 485/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 486/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 487/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 488/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 489/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 490/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 491/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 492/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 493/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 494/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 495/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 496/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 497/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 498/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 499/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 500/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "\n",
      "Evaluating on rest of Day 6...\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Evaluating model...\n",
      "Day 6 Results: AUROC=0.5000, AUPRC=0.5000, F4=0.0000\n",
      "Day 7/7: 2015-01-12-TOTF.PA-book.csv.gz\n",
      "--------------------------------------------------------------------------------\n",
      "Loading data from ../data/TOTF.PA-book\\2015-01-12-TOTF.PA-book.csv.gz...\n",
      "Successfully loaded 1178388 rows.\n",
      "Filtered 2074 pre-market samples.\n",
      "First hour samples: 97467, Rest of day: 1078847\n",
      "Training on 3 time blocks...\n",
      "Block 1/3: Train=13396, Val=19727\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Neural Network (PNN)...\n",
      "Training PNN (Max Epochs=500)...\n",
      "Epoch 1/500 - Train: 3.247624 | Val: 2.637435\n",
      "Validation loss decreased (inf --> 2.637435).  Saving model ...\n",
      "Epoch 2/500 - Train: 4.715459 | Val: 4.984166\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 3/500 - Train: 4.676302 | Val: 4.551018\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 4/500 - Train: 4.388102 | Val: 4.354663\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 5/500 - Train: 4.228055 | Val: 4.222255\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 6/500 - Train: 4.108999 | Val: 4.112014\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "Block 2/3: Train=18530, Val=17941\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Neural Network (PNN)...\n",
      "Training PNN (Max Epochs=500)...\n",
      "Epoch 1/500 - Train: 22.849297 | Val: 23.025850\n",
      "Validation loss decreased (inf --> 23.025850).  Saving model ...\n",
      "Epoch 2/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 3/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 4/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 5/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 6/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 7/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 8/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 9/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 10/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 11/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 12/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 13/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 14/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 15/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 16/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 17/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 18/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 19/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 20/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 21/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 22/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 23/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 24/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 25/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 26/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 27/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 28/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 29/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 30/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 31/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 32/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 33/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 34/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 35/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 36/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 37/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 38/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 39/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 40/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 41/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 42/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 43/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 44/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 45/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 46/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 47/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 48/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 49/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 50/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 51/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 52/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 53/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 54/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 55/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 56/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 57/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 58/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 59/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 60/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 61/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 62/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 63/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 64/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 65/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 66/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 67/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 68/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 69/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 70/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 71/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 72/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 73/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 74/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 75/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 76/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 77/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 78/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 79/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 80/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 81/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 82/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 83/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 84/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 85/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 86/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 87/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 88/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 89/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 90/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 91/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 92/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 93/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 94/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 95/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 96/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 97/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 98/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 99/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 100/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 101/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 102/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 103/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 104/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 105/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 106/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 107/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 108/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 109/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 110/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 111/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 112/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 113/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 114/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 115/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 116/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 117/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 118/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 119/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 120/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 121/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 122/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 123/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 124/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 125/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 126/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 127/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 128/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 129/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 130/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 131/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 132/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 133/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 134/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 135/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 136/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 137/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 138/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 139/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 140/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 141/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 142/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 143/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 144/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 145/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 146/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 147/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 148/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 149/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 150/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 151/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 152/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 153/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 154/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 155/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 156/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 157/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 158/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 159/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 160/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 161/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 162/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 163/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 164/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 165/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 166/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 167/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 168/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 169/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 170/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 171/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 172/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 173/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 174/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 175/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 176/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 177/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 178/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 179/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 180/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 181/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 182/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 183/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 184/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 185/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 186/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 187/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 188/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 189/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 190/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 191/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 192/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 193/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 194/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 195/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 196/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 197/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 198/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 199/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 200/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 201/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 202/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 203/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 204/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 205/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 206/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 207/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 208/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 209/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 210/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 211/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 212/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 213/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 214/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 215/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 216/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 217/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 218/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 219/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 220/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 221/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 222/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 223/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 224/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 225/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 226/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 227/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 228/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 229/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 230/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 231/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 232/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 233/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 234/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 235/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 236/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 237/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 238/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 239/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 240/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 241/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 242/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 243/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 244/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 245/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 246/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 247/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 248/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 249/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 250/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 251/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 252/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 253/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 254/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 255/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 256/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 257/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 258/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 259/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 260/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 261/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 262/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 263/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 264/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 265/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 266/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 267/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 268/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 269/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 270/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 271/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 272/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 273/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 274/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 275/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 276/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 277/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 278/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 279/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 280/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 281/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 282/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 283/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 284/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 285/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 286/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 287/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 288/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 289/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 290/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 291/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 292/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 293/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 294/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 295/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 296/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 297/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 298/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 299/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 300/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 301/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 302/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 303/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 304/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 305/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 306/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 307/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 308/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 309/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 310/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 311/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 312/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 313/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 314/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 315/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 316/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 317/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 318/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 319/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 320/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 321/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 322/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 323/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 324/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 325/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 326/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 327/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 328/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 329/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 330/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 331/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 332/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 333/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 334/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 335/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 336/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 337/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 338/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 339/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 340/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 341/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 342/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 343/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 344/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 345/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 346/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 347/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 348/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 349/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 350/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 351/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 352/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 353/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 354/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 355/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 356/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 357/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 358/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 359/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 360/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 361/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 362/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 363/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 364/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 365/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 366/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 367/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 368/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 369/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 370/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 371/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 372/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 373/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 374/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 375/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 376/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 377/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 378/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 379/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 380/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 381/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 382/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 383/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 384/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 385/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 386/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 387/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 388/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 389/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 390/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 391/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 392/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 393/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 394/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 395/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 396/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 397/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 398/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 399/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 400/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 401/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 402/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 403/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 404/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 405/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 406/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 407/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 408/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 409/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 410/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 411/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 412/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 413/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 414/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 415/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 416/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 417/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 418/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 419/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 420/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 421/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 422/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 423/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 424/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 425/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 426/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 427/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 428/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 429/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 430/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 431/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 432/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 433/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 434/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 435/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 436/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 437/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 438/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 439/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 440/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 441/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 442/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 443/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 444/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 445/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 446/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 447/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 448/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 449/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 450/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 451/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 452/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 453/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 454/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 455/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 456/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 457/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 458/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 459/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 460/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 461/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 462/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 463/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 464/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 465/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 466/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 467/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 468/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 469/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 470/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 471/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 472/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 473/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 474/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 475/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 476/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 477/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 478/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 479/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 480/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 481/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 482/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 483/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 484/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 485/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 486/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 487/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 488/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 489/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 490/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 491/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 492/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 493/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 494/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 495/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 496/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 497/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 498/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 499/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Epoch 500/500 - Train: 23.025850 | Val: 23.025850\n",
      "Validation loss decreased (23.025850 --> 23.025850).  Saving model ...\n",
      "Block 3/3: Train=12909, Val=14964\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Incremental training...\n",
      "Initializing Probabilistic Neural Network (PNN)...\n",
      "Training PNN (Max Epochs=500)...\n",
      "Epoch 1/500 - Train: 4.695921 | Val: 3.806130\n",
      "Validation loss decreased (inf --> 3.806130).  Saving model ...\n",
      "Epoch 2/500 - Train: 3.486297 | Val: 2.955466\n",
      "Validation loss decreased (3.806130 --> 2.955466).  Saving model ...\n",
      "Epoch 3/500 - Train: 14.293219 | Val: 23.025850\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 4/500 - Train: 23.025850 | Val: 23.025850\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 5/500 - Train: 23.025850 | Val: 23.025850\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 6/500 - Train: 23.025850 | Val: 23.025850\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 7/500 - Train: 23.025850 | Val: 23.025850\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered.\n",
      "\n",
      "Evaluating on rest of Day 7...\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Evaluating model...\n",
      "Day 7 Results: AUROC=0.9970, AUPRC=0.9831, F4=0.9986\n",
      "\n",
      "================================================================================\n",
      "Final Day (Test Day): 2015-01-13-TOTF.PA-book.csv.gz\n",
      "--------------------------------------------------------------------------------\n",
      "Loading data from ../data/TOTF.PA-book\\2015-01-13-TOTF.PA-book.csv.gz...\n",
      "Successfully loaded 1211246 rows.\n",
      "Filtered 2059 pre-market samples.\n",
      "Morning samples: 163357, Rest of day: 1045830\n",
      "\n",
      "Evaluating on Final Day Morning...\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Evaluating model...\n",
      "Final Day Morning: AUROC=1.0000, AUPRC=1.0000, F4=0.9988\n",
      "\n",
      "Evaluating on Final Day Rest...\n",
      "Engineering features: ['base', 'tao', 'poutre', 'hawkes', 'ofi']...\n",
      "Hawkes warm-up: Dropped first 3000 rows for EWMA stabilization.\n",
      "Feature Engineering complete. Total features: 131\n",
      "Evaluating model...\n",
      "Final Day Rest: AUROC=0.9955, AUPRC=0.9766, F4=0.5716\n",
      "\n",
      "================================================================================\n",
      "Sequential Training Complete!\n",
      "\n",
      "Saved PNN model to ../models/TOTF_box-cox_pnn_sequential\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAINING: PNN\")\n",
    "\n",
    "results_pnn, pipeline_pnn = sequential_training_pipeline(\n",
    "    data_dir=DATA_DIR,\n",
    "    num_days=NUM_DAYS,\n",
    "    first_hour_minutes=FIRST_HOUR_MINUTES,\n",
    "    train_block_minutes=TRAIN_BLOCK_MINUTES,\n",
    "    val_block_minutes=VAL_BLOCK_MINUTES,\n",
    "    model_type='pnn',\n",
    "    feature_sets=FEATURE_SETS,\n",
    "    epochs=EPOCHS,\n",
    "    lr=LR,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    patience=PATIENCE,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Save results\n",
    "with open('../models/TOTF_sequential_pnn_results.json', 'w') as f:\n",
    "    json.dump(results_pnn, f, indent=2)\n",
    "\n",
    "# Save trained model\n",
    "base_filename = f\"../models/TOTF_{SCALER_TYPE}_pnn_sequential\"\n",
    "torch.save(pipeline_pnn.model.state_dict(), f\"{base_filename}_weights.pth\")\n",
    "joblib.dump(pipeline_pnn.scaler, f\"{base_filename}_scaler.pkl\")\n",
    "\n",
    "config = {\n",
    "    'dataset': 'TOTF',\n",
    "    'model_type': 'pnn',\n",
    "    'scaler_type': SCALER_TYPE,\n",
    "    'training_type': 'sequential',\n",
    "    'num_days': NUM_DAYS,\n",
    "    'seq_length': SEQ_LENGTH,\n",
    "    'input_dim': len(pipeline_pnn.feature_names),\n",
    "    'hidden_dim': HIDDEN_DIM,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'feature_names': pipeline_pnn.feature_names\n",
    "}\n",
    "with open(f\"{base_filename}_config.json\", 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved PNN model to {base_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f0cee2",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb6ca3e",
   "metadata": {},
   "source": [
    "### Transformer + OC-SVM Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ff0148d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential training results plot saved to ../results/training/figures/TOTF_sequential_transformer_ocsvm_results.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1000 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "viz.plot_sequential_results(results_transformer, save_path='../results/training/figures/TOTF_sequential_transformer_ocsvm_results.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e67e47",
   "metadata": {},
   "source": [
    "### PRAE Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9a0ecd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential training results plot saved to ../results/training/figures/TOTF_sequential_prae_results.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1000 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "viz.plot_sequential_results(results_prae, save_path='../results/training/figures/TOTF_sequential_prae_results.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b119480",
   "metadata": {},
   "source": [
    "### PNN Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84bb9e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential training results plot saved to ../results/training/figures/TOTF_sequential_pnn_results.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1000 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "viz.plot_sequential_results(results_pnn, save_path='../results/training/figures/TOTF_sequential_pnn_results.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99c5e25",
   "metadata": {},
   "source": [
    "## Compare All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e68c5775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL COMPARISON - Sequential Training\n",
      "            Model  Avg_AUROC  Avg_AUPRC   Avg_F4  FinalDay_Morning_AUROC  FinalDay_Morning_AUPRC  FinalDay_Morning_F4  FinalDay_Rest_AUROC  FinalDay_Rest_AUPRC  FinalDay_Rest_F4\n",
      "Transformer+OCSVM   0.986975   0.977899 0.971849                     1.0                     1.0             0.994192             1.000000             1.000000          0.952589\n",
      "             PRAE   0.998220   0.989978 0.001830                     1.0                     1.0             0.999977             0.998308             0.990150          0.001234\n",
      "              PNN   0.751653   0.727667 0.285067                     1.0                     1.0             0.998783             0.995500             0.976584          0.571620\n",
      "\n",
      "Saved comparison to ../results/training/TOTF_sequential_model_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, results in [('Transformer+OCSVM', results_transformer), \n",
    "                             ('PRAE', results_prae), \n",
    "                             ('PNN', results_pnn)]:\n",
    "    \n",
    "    # Average over all training days\n",
    "    daily_df = pd.DataFrame(results['daily_metrics'])\n",
    "    avg_metrics = {\n",
    "        'Model': model_name,\n",
    "        'Avg_AUROC': daily_df['AUROC'].mean(),\n",
    "        'Avg_AUPRC': daily_df['AUPRC'].mean(),\n",
    "        'Avg_F4': daily_df['F4_Score'].mean(),\n",
    "    }\n",
    "    \n",
    "    # Final Day Morning\n",
    "    if results['final_day_morning_metrics']:\n",
    "        avg_metrics['FinalDay_Morning_AUROC'] = results['final_day_morning_metrics']['AUROC']\n",
    "        avg_metrics['FinalDay_Morning_AUPRC'] = results['final_day_morning_metrics']['AUPRC']\n",
    "        avg_metrics['FinalDay_Morning_F4'] = results['final_day_morning_metrics']['F4_Score']\n",
    "    \n",
    "    # Final Day Rest\n",
    "    if results['final_day_rest_metrics']:\n",
    "        avg_metrics['FinalDay_Rest_AUROC'] = results['final_day_rest_metrics']['AUROC']\n",
    "        avg_metrics['FinalDay_Rest_AUPRC'] = results['final_day_rest_metrics']['AUPRC']\n",
    "        avg_metrics['FinalDay_Rest_F4'] = results['final_day_rest_metrics']['F4_Score']\n",
    "    \n",
    "    comparison_data.append(avg_metrics)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"MODEL COMPARISON - Sequential Training\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Save comparison\n",
    "comparison_df.to_csv('../results/training/TOTF_sequential_model_comparison.csv', index=False)\n",
    "print(\"\\nSaved comparison to ../results/training/TOTF_sequential_model_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d3b14b",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04841081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQUENTIAL TRAINING SUMMARY\n",
      "Dataset: TOTF\n",
      "Scaler: box-cox\n",
      "Training Days: 7\n",
      "Training Strategy: First 60 minutes per day\n",
      "  - Train blocks: 10 minutes\n",
      "  - Val blocks: 10 minutes\n",
      "Test: Rest of each day + Final Day (morning & rest)\n",
      "\n",
      "Hyperparameters:\n",
      "  - Sequence Length: 25\n",
      "  - Batch Size: 128\n",
      "  - Epochs per block: 1000\n",
      "  - Hidden Dim: 64\n",
      "  - Learning Rate: 0.001\n",
      "\n",
      "Best Model (by Final Day Rest AUROC):\n",
      "  Transformer+OCSVM\n",
      "  - AUROC: 1.0000\n",
      "  - AUPRC: 1.0000\n",
      "  - F4 Score: 0.9526\n"
     ]
    }
   ],
   "source": [
    "print(\"SEQUENTIAL TRAINING SUMMARY\")\n",
    "print(f\"Dataset: TOTF\")\n",
    "print(f\"Scaler: {SCALER_TYPE}\")\n",
    "print(f\"Training Days: {NUM_DAYS}\")\n",
    "print(f\"Training Strategy: First {FIRST_HOUR_MINUTES} minutes per day\")\n",
    "print(f\"  - Train blocks: {TRAIN_BLOCK_MINUTES} minutes\")\n",
    "print(f\"  - Val blocks: {VAL_BLOCK_MINUTES} minutes\")\n",
    "print(f\"Test: Rest of each day + Final Day (morning & rest)\")\n",
    "print(\"\\nHyperparameters:\")\n",
    "print(f\"  - Sequence Length: {SEQ_LENGTH}\")\n",
    "print(f\"  - Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  - Epochs per block: {EPOCHS}\")\n",
    "print(f\"  - Hidden Dim: {HIDDEN_DIM}\")\n",
    "print(f\"  - Learning Rate: {LR}\")\n",
    "print(\"\\nBest Model (by Final Day Rest AUROC):\")\n",
    "best_idx = comparison_df['FinalDay_Rest_AUROC'].idxmax()\n",
    "best_model = comparison_df.loc[best_idx]\n",
    "print(f\"  {best_model['Model']}\")\n",
    "print(f\"  - AUROC: {best_model['FinalDay_Rest_AUROC']:.4f}\")\n",
    "print(f\"  - AUPRC: {best_model['FinalDay_Rest_AUPRC']:.4f}\")\n",
    "print(f\"  - F4 Score: {best_model['FinalDay_Rest_F4']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
